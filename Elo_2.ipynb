{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 151.74 Mb (66.3% reduction)\n",
      "Wall time: 3min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path=\"D:\\Python\\Elo\"\n",
    "df_train = pd.read_csv(os.path.join(path,'main.csv'))\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "df_train=reduce_mem_usage(df_train)\n",
    "hist_data=pd.read_csv(os.path.join(path,'hard_data_hist.csv'))\n",
    "df_train=df_train.merge(hist_data,how='left',on='card_id')\n",
    "new_data=pd.read_csv(os.path.join(path,'hard_data_new.csv'))\n",
    "df_train=df_train.merge(new_data,how='left',on='card_id')\n",
    "\n",
    "hist_data=pd.read_csv(os.path.join(path,'hist_last_update.csv'))\n",
    "df_train=df_train.merge(hist_data,how='left',on='card_id')\n",
    "new_data=pd.read_csv(os.path.join(path,'new_last_update.csv'))\n",
    "df_train=df_train.merge(new_data,how='left',on='card_id')\n",
    "def ratio_month(df):\n",
    "    cl_one=[a for a in df.columns if 'hist_month' in str(a)]\n",
    "    cl_two=[a for a in df.columns if 'new_month' in str(a)]\n",
    "    for one,two in zip(cl_one,cl_two):\n",
    "        df['new_'+one+'_ratio']=df[two]/df[one]\n",
    "    return df\n",
    "df_train=ratio_month(df_train)\n",
    "df_train.to_csv(os.path.join(path,'result.csv'))\n",
    "#selector=['hist_month_lag_mean', 'hist_month_lag_var', 'hist_month_diff_mean',\n",
    "#       'hist_fathers_day_2017_mean', 'hist_duration_min',\n",
    "#       'hist_purchase_date_diff', 'hist_purchase_date_uptonow','card_id','target']\n",
    "#df_train=df_train[selector]\n",
    "#del hist_data; new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb=LabelEncoder()\n",
    "obj=list(df_train.select_dtypes(include=['object']).columns)[2:]\n",
    "for o in obj:\n",
    "    df_train[o]=lb.fit_transform(df_train[o])\n",
    "df_train=df_train.replace([np.inf, -np.inf], np.nan)\n",
    "df_train=df_train.fillna(0)\n",
    "df_train['outliers'] = 0\n",
    "df_train.loc[df_train['target'] < -30, 'outliers'] = 1\n",
    "all_df_train_columns = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','target','outliers']]\n",
    "best_factor=[all_df_train_columns[1]]\n",
    "best_score=9000\n",
    "for fact in all_df_train_columns[2:]:\n",
    "    df_train_columns=best_factor.copy()\n",
    "    df_train_columns.append(fact)\n",
    "    rskf=StratifiedKFold(5,shuffle=True,random_state=4590)\n",
    "    val_pr=np.zeros(len(df_train))\n",
    "    for train_index,val_index in rskf.split(df_train,df_train['outliers'].values):\n",
    "    #    train_data=lgb.Dataset(df_train[df_train_columns].loc[train_index],label=df_train['target'].loc[train_index])\n",
    "    #    val_data=lgb.Dataset(df_train[df_train_columns].loc[val_index],label=df_train['target'].loc[val_index])\n",
    "        model=Ridge()\n",
    "        model.fit(df_train[df_train_columns].loc[train_index],df_train['target'].loc[train_index])\n",
    "        val_pr[val_index]=model.predict(df_train[df_train_columns].loc[val_index])\n",
    "    score=np.sqrt(mean_squared_error(val_pr,df_train['target']))\n",
    "    if score<best_score:\n",
    "        best_factor.append(fact)\n",
    "        best_score=score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.7993\tvalid_1's rmse: 3.81975\n",
      "[200]\ttraining's rmse: 3.77654\tvalid_1's rmse: 3.80353\n",
      "[300]\ttraining's rmse: 3.76332\tvalid_1's rmse: 3.79703\n",
      "[400]\ttraining's rmse: 3.75406\tvalid_1's rmse: 3.79459\n",
      "[500]\ttraining's rmse: 3.74649\tvalid_1's rmse: 3.7937\n",
      "[600]\ttraining's rmse: 3.73988\tvalid_1's rmse: 3.79363\n",
      "[700]\ttraining's rmse: 3.73381\tvalid_1's rmse: 3.79365\n",
      "[800]\ttraining's rmse: 3.72825\tvalid_1's rmse: 3.79364\n",
      "[900]\ttraining's rmse: 3.72297\tvalid_1's rmse: 3.79378\n",
      "Early stopping, best iteration is:\n",
      "[581]\ttraining's rmse: 3.741\tvalid_1's rmse: 3.79358\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.80269\tvalid_1's rmse: 3.80616\n",
      "[200]\ttraining's rmse: 3.78011\tvalid_1's rmse: 3.78984\n",
      "[300]\ttraining's rmse: 3.76683\tvalid_1's rmse: 3.78344\n",
      "[400]\ttraining's rmse: 3.75758\tvalid_1's rmse: 3.7811\n",
      "[500]\ttraining's rmse: 3.74998\tvalid_1's rmse: 3.78028\n",
      "[600]\ttraining's rmse: 3.74362\tvalid_1's rmse: 3.77994\n",
      "[700]\ttraining's rmse: 3.73755\tvalid_1's rmse: 3.77987\n",
      "[800]\ttraining's rmse: 3.732\tvalid_1's rmse: 3.7798\n",
      "[900]\ttraining's rmse: 3.72673\tvalid_1's rmse: 3.78005\n",
      "[1000]\ttraining's rmse: 3.72166\tvalid_1's rmse: 3.78048\n",
      "[1100]\ttraining's rmse: 3.71704\tvalid_1's rmse: 3.7807\n",
      "Early stopping, best iteration is:\n",
      "[751]\ttraining's rmse: 3.73459\tvalid_1's rmse: 3.77975\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.801\tvalid_1's rmse: 3.81271\n",
      "[200]\ttraining's rmse: 3.77828\tvalid_1's rmse: 3.79599\n",
      "[300]\ttraining's rmse: 3.76506\tvalid_1's rmse: 3.78923\n",
      "[400]\ttraining's rmse: 3.756\tvalid_1's rmse: 3.78672\n",
      "[500]\ttraining's rmse: 3.74865\tvalid_1's rmse: 3.78564\n",
      "[600]\ttraining's rmse: 3.74219\tvalid_1's rmse: 3.7853\n",
      "[700]\ttraining's rmse: 3.73633\tvalid_1's rmse: 3.78542\n",
      "[800]\ttraining's rmse: 3.7309\tvalid_1's rmse: 3.78543\n",
      "[900]\ttraining's rmse: 3.72569\tvalid_1's rmse: 3.78555\n",
      "[1000]\ttraining's rmse: 3.72054\tvalid_1's rmse: 3.78556\n",
      "Early stopping, best iteration is:\n",
      "[630]\ttraining's rmse: 3.74037\tvalid_1's rmse: 3.78523\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.80207\tvalid_1's rmse: 3.80762\n",
      "[200]\ttraining's rmse: 3.77927\tvalid_1's rmse: 3.79172\n",
      "[300]\ttraining's rmse: 3.76592\tvalid_1's rmse: 3.78544\n",
      "[400]\ttraining's rmse: 3.75679\tvalid_1's rmse: 3.78353\n",
      "[500]\ttraining's rmse: 3.74929\tvalid_1's rmse: 3.78268\n",
      "[600]\ttraining's rmse: 3.7428\tvalid_1's rmse: 3.78243\n",
      "[700]\ttraining's rmse: 3.73673\tvalid_1's rmse: 3.7825\n",
      "[800]\ttraining's rmse: 3.73093\tvalid_1's rmse: 3.78259\n",
      "[900]\ttraining's rmse: 3.72556\tvalid_1's rmse: 3.78285\n",
      "Early stopping, best iteration is:\n",
      "[597]\ttraining's rmse: 3.743\tvalid_1's rmse: 3.78239\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.80445\tvalid_1's rmse: 3.79612\n",
      "[200]\ttraining's rmse: 3.78133\tvalid_1's rmse: 3.7818\n",
      "[300]\ttraining's rmse: 3.7678\tvalid_1's rmse: 3.77658\n",
      "[400]\ttraining's rmse: 3.75861\tvalid_1's rmse: 3.77508\n",
      "[500]\ttraining's rmse: 3.75099\tvalid_1's rmse: 3.77492\n",
      "[600]\ttraining's rmse: 3.74438\tvalid_1's rmse: 3.7749\n",
      "[700]\ttraining's rmse: 3.73842\tvalid_1's rmse: 3.77498\n",
      "[800]\ttraining's rmse: 3.73263\tvalid_1's rmse: 3.77522\n",
      "[900]\ttraining's rmse: 3.72732\tvalid_1's rmse: 3.77552\n",
      "Early stopping, best iteration is:\n",
      "[560]\ttraining's rmse: 3.74694\tvalid_1's rmse: 3.77484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.783163030815591"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['outliers'] = 0\n",
    "df_train.loc[df_train['target'] < -30, 'outliers'] = 1\n",
    "obj=list(df_train.select_dtypes(include=['object']).columns)[2:]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb=LabelEncoder()\n",
    "for o in obj:\n",
    "    df_train[o]=lb.fit_transform(df_train[o]) \n",
    "df_train_columns = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','target','outliers','Unnamed: 0','Unnamed: 0_x']]\n",
    "param = {'objective': 'regression_l2', \n",
    "    'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt', \n",
    "            'n_jobs': 4, 'max_depth': 18, \n",
    "            'n_estimators': 6100, \n",
    "            'subsample_freq': 2, \n",
    "            'min_data_per_group': 100, \n",
    "            'max_cat_to_onehot': 4, \n",
    "            'cat_l2': 10.0, \n",
    "            'cat_smooth': 10.0, \n",
    "            'max_cat_threshold': 32, \n",
    "            'metric_freq': 10, \n",
    "            'verbosity': -1, \n",
    "            'colsample_bytree': 0.5, \n",
    "            'learning_rate': 0.0061033234451294376, \n",
    "            'min_child_samples': 20, \n",
    "            'min_child_weight': 9.0, \n",
    "            'min_split_gain': 1e-06, \n",
    "            'num_leaves': 36, \n",
    "            'reg_alpha': 40.0, \n",
    "            'reg_lambda': 13.3, \n",
    "            'subsample': 0.9}\n",
    "rskf=StratifiedKFold(5,shuffle=True,random_state=4590)\n",
    "val_pr=np.zeros(len(df_train))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "#test_pr=np.zeros(len(df_test))\n",
    "for train_index,val_index in rskf.split(df_train,df_train['outliers'].values):\n",
    "    train_data=lgb.Dataset(df_train[df_train_columns].loc[train_index],label=df_train['target'].loc[train_index])\n",
    "    val_data=lgb.Dataset(df_train[df_train_columns].loc[val_index],label=df_train['target'].loc[val_index])\n",
    "    num_round = 10000\n",
    "    model=lgb.train(param,train_data,num_round,valid_sets=[train_data,val_data],verbose_eval=100,early_stopping_rounds=400)\n",
    "    fold_importance_df=pd.DataFrame()\n",
    "    fold_importance_df['feature'] = df_train_columns\n",
    "    fold_importance_df['importance']=model.feature_importance()\n",
    "    feature_importance_df=pd.concat([feature_importance_df,fold_importance_df],axis=0)\n",
    "    \n",
    "    val_pr[val_index]=model.predict(df_train[df_train_columns].loc[val_index],num_iteration=model.best_iteration)\n",
    "np.sqrt(mean_squared_error(val_pr,df_train['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.71247\tvalid_1's rmse: 3.74918\n",
      "[200]\ttraining's rmse: 3.64187\tvalid_1's rmse: 3.6996\n",
      "[300]\ttraining's rmse: 3.59826\tvalid_1's rmse: 3.6771\n",
      "[400]\ttraining's rmse: 3.56789\tvalid_1's rmse: 3.6653\n",
      "[500]\ttraining's rmse: 3.54509\tvalid_1's rmse: 3.65721\n",
      "[600]\ttraining's rmse: 3.52739\tvalid_1's rmse: 3.65263\n",
      "[700]\ttraining's rmse: 3.51212\tvalid_1's rmse: 3.64931\n",
      "[800]\ttraining's rmse: 3.49854\tvalid_1's rmse: 3.6471\n",
      "[900]\ttraining's rmse: 3.48566\tvalid_1's rmse: 3.64532\n",
      "[1000]\ttraining's rmse: 3.47426\tvalid_1's rmse: 3.64405\n",
      "[1100]\ttraining's rmse: 3.46305\tvalid_1's rmse: 3.64288\n",
      "[1200]\ttraining's rmse: 3.45268\tvalid_1's rmse: 3.64253\n",
      "[1300]\ttraining's rmse: 3.44319\tvalid_1's rmse: 3.64186\n",
      "[1400]\ttraining's rmse: 3.43309\tvalid_1's rmse: 3.64121\n",
      "[1500]\ttraining's rmse: 3.42311\tvalid_1's rmse: 3.64068\n",
      "[1600]\ttraining's rmse: 3.41422\tvalid_1's rmse: 3.64057\n",
      "[1700]\ttraining's rmse: 3.40548\tvalid_1's rmse: 3.64017\n",
      "[1800]\ttraining's rmse: 3.39595\tvalid_1's rmse: 3.64\n",
      "[1900]\ttraining's rmse: 3.38703\tvalid_1's rmse: 3.63966\n",
      "[2000]\ttraining's rmse: 3.37848\tvalid_1's rmse: 3.63947\n",
      "[2100]\ttraining's rmse: 3.36954\tvalid_1's rmse: 3.63938\n",
      "[2200]\ttraining's rmse: 3.3609\tvalid_1's rmse: 3.63948\n",
      "[2300]\ttraining's rmse: 3.352\tvalid_1's rmse: 3.6395\n",
      "[2400]\ttraining's rmse: 3.34373\tvalid_1's rmse: 3.63945\n",
      "Early stopping, best iteration is:\n",
      "[2022]\ttraining's rmse: 3.37642\tvalid_1's rmse: 3.63928\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.71138\tvalid_1's rmse: 3.75199\n",
      "[200]\ttraining's rmse: 3.64056\tvalid_1's rmse: 3.70664\n",
      "[300]\ttraining's rmse: 3.59721\tvalid_1's rmse: 3.68544\n",
      "[400]\ttraining's rmse: 3.56697\tvalid_1's rmse: 3.67433\n",
      "[500]\ttraining's rmse: 3.54516\tvalid_1's rmse: 3.66628\n",
      "[600]\ttraining's rmse: 3.52692\tvalid_1's rmse: 3.66164\n",
      "[700]\ttraining's rmse: 3.51195\tvalid_1's rmse: 3.6586\n",
      "[800]\ttraining's rmse: 3.49829\tvalid_1's rmse: 3.65668\n",
      "[900]\ttraining's rmse: 3.48589\tvalid_1's rmse: 3.65514\n",
      "[1000]\ttraining's rmse: 3.47434\tvalid_1's rmse: 3.65404\n",
      "[1100]\ttraining's rmse: 3.46285\tvalid_1's rmse: 3.65287\n",
      "[1200]\ttraining's rmse: 3.45172\tvalid_1's rmse: 3.65177\n",
      "[1300]\ttraining's rmse: 3.44139\tvalid_1's rmse: 3.65141\n",
      "[1400]\ttraining's rmse: 3.43175\tvalid_1's rmse: 3.65118\n",
      "[1500]\ttraining's rmse: 3.4219\tvalid_1's rmse: 3.65085\n",
      "[1600]\ttraining's rmse: 3.4128\tvalid_1's rmse: 3.65059\n",
      "[1700]\ttraining's rmse: 3.40365\tvalid_1's rmse: 3.65052\n",
      "[1800]\ttraining's rmse: 3.39508\tvalid_1's rmse: 3.65\n",
      "[1900]\ttraining's rmse: 3.38671\tvalid_1's rmse: 3.64988\n",
      "[2000]\ttraining's rmse: 3.37797\tvalid_1's rmse: 3.64938\n",
      "[2100]\ttraining's rmse: 3.3693\tvalid_1's rmse: 3.64919\n",
      "[2200]\ttraining's rmse: 3.36105\tvalid_1's rmse: 3.64945\n",
      "[2300]\ttraining's rmse: 3.35273\tvalid_1's rmse: 3.64911\n",
      "[2400]\ttraining's rmse: 3.34388\tvalid_1's rmse: 3.6493\n",
      "[2500]\ttraining's rmse: 3.33512\tvalid_1's rmse: 3.64893\n",
      "[2600]\ttraining's rmse: 3.32729\tvalid_1's rmse: 3.64819\n",
      "[2700]\ttraining's rmse: 3.31853\tvalid_1's rmse: 3.64823\n",
      "[2800]\ttraining's rmse: 3.31031\tvalid_1's rmse: 3.64831\n",
      "[2900]\ttraining's rmse: 3.30172\tvalid_1's rmse: 3.64844\n",
      "[3000]\ttraining's rmse: 3.29397\tvalid_1's rmse: 3.64844\n",
      "Early stopping, best iteration is:\n",
      "[2654]\ttraining's rmse: 3.32226\tvalid_1's rmse: 3.64796\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.71467\tvalid_1's rmse: 3.72865\n",
      "[200]\ttraining's rmse: 3.64508\tvalid_1's rmse: 3.67565\n",
      "[300]\ttraining's rmse: 3.60211\tvalid_1's rmse: 3.65085\n",
      "[400]\ttraining's rmse: 3.57224\tvalid_1's rmse: 3.63689\n",
      "[500]\ttraining's rmse: 3.54987\tvalid_1's rmse: 3.62866\n",
      "[600]\ttraining's rmse: 3.53199\tvalid_1's rmse: 3.62333\n",
      "[700]\ttraining's rmse: 3.517\tvalid_1's rmse: 3.61988\n",
      "[800]\ttraining's rmse: 3.50369\tvalid_1's rmse: 3.61785\n",
      "[900]\ttraining's rmse: 3.49127\tvalid_1's rmse: 3.61636\n",
      "[1000]\ttraining's rmse: 3.48023\tvalid_1's rmse: 3.61531\n",
      "[1100]\ttraining's rmse: 3.46854\tvalid_1's rmse: 3.61399\n",
      "[1200]\ttraining's rmse: 3.45795\tvalid_1's rmse: 3.61359\n",
      "[1300]\ttraining's rmse: 3.44811\tvalid_1's rmse: 3.61324\n",
      "[1400]\ttraining's rmse: 3.43859\tvalid_1's rmse: 3.61252\n",
      "[1500]\ttraining's rmse: 3.42939\tvalid_1's rmse: 3.61209\n",
      "[1600]\ttraining's rmse: 3.41957\tvalid_1's rmse: 3.61191\n",
      "[1700]\ttraining's rmse: 3.41053\tvalid_1's rmse: 3.61153\n",
      "[1800]\ttraining's rmse: 3.40208\tvalid_1's rmse: 3.61111\n",
      "[1900]\ttraining's rmse: 3.39315\tvalid_1's rmse: 3.61077\n",
      "[2000]\ttraining's rmse: 3.38415\tvalid_1's rmse: 3.61081\n",
      "[2100]\ttraining's rmse: 3.37571\tvalid_1's rmse: 3.61022\n",
      "[2200]\ttraining's rmse: 3.36703\tvalid_1's rmse: 3.61002\n",
      "[2300]\ttraining's rmse: 3.35766\tvalid_1's rmse: 3.60991\n",
      "[2400]\ttraining's rmse: 3.3492\tvalid_1's rmse: 3.60984\n",
      "[2500]\ttraining's rmse: 3.34049\tvalid_1's rmse: 3.60955\n",
      "[2600]\ttraining's rmse: 3.33288\tvalid_1's rmse: 3.60979\n",
      "[2700]\ttraining's rmse: 3.32536\tvalid_1's rmse: 3.60954\n",
      "[2800]\ttraining's rmse: 3.31777\tvalid_1's rmse: 3.60956\n",
      "[2900]\ttraining's rmse: 3.30969\tvalid_1's rmse: 3.60952\n",
      "[3000]\ttraining's rmse: 3.30184\tvalid_1's rmse: 3.60937\n",
      "[3100]\ttraining's rmse: 3.2935\tvalid_1's rmse: 3.60957\n",
      "[3200]\ttraining's rmse: 3.28606\tvalid_1's rmse: 3.6096\n",
      "[3300]\ttraining's rmse: 3.27813\tvalid_1's rmse: 3.60954\n",
      "[3400]\ttraining's rmse: 3.27044\tvalid_1's rmse: 3.60949\n",
      "Early stopping, best iteration is:\n",
      "[3023]\ttraining's rmse: 3.29977\tvalid_1's rmse: 3.60927\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.71063\tvalid_1's rmse: 3.75257\n",
      "[200]\ttraining's rmse: 3.63931\tvalid_1's rmse: 3.71256\n",
      "[300]\ttraining's rmse: 3.5962\tvalid_1's rmse: 3.69655\n",
      "[400]\ttraining's rmse: 3.56636\tvalid_1's rmse: 3.68902\n",
      "[500]\ttraining's rmse: 3.54357\tvalid_1's rmse: 3.68525\n",
      "[600]\ttraining's rmse: 3.52515\tvalid_1's rmse: 3.68252\n",
      "[700]\ttraining's rmse: 3.50926\tvalid_1's rmse: 3.6809\n",
      "[800]\ttraining's rmse: 3.49543\tvalid_1's rmse: 3.67902\n",
      "[900]\ttraining's rmse: 3.48275\tvalid_1's rmse: 3.67834\n",
      "[1000]\ttraining's rmse: 3.47108\tvalid_1's rmse: 3.67738\n",
      "[1100]\ttraining's rmse: 3.4596\tvalid_1's rmse: 3.67677\n",
      "[1200]\ttraining's rmse: 3.4492\tvalid_1's rmse: 3.67659\n",
      "[1300]\ttraining's rmse: 3.43842\tvalid_1's rmse: 3.67625\n",
      "[1400]\ttraining's rmse: 3.42859\tvalid_1's rmse: 3.6763\n",
      "[1500]\ttraining's rmse: 3.41887\tvalid_1's rmse: 3.67582\n",
      "[1600]\ttraining's rmse: 3.40928\tvalid_1's rmse: 3.67578\n",
      "[1700]\ttraining's rmse: 3.40005\tvalid_1's rmse: 3.6756\n",
      "[1800]\ttraining's rmse: 3.39124\tvalid_1's rmse: 3.67512\n",
      "[1900]\ttraining's rmse: 3.38275\tvalid_1's rmse: 3.67535\n",
      "[2000]\ttraining's rmse: 3.37377\tvalid_1's rmse: 3.67594\n",
      "[2100]\ttraining's rmse: 3.3655\tvalid_1's rmse: 3.6758\n",
      "[2200]\ttraining's rmse: 3.35698\tvalid_1's rmse: 3.67595\n",
      "Early stopping, best iteration is:\n",
      "[1800]\ttraining's rmse: 3.39124\tvalid_1's rmse: 3.67512\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.71546\tvalid_1's rmse: 3.72254\n",
      "[200]\ttraining's rmse: 3.64601\tvalid_1's rmse: 3.66609\n",
      "[300]\ttraining's rmse: 3.6037\tvalid_1's rmse: 3.63915\n",
      "[400]\ttraining's rmse: 3.57364\tvalid_1's rmse: 3.62406\n",
      "[500]\ttraining's rmse: 3.55134\tvalid_1's rmse: 3.61595\n",
      "[600]\ttraining's rmse: 3.5338\tvalid_1's rmse: 3.61166\n",
      "[700]\ttraining's rmse: 3.51791\tvalid_1's rmse: 3.60835\n",
      "[800]\ttraining's rmse: 3.50368\tvalid_1's rmse: 3.60552\n",
      "[900]\ttraining's rmse: 3.49103\tvalid_1's rmse: 3.60379\n",
      "[1000]\ttraining's rmse: 3.47938\tvalid_1's rmse: 3.60239\n",
      "[1100]\ttraining's rmse: 3.46817\tvalid_1's rmse: 3.60135\n",
      "[1200]\ttraining's rmse: 3.4571\tvalid_1's rmse: 3.60011\n",
      "[1300]\ttraining's rmse: 3.44757\tvalid_1's rmse: 3.59933\n",
      "[1400]\ttraining's rmse: 3.43794\tvalid_1's rmse: 3.59888\n",
      "[1500]\ttraining's rmse: 3.42912\tvalid_1's rmse: 3.59852\n",
      "[1600]\ttraining's rmse: 3.42008\tvalid_1's rmse: 3.59791\n",
      "[1700]\ttraining's rmse: 3.41119\tvalid_1's rmse: 3.59756\n",
      "[1800]\ttraining's rmse: 3.40221\tvalid_1's rmse: 3.59777\n",
      "[1900]\ttraining's rmse: 3.39393\tvalid_1's rmse: 3.59764\n",
      "[2000]\ttraining's rmse: 3.38489\tvalid_1's rmse: 3.59732\n",
      "[2100]\ttraining's rmse: 3.37609\tvalid_1's rmse: 3.59739\n",
      "[2200]\ttraining's rmse: 3.36743\tvalid_1's rmse: 3.59751\n",
      "[2300]\ttraining's rmse: 3.35988\tvalid_1's rmse: 3.59717\n",
      "[2400]\ttraining's rmse: 3.35126\tvalid_1's rmse: 3.59718\n",
      "[2500]\ttraining's rmse: 3.34323\tvalid_1's rmse: 3.5974\n",
      "[2600]\ttraining's rmse: 3.33501\tvalid_1's rmse: 3.5978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2700]\ttraining's rmse: 3.32628\tvalid_1's rmse: 3.59808\n",
      "[2800]\ttraining's rmse: 3.31803\tvalid_1's rmse: 3.5981\n",
      "Early stopping, best iteration is:\n",
      "[2417]\ttraining's rmse: 3.34991\tvalid_1's rmse: 3.59705\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.70869\tvalid_1's rmse: 3.77408\n",
      "[200]\ttraining's rmse: 3.63713\tvalid_1's rmse: 3.73756\n",
      "[300]\ttraining's rmse: 3.59327\tvalid_1's rmse: 3.72126\n",
      "[400]\ttraining's rmse: 3.56254\tvalid_1's rmse: 3.71282\n",
      "[500]\ttraining's rmse: 3.53935\tvalid_1's rmse: 3.70718\n",
      "[600]\ttraining's rmse: 3.52077\tvalid_1's rmse: 3.70371\n",
      "[700]\ttraining's rmse: 3.50521\tvalid_1's rmse: 3.70121\n",
      "[800]\ttraining's rmse: 3.49152\tvalid_1's rmse: 3.6996\n",
      "[900]\ttraining's rmse: 3.47936\tvalid_1's rmse: 3.69843\n",
      "[1000]\ttraining's rmse: 3.46809\tvalid_1's rmse: 3.69755\n",
      "[1100]\ttraining's rmse: 3.45717\tvalid_1's rmse: 3.6972\n",
      "[1200]\ttraining's rmse: 3.44641\tvalid_1's rmse: 3.69665\n",
      "[1300]\ttraining's rmse: 3.43625\tvalid_1's rmse: 3.69627\n",
      "[1400]\ttraining's rmse: 3.42666\tvalid_1's rmse: 3.6962\n",
      "[1500]\ttraining's rmse: 3.41699\tvalid_1's rmse: 3.69577\n",
      "[1600]\ttraining's rmse: 3.40802\tvalid_1's rmse: 3.69533\n",
      "[1700]\ttraining's rmse: 3.39847\tvalid_1's rmse: 3.69498\n",
      "[1800]\ttraining's rmse: 3.38889\tvalid_1's rmse: 3.69439\n",
      "[1900]\ttraining's rmse: 3.38007\tvalid_1's rmse: 3.69441\n",
      "[2000]\ttraining's rmse: 3.37074\tvalid_1's rmse: 3.69437\n",
      "[2100]\ttraining's rmse: 3.36202\tvalid_1's rmse: 3.69425\n",
      "[2200]\ttraining's rmse: 3.35316\tvalid_1's rmse: 3.69394\n",
      "[2300]\ttraining's rmse: 3.34457\tvalid_1's rmse: 3.69442\n",
      "[2400]\ttraining's rmse: 3.33577\tvalid_1's rmse: 3.69447\n",
      "[2500]\ttraining's rmse: 3.32739\tvalid_1's rmse: 3.69473\n",
      "Early stopping, best iteration is:\n",
      "[2194]\ttraining's rmse: 3.35364\tvalid_1's rmse: 3.6939\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.71296\tvalid_1's rmse: 3.73113\n",
      "[200]\ttraining's rmse: 3.64231\tvalid_1's rmse: 3.6874\n",
      "[300]\ttraining's rmse: 3.59875\tvalid_1's rmse: 3.66873\n",
      "[400]\ttraining's rmse: 3.56888\tvalid_1's rmse: 3.65946\n",
      "[500]\ttraining's rmse: 3.54572\tvalid_1's rmse: 3.65442\n",
      "[600]\ttraining's rmse: 3.5277\tvalid_1's rmse: 3.65116\n",
      "[700]\ttraining's rmse: 3.51133\tvalid_1's rmse: 3.64878\n",
      "[800]\ttraining's rmse: 3.49762\tvalid_1's rmse: 3.64779\n",
      "[900]\ttraining's rmse: 3.4849\tvalid_1's rmse: 3.64737\n",
      "[1000]\ttraining's rmse: 3.47326\tvalid_1's rmse: 3.64628\n",
      "[1100]\ttraining's rmse: 3.46204\tvalid_1's rmse: 3.64589\n",
      "[1200]\ttraining's rmse: 3.45237\tvalid_1's rmse: 3.64541\n",
      "[1300]\ttraining's rmse: 3.44239\tvalid_1's rmse: 3.64497\n",
      "[1400]\ttraining's rmse: 3.43328\tvalid_1's rmse: 3.64501\n",
      "[1500]\ttraining's rmse: 3.42397\tvalid_1's rmse: 3.64497\n",
      "[1600]\ttraining's rmse: 3.41515\tvalid_1's rmse: 3.64474\n",
      "[1700]\ttraining's rmse: 3.4061\tvalid_1's rmse: 3.64445\n",
      "[1800]\ttraining's rmse: 3.39719\tvalid_1's rmse: 3.64452\n",
      "[1900]\ttraining's rmse: 3.38907\tvalid_1's rmse: 3.64482\n",
      "[2000]\ttraining's rmse: 3.38\tvalid_1's rmse: 3.64448\n",
      "[2100]\ttraining's rmse: 3.37153\tvalid_1's rmse: 3.64463\n",
      "[2200]\ttraining's rmse: 3.36316\tvalid_1's rmse: 3.64451\n",
      "[2300]\ttraining's rmse: 3.35459\tvalid_1's rmse: 3.64449\n",
      "[2400]\ttraining's rmse: 3.34618\tvalid_1's rmse: 3.64383\n",
      "[2500]\ttraining's rmse: 3.33813\tvalid_1's rmse: 3.64386\n",
      "[2600]\ttraining's rmse: 3.33011\tvalid_1's rmse: 3.64371\n",
      "[2700]\ttraining's rmse: 3.3214\tvalid_1's rmse: 3.64354\n",
      "[2800]\ttraining's rmse: 3.313\tvalid_1's rmse: 3.64347\n",
      "[2900]\ttraining's rmse: 3.30499\tvalid_1's rmse: 3.64363\n",
      "[3000]\ttraining's rmse: 3.29678\tvalid_1's rmse: 3.64403\n",
      "[3100]\ttraining's rmse: 3.28876\tvalid_1's rmse: 3.64386\n",
      "Early stopping, best iteration is:\n",
      "[2781]\ttraining's rmse: 3.31474\tvalid_1's rmse: 3.64331\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.71195\tvalid_1's rmse: 3.73629\n",
      "[200]\ttraining's rmse: 3.64066\tvalid_1's rmse: 3.69185\n",
      "[300]\ttraining's rmse: 3.59672\tvalid_1's rmse: 3.67254\n",
      "[400]\ttraining's rmse: 3.5659\tvalid_1's rmse: 3.66287\n",
      "[500]\ttraining's rmse: 3.54237\tvalid_1's rmse: 3.65768\n",
      "[600]\ttraining's rmse: 3.52332\tvalid_1's rmse: 3.65455\n",
      "[700]\ttraining's rmse: 3.50766\tvalid_1's rmse: 3.652\n",
      "[800]\ttraining's rmse: 3.49442\tvalid_1's rmse: 3.65042\n",
      "[900]\ttraining's rmse: 3.48186\tvalid_1's rmse: 3.64916\n",
      "[1000]\ttraining's rmse: 3.4702\tvalid_1's rmse: 3.64843\n",
      "[1100]\ttraining's rmse: 3.45949\tvalid_1's rmse: 3.6479\n",
      "[1200]\ttraining's rmse: 3.4484\tvalid_1's rmse: 3.64705\n",
      "[1300]\ttraining's rmse: 3.43818\tvalid_1's rmse: 3.64641\n",
      "[1400]\ttraining's rmse: 3.42906\tvalid_1's rmse: 3.64598\n",
      "[1500]\ttraining's rmse: 3.41962\tvalid_1's rmse: 3.64573\n",
      "[1600]\ttraining's rmse: 3.41079\tvalid_1's rmse: 3.64582\n",
      "[1700]\ttraining's rmse: 3.40165\tvalid_1's rmse: 3.64541\n",
      "[1800]\ttraining's rmse: 3.39248\tvalid_1's rmse: 3.64537\n",
      "[1900]\ttraining's rmse: 3.383\tvalid_1's rmse: 3.64526\n",
      "[2000]\ttraining's rmse: 3.3744\tvalid_1's rmse: 3.64504\n",
      "[2100]\ttraining's rmse: 3.36581\tvalid_1's rmse: 3.6447\n",
      "[2200]\ttraining's rmse: 3.35655\tvalid_1's rmse: 3.64457\n",
      "[2300]\ttraining's rmse: 3.34825\tvalid_1's rmse: 3.64456\n",
      "[2400]\ttraining's rmse: 3.33961\tvalid_1's rmse: 3.64444\n",
      "[2500]\ttraining's rmse: 3.33075\tvalid_1's rmse: 3.6442\n",
      "[2600]\ttraining's rmse: 3.32176\tvalid_1's rmse: 3.64415\n",
      "[2700]\ttraining's rmse: 3.31359\tvalid_1's rmse: 3.64411\n",
      "[2800]\ttraining's rmse: 3.30556\tvalid_1's rmse: 3.64391\n",
      "[2900]\ttraining's rmse: 3.2965\tvalid_1's rmse: 3.64412\n",
      "Early stopping, best iteration is:\n",
      "[2572]\ttraining's rmse: 3.32417\tvalid_1's rmse: 3.64389\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.71195\tvalid_1's rmse: 3.73641\n",
      "[200]\ttraining's rmse: 3.64049\tvalid_1's rmse: 3.69533\n",
      "[300]\ttraining's rmse: 3.5965\tvalid_1's rmse: 3.67823\n",
      "[400]\ttraining's rmse: 3.56644\tvalid_1's rmse: 3.66929\n",
      "[500]\ttraining's rmse: 3.54395\tvalid_1's rmse: 3.66387\n",
      "[600]\ttraining's rmse: 3.52625\tvalid_1's rmse: 3.66033\n",
      "[700]\ttraining's rmse: 3.5108\tvalid_1's rmse: 3.65771\n",
      "[800]\ttraining's rmse: 3.49735\tvalid_1's rmse: 3.656\n",
      "[900]\ttraining's rmse: 3.48535\tvalid_1's rmse: 3.65464\n",
      "[1000]\ttraining's rmse: 3.47402\tvalid_1's rmse: 3.65408\n",
      "[1100]\ttraining's rmse: 3.4631\tvalid_1's rmse: 3.65373\n",
      "[1200]\ttraining's rmse: 3.45228\tvalid_1's rmse: 3.65307\n",
      "[1300]\ttraining's rmse: 3.44196\tvalid_1's rmse: 3.653\n",
      "[1400]\ttraining's rmse: 3.43191\tvalid_1's rmse: 3.65324\n",
      "[1500]\ttraining's rmse: 3.42208\tvalid_1's rmse: 3.65379\n",
      "[1600]\ttraining's rmse: 3.41277\tvalid_1's rmse: 3.65395\n",
      "Early stopping, best iteration is:\n",
      "[1278]\ttraining's rmse: 3.4444\tvalid_1's rmse: 3.65277\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.71566\tvalid_1's rmse: 3.71568\n",
      "[200]\ttraining's rmse: 3.64536\tvalid_1's rmse: 3.66803\n",
      "[300]\ttraining's rmse: 3.60186\tvalid_1's rmse: 3.64656\n",
      "[400]\ttraining's rmse: 3.57191\tvalid_1's rmse: 3.63415\n",
      "[500]\ttraining's rmse: 3.54955\tvalid_1's rmse: 3.62677\n",
      "[600]\ttraining's rmse: 3.53124\tvalid_1's rmse: 3.62224\n",
      "[700]\ttraining's rmse: 3.51643\tvalid_1's rmse: 3.61891\n",
      "[800]\ttraining's rmse: 3.50314\tvalid_1's rmse: 3.61664\n",
      "[900]\ttraining's rmse: 3.49144\tvalid_1's rmse: 3.61495\n",
      "[1000]\ttraining's rmse: 3.47989\tvalid_1's rmse: 3.61416\n",
      "[1100]\ttraining's rmse: 3.46955\tvalid_1's rmse: 3.61338\n",
      "[1200]\ttraining's rmse: 3.46046\tvalid_1's rmse: 3.61269\n",
      "[1300]\ttraining's rmse: 3.45054\tvalid_1's rmse: 3.61257\n",
      "[1400]\ttraining's rmse: 3.44075\tvalid_1's rmse: 3.61248\n",
      "[1500]\ttraining's rmse: 3.43196\tvalid_1's rmse: 3.61248\n",
      "[1600]\ttraining's rmse: 3.42358\tvalid_1's rmse: 3.61272\n",
      "[1700]\ttraining's rmse: 3.41512\tvalid_1's rmse: 3.61275\n",
      "[1800]\ttraining's rmse: 3.40672\tvalid_1's rmse: 3.61296\n",
      "[1900]\ttraining's rmse: 3.39842\tvalid_1's rmse: 3.61284\n",
      "Early stopping, best iteration is:\n",
      "[1509]\ttraining's rmse: 3.43134\tvalid_1's rmse: 3.61235\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.71088\tvalid_1's rmse: 3.7385\n",
      "[200]\ttraining's rmse: 3.63849\tvalid_1's rmse: 3.70403\n",
      "[300]\ttraining's rmse: 3.59451\tvalid_1's rmse: 3.69055\n",
      "[400]\ttraining's rmse: 3.56324\tvalid_1's rmse: 3.68554\n",
      "[500]\ttraining's rmse: 3.53963\tvalid_1's rmse: 3.6814\n",
      "[600]\ttraining's rmse: 3.52136\tvalid_1's rmse: 3.67976\n",
      "[700]\ttraining's rmse: 3.50607\tvalid_1's rmse: 3.67883\n",
      "[800]\ttraining's rmse: 3.49195\tvalid_1's rmse: 3.67851\n",
      "[900]\ttraining's rmse: 3.4789\tvalid_1's rmse: 3.67835\n",
      "[1000]\ttraining's rmse: 3.46735\tvalid_1's rmse: 3.67863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100]\ttraining's rmse: 3.45571\tvalid_1's rmse: 3.67902\n",
      "[1200]\ttraining's rmse: 3.44535\tvalid_1's rmse: 3.67924\n",
      "Early stopping, best iteration is:\n",
      "[870]\ttraining's rmse: 3.4829\tvalid_1's rmse: 3.67816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.643498400318252"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one model 3.644954699945824\n",
    "# last 3.6441864880195727\n",
    "df_train['outliers'] = 0\n",
    "df_train.loc[df_train['target'] < -30, 'outliers'] = 1\n",
    "obj=list(df_train.select_dtypes(include=['object']).columns)[2:]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb=LabelEncoder()\n",
    "for o in obj:\n",
    "    df_train[o]=lb.fit_transform(df_train[o]) \n",
    "df_train_columns = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','target','outliers','Unnamed: 0','Unnamed: 0_x']]\n",
    "param = {'objective': 'regression_l2', \n",
    "            'boosting_type': 'gbdt', \n",
    "            'n_jobs': 4, 'max_depth': 7, \n",
    "            'n_estimators': 20000, \n",
    "            'subsample_freq': 2, \n",
    "            'subsample_for_bin': 200000, \n",
    "            'min_data_per_group': 100, \n",
    "            'max_cat_to_onehot': 4, \n",
    "            'cat_l2': 10.0, \n",
    "            'cat_smooth': 10.0, \n",
    "            'max_cat_threshold': 32, \n",
    "            'metric_freq': 10, \n",
    "            'verbosity': -1, \n",
    "            'metric': 'rmse', \n",
    "            'colsample_bytree': 0.5, \n",
    "            'learning_rate': 0.0061033234451294376, \n",
    "            'min_child_samples': 80, \n",
    "            'min_child_weight': 100.0, \n",
    "            'min_split_gain': 1e-06, \n",
    "            'num_leaves': 47, \n",
    "            'reg_alpha': 10.0, \n",
    "            'reg_lambda': 10.0, \n",
    "            'subsample': 0.9}\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "rskf=StratifiedKFold(11,shuffle=True,random_state=4590)\n",
    "val_pr=np.zeros(len(df_train))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "#test_pr=np.zeros(len(df_test))\n",
    "for train_index,val_index in rskf.split(df_train,df_train['outliers'].values):\n",
    "    train_data=lgb.Dataset(df_train[df_train_columns].loc[train_index],label=df_train['target'].loc[train_index])\n",
    "    val_data=lgb.Dataset(df_train[df_train_columns].loc[val_index],label=df_train['target'].loc[val_index])\n",
    "    num_round = 10000\n",
    "    \n",
    "    pool=Pool(df_train[df_train_columns].loc[train_index],df_train['outliers'].loc[train_index])\n",
    "    val_pool=Pool(df_train[df_train_columns].loc[val_index],df_train['outliers'].loc[val_index])\n",
    "    model = CatBoostClassifier(iterations=1000,learning_rate=0.01,verbose=0, loss_function='CrossEntropy',eval_metric='AUC')\n",
    "    model.fit(pool,eval_set=val_pool,use_best_model=True,verbose_eval=False)\n",
    "    cat_pr=model.predict(df_train[df_train_columns].loc[val_index],prediction_type=\"Probability\")[:,1]\n",
    "    \n",
    "    model=lgb.train(param,train_data,num_round,valid_sets=[train_data,val_data],verbose_eval=100,early_stopping_rounds=400)\n",
    "    lgb_pr=model.predict(df_train[df_train_columns].loc[val_index],num_iteration=model.best_iteration)\n",
    "    prediction=pd.DataFrame({'cat_pr':cat_pr,'lgb_pr':lgb_pr,'target':df_train['target'].loc[val_index]})\n",
    "    thresholds = []\n",
    "    for thresh in np.arange(0.01, 1, 0.01):\n",
    "        thresh = np.round(thresh, 2)\n",
    "        pr=np.where(prediction['cat_pr']>thresh,-33.22,prediction['lgb_pr'])\n",
    "        res = np.sqrt(mean_squared_error(pr,prediction['target'].values))\n",
    "        thresholds.append([thresh, res])\n",
    "    thresholds.sort(key=lambda x: x[1], reverse=False)\n",
    "    best_thresh = thresholds[0][0]\n",
    "    \n",
    "    val_pr[val_index]=np.where(prediction['cat_pr']>best_thresh,-33.22,prediction['lgb_pr'])\n",
    "    \n",
    "np.sqrt(mean_squared_error(val_pr,df_train['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0442728\tvalid_1's binary_logloss: 0.0449508\n",
      "[200]\ttraining's binary_logloss: 0.0442359\tvalid_1's binary_logloss: 0.0449403\n",
      "Early stopping, best iteration is:\n",
      "[57]\ttraining's binary_logloss: 0.0442299\tvalid_1's binary_logloss: 0.0448892\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.71265\tvalid_1's rmse: 3.74979\n",
      "[200]\ttraining's rmse: 3.64397\tvalid_1's rmse: 3.70293\n",
      "[300]\ttraining's rmse: 3.59972\tvalid_1's rmse: 3.68022\n",
      "[400]\ttraining's rmse: 3.5689\tvalid_1's rmse: 3.66885\n",
      "[500]\ttraining's rmse: 3.54513\tvalid_1's rmse: 3.66168\n",
      "[600]\ttraining's rmse: 3.52562\tvalid_1's rmse: 3.65669\n",
      "[700]\ttraining's rmse: 3.50936\tvalid_1's rmse: 3.65363\n",
      "[800]\ttraining's rmse: 3.49437\tvalid_1's rmse: 3.65141\n",
      "[900]\ttraining's rmse: 3.48028\tvalid_1's rmse: 3.64934\n",
      "[1000]\ttraining's rmse: 3.46741\tvalid_1's rmse: 3.64762\n",
      "[1100]\ttraining's rmse: 3.45501\tvalid_1's rmse: 3.6463\n",
      "[1200]\ttraining's rmse: 3.44339\tvalid_1's rmse: 3.64535\n",
      "[1300]\ttraining's rmse: 3.43279\tvalid_1's rmse: 3.64467\n",
      "[1400]\ttraining's rmse: 3.42211\tvalid_1's rmse: 3.64411\n",
      "[1500]\ttraining's rmse: 3.4123\tvalid_1's rmse: 3.64362\n",
      "[1600]\ttraining's rmse: 3.40261\tvalid_1's rmse: 3.64348\n",
      "[1700]\ttraining's rmse: 3.39359\tvalid_1's rmse: 3.64326\n",
      "[1800]\ttraining's rmse: 3.38444\tvalid_1's rmse: 3.64281\n",
      "[1900]\ttraining's rmse: 3.37467\tvalid_1's rmse: 3.64274\n",
      "[2000]\ttraining's rmse: 3.36545\tvalid_1's rmse: 3.64258\n",
      "[2100]\ttraining's rmse: 3.35625\tvalid_1's rmse: 3.64253\n",
      "[2200]\ttraining's rmse: 3.34793\tvalid_1's rmse: 3.64263\n",
      "[2300]\ttraining's rmse: 3.3389\tvalid_1's rmse: 3.64258\n",
      "[2400]\ttraining's rmse: 3.32967\tvalid_1's rmse: 3.64261\n",
      "[2500]\ttraining's rmse: 3.32096\tvalid_1's rmse: 3.64257\n",
      "[2600]\ttraining's rmse: 3.31171\tvalid_1's rmse: 3.64277\n",
      "[2700]\ttraining's rmse: 3.30264\tvalid_1's rmse: 3.64265\n",
      "Early stopping, best iteration is:\n",
      "[2316]\ttraining's rmse: 3.33722\tvalid_1's rmse: 3.64246\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0440996\tvalid_1's binary_logloss: 0.045602\n",
      "[200]\ttraining's binary_logloss: 0.0441098\tvalid_1's binary_logloss: 0.0456196\n",
      "Early stopping, best iteration is:\n",
      "[97]\ttraining's binary_logloss: 0.0440938\tvalid_1's binary_logloss: 0.0455972\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.7149\tvalid_1's rmse: 3.73871\n",
      "[200]\ttraining's rmse: 3.64528\tvalid_1's rmse: 3.69456\n",
      "[300]\ttraining's rmse: 3.60092\tvalid_1's rmse: 3.67403\n",
      "[400]\ttraining's rmse: 3.56928\tvalid_1's rmse: 3.66297\n",
      "[500]\ttraining's rmse: 3.54484\tvalid_1's rmse: 3.65661\n",
      "[600]\ttraining's rmse: 3.52542\tvalid_1's rmse: 3.6534\n",
      "[700]\ttraining's rmse: 3.50812\tvalid_1's rmse: 3.65117\n",
      "[800]\ttraining's rmse: 3.49303\tvalid_1's rmse: 3.6496\n",
      "[900]\ttraining's rmse: 3.47906\tvalid_1's rmse: 3.64808\n",
      "[1000]\ttraining's rmse: 3.4656\tvalid_1's rmse: 3.64741\n",
      "[1100]\ttraining's rmse: 3.45343\tvalid_1's rmse: 3.64668\n",
      "[1200]\ttraining's rmse: 3.44189\tvalid_1's rmse: 3.64619\n",
      "[1300]\ttraining's rmse: 3.43048\tvalid_1's rmse: 3.64594\n",
      "[1400]\ttraining's rmse: 3.42017\tvalid_1's rmse: 3.64547\n",
      "[1500]\ttraining's rmse: 3.40928\tvalid_1's rmse: 3.64519\n",
      "[1600]\ttraining's rmse: 3.39898\tvalid_1's rmse: 3.64498\n",
      "[1700]\ttraining's rmse: 3.38817\tvalid_1's rmse: 3.64459\n",
      "[1800]\ttraining's rmse: 3.37842\tvalid_1's rmse: 3.64465\n",
      "[1900]\ttraining's rmse: 3.36901\tvalid_1's rmse: 3.64452\n",
      "[2000]\ttraining's rmse: 3.35987\tvalid_1's rmse: 3.64421\n",
      "[2100]\ttraining's rmse: 3.3497\tvalid_1's rmse: 3.64375\n",
      "[2200]\ttraining's rmse: 3.34018\tvalid_1's rmse: 3.64365\n",
      "[2300]\ttraining's rmse: 3.33069\tvalid_1's rmse: 3.64359\n",
      "[2400]\ttraining's rmse: 3.32107\tvalid_1's rmse: 3.64342\n",
      "[2500]\ttraining's rmse: 3.31171\tvalid_1's rmse: 3.64349\n",
      "[2600]\ttraining's rmse: 3.30218\tvalid_1's rmse: 3.64346\n",
      "[2700]\ttraining's rmse: 3.2937\tvalid_1's rmse: 3.64314\n",
      "[2800]\ttraining's rmse: 3.28487\tvalid_1's rmse: 3.64319\n",
      "[2900]\ttraining's rmse: 3.27608\tvalid_1's rmse: 3.64321\n",
      "[3000]\ttraining's rmse: 3.26657\tvalid_1's rmse: 3.64312\n",
      "[3100]\ttraining's rmse: 3.25779\tvalid_1's rmse: 3.64309\n",
      "[3200]\ttraining's rmse: 3.24976\tvalid_1's rmse: 3.64303\n",
      "[3300]\ttraining's rmse: 3.24066\tvalid_1's rmse: 3.64315\n",
      "[3400]\ttraining's rmse: 3.23245\tvalid_1's rmse: 3.64319\n",
      "[3500]\ttraining's rmse: 3.22441\tvalid_1's rmse: 3.64318\n",
      "[3600]\ttraining's rmse: 3.21607\tvalid_1's rmse: 3.64332\n",
      "Early stopping, best iteration is:\n",
      "[3206]\ttraining's rmse: 3.24924\tvalid_1's rmse: 3.64298\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0441481\tvalid_1's binary_logloss: 0.0458493\n",
      "[200]\ttraining's binary_logloss: 0.0441149\tvalid_1's binary_logloss: 0.0458006\n",
      "Early stopping, best iteration is:\n",
      "[35]\ttraining's binary_logloss: 0.0441447\tvalid_1's binary_logloss: 0.0457813\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.71218\tvalid_1's rmse: 3.74651\n",
      "[200]\ttraining's rmse: 3.64253\tvalid_1's rmse: 3.70357\n",
      "[300]\ttraining's rmse: 3.59769\tvalid_1's rmse: 3.68305\n",
      "[400]\ttraining's rmse: 3.56665\tvalid_1's rmse: 3.67178\n",
      "[500]\ttraining's rmse: 3.54248\tvalid_1's rmse: 3.66442\n",
      "[600]\ttraining's rmse: 3.52258\tvalid_1's rmse: 3.66001\n",
      "[700]\ttraining's rmse: 3.506\tvalid_1's rmse: 3.65743\n",
      "[800]\ttraining's rmse: 3.49024\tvalid_1's rmse: 3.65573\n",
      "[900]\ttraining's rmse: 3.47561\tvalid_1's rmse: 3.65428\n",
      "[1000]\ttraining's rmse: 3.46216\tvalid_1's rmse: 3.65347\n",
      "[1100]\ttraining's rmse: 3.44977\tvalid_1's rmse: 3.65285\n",
      "[1200]\ttraining's rmse: 3.43847\tvalid_1's rmse: 3.6521\n",
      "[1300]\ttraining's rmse: 3.42829\tvalid_1's rmse: 3.6514\n",
      "[1400]\ttraining's rmse: 3.41797\tvalid_1's rmse: 3.65117\n",
      "[1500]\ttraining's rmse: 3.40766\tvalid_1's rmse: 3.6507\n",
      "[1600]\ttraining's rmse: 3.3982\tvalid_1's rmse: 3.65036\n",
      "[1700]\ttraining's rmse: 3.38801\tvalid_1's rmse: 3.65004\n",
      "[1800]\ttraining's rmse: 3.37836\tvalid_1's rmse: 3.64978\n",
      "[1900]\ttraining's rmse: 3.36896\tvalid_1's rmse: 3.64948\n",
      "[2000]\ttraining's rmse: 3.35998\tvalid_1's rmse: 3.64921\n",
      "[2100]\ttraining's rmse: 3.35122\tvalid_1's rmse: 3.64888\n",
      "[2200]\ttraining's rmse: 3.34246\tvalid_1's rmse: 3.64871\n",
      "[2300]\ttraining's rmse: 3.33385\tvalid_1's rmse: 3.64878\n",
      "[2400]\ttraining's rmse: 3.32502\tvalid_1's rmse: 3.64865\n",
      "[2500]\ttraining's rmse: 3.31574\tvalid_1's rmse: 3.64873\n",
      "[2600]\ttraining's rmse: 3.30688\tvalid_1's rmse: 3.64871\n",
      "[2700]\ttraining's rmse: 3.29843\tvalid_1's rmse: 3.64886\n",
      "Early stopping, best iteration is:\n",
      "[2380]\ttraining's rmse: 3.32706\tvalid_1's rmse: 3.6486\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0439065\tvalid_1's binary_logloss: 0.0462854\n",
      "[200]\ttraining's binary_logloss: 0.043906\tvalid_1's binary_logloss: 0.0462793\n",
      "Early stopping, best iteration is:\n",
      "[75]\ttraining's binary_logloss: 0.0438792\tvalid_1's binary_logloss: 0.0462291\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.71218\tvalid_1's rmse: 3.74349\n",
      "[200]\ttraining's rmse: 3.64085\tvalid_1's rmse: 3.7018\n",
      "[300]\ttraining's rmse: 3.59511\tvalid_1's rmse: 3.68316\n",
      "[400]\ttraining's rmse: 3.56259\tvalid_1's rmse: 3.67386\n",
      "[500]\ttraining's rmse: 3.53773\tvalid_1's rmse: 3.66814\n",
      "[600]\ttraining's rmse: 3.51789\tvalid_1's rmse: 3.66455\n",
      "[700]\ttraining's rmse: 3.50081\tvalid_1's rmse: 3.66232\n",
      "[800]\ttraining's rmse: 3.4857\tvalid_1's rmse: 3.66039\n",
      "[900]\ttraining's rmse: 3.47143\tvalid_1's rmse: 3.65876\n",
      "[1000]\ttraining's rmse: 3.45885\tvalid_1's rmse: 3.65773\n",
      "[1100]\ttraining's rmse: 3.44714\tvalid_1's rmse: 3.65721\n",
      "[1200]\ttraining's rmse: 3.4355\tvalid_1's rmse: 3.65662\n",
      "[1300]\ttraining's rmse: 3.42547\tvalid_1's rmse: 3.65605\n",
      "[1400]\ttraining's rmse: 3.41421\tvalid_1's rmse: 3.65576\n",
      "[1500]\ttraining's rmse: 3.40391\tvalid_1's rmse: 3.65564\n",
      "[1600]\ttraining's rmse: 3.39425\tvalid_1's rmse: 3.65544\n",
      "[1700]\ttraining's rmse: 3.38461\tvalid_1's rmse: 3.65531\n",
      "[1800]\ttraining's rmse: 3.37604\tvalid_1's rmse: 3.65514\n",
      "[1900]\ttraining's rmse: 3.36653\tvalid_1's rmse: 3.65514\n",
      "[2000]\ttraining's rmse: 3.357\tvalid_1's rmse: 3.6549\n",
      "[2100]\ttraining's rmse: 3.34753\tvalid_1's rmse: 3.65515\n",
      "[2200]\ttraining's rmse: 3.33741\tvalid_1's rmse: 3.65534\n",
      "[2300]\ttraining's rmse: 3.32872\tvalid_1's rmse: 3.65546\n",
      "Early stopping, best iteration is:\n",
      "[1985]\ttraining's rmse: 3.3586\tvalid_1's rmse: 3.65482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0434539\tvalid_1's binary_logloss: 0.0473659\n",
      "[200]\ttraining's binary_logloss: 0.0434244\tvalid_1's binary_logloss: 0.0473453\n",
      "[300]\ttraining's binary_logloss: 0.0434177\tvalid_1's binary_logloss: 0.0473318\n",
      "[400]\ttraining's binary_logloss: 0.0433987\tvalid_1's binary_logloss: 0.0473091\n",
      "[500]\ttraining's binary_logloss: 0.0434052\tvalid_1's binary_logloss: 0.0473238\n",
      "Early stopping, best iteration is:\n",
      "[392]\ttraining's binary_logloss: 0.0433992\tvalid_1's binary_logloss: 0.0473049\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 3.71504\tvalid_1's rmse: 3.73092\n",
      "[200]\ttraining's rmse: 3.64335\tvalid_1's rmse: 3.6914\n",
      "[300]\ttraining's rmse: 3.59728\tvalid_1's rmse: 3.67346\n",
      "[400]\ttraining's rmse: 3.56458\tvalid_1's rmse: 3.66518\n",
      "[500]\ttraining's rmse: 3.53994\tvalid_1's rmse: 3.65998\n",
      "[600]\ttraining's rmse: 3.52098\tvalid_1's rmse: 3.65657\n",
      "[700]\ttraining's rmse: 3.50427\tvalid_1's rmse: 3.65476\n",
      "[800]\ttraining's rmse: 3.48909\tvalid_1's rmse: 3.6537\n",
      "[900]\ttraining's rmse: 3.47491\tvalid_1's rmse: 3.6527\n",
      "[1000]\ttraining's rmse: 3.46228\tvalid_1's rmse: 3.65239\n",
      "[1100]\ttraining's rmse: 3.45001\tvalid_1's rmse: 3.65229\n",
      "[1200]\ttraining's rmse: 3.4386\tvalid_1's rmse: 3.65217\n",
      "[1300]\ttraining's rmse: 3.42747\tvalid_1's rmse: 3.65243\n",
      "[1400]\ttraining's rmse: 3.41705\tvalid_1's rmse: 3.65239\n",
      "[1500]\ttraining's rmse: 3.40635\tvalid_1's rmse: 3.6524\n",
      "Early stopping, best iteration is:\n",
      "[1164]\ttraining's rmse: 3.44234\tvalid_1's rmse: 3.65197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.6472082581786145"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best score with catboost feature 3.643498400318252\n",
    "#df_train['outliers'] = 0\n",
    "#df_train.loc[df_train['target'] < -30, 'outliers'] = 1\n",
    "best_pr=val_pr.copy()\n",
    "obj=list(df_train.select_dtypes(include=['object']).columns)[2:]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb=LabelEncoder()\n",
    "for o in obj:\n",
    "    df_train[o]=lb.fit_transform(df_train[o]) \n",
    "df_train_columns = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','target','outliers','Unnamed: 0']]\n",
    "param = {'objective': 'regression_l2', \n",
    "    'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt', \n",
    "            'n_jobs': 4, 'max_depth': 18, \n",
    "            'n_estimators': 6100, \n",
    "            'subsample_freq': 2, \n",
    "            'min_data_per_group': 100, \n",
    "            'max_cat_to_onehot': 4, \n",
    "            'cat_l2': 10.0, \n",
    "            'cat_smooth': 10.0, \n",
    "            'max_cat_threshold': 32, \n",
    "            'metric_freq': 10, \n",
    "            'verbosity': -1, \n",
    "            'colsample_bytree': 0.5, \n",
    "            'learning_rate': 0.0061033234451294376, \n",
    "            'min_child_samples': 20, \n",
    "            'min_child_weight': 9.0, \n",
    "            'min_split_gain': 1e-06, \n",
    "            'num_leaves': 36, \n",
    "            'reg_alpha': 40.0, \n",
    "            'reg_lambda': 13.3, \n",
    "            'subsample': 0.9}\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "rskf=StratifiedKFold(11,shuffle=True,random_state=4590)\n",
    "val_pr=np.zeros(len(df_train))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "#test_pr=np.zeros(len(df_test))\n",
    "for train_index,val_index in rskf.split(df_train,df_train['outliers'].values):\n",
    "    num_round = 10000\n",
    "    train_data=lgb.Dataset(df_train[df_train_columns].loc[train_index],label=df_train['outliers'].loc[train_index])\n",
    "    val_data=lgb.Dataset(df_train[df_train_columns].loc[val_index],label=df_train['outliers'].loc[val_index])\n",
    "    params={\n",
    "        'num_leaves': 31,\n",
    "         'n_jobs': 4,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'binary',\n",
    "         'max_depth': 6,\n",
    "         'learning_rate': 0.01,\n",
    "         \"boosting\": \"rf\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'binary_logloss',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"random_state\": 2333\n",
    "    }\n",
    "    model=lgb.train(params,train_data,num_round,valid_sets=[train_data,val_data],verbose_eval=100,early_stopping_rounds=200)\n",
    "    cat_pr=model.predict(df_train[df_train_columns].loc[val_index],num_iteration=model.best_iteration)\n",
    "    \n",
    "    train_data=lgb.Dataset(df_train[df_train_columns].loc[train_index],label=df_train['target'].loc[train_index])\n",
    "    val_data=lgb.Dataset(df_train[df_train_columns].loc[val_index],label=df_train['target'].loc[val_index])\n",
    "    model=lgb.train(param,train_data,num_round,valid_sets=[train_data,val_data],verbose_eval=100,early_stopping_rounds=400)\n",
    "    lgb_pr=model.predict(df_train[df_train_columns].loc[val_index],num_iteration=model.best_iteration)\n",
    "    prediction=pd.DataFrame({'cat_pr':cat_pr,'lgb_pr':lgb_pr,'target':df_train['target'].loc[val_index]})\n",
    "    thresholds = []\n",
    "    for thresh in np.arange(0.01, 1, 0.01):\n",
    "        thresh = np.round(thresh, 2)\n",
    "        pr=np.where(prediction['cat_pr']>thresh,-33.22,prediction['lgb_pr'])\n",
    "        res = np.sqrt(mean_squared_error(pr,prediction['target'].values))\n",
    "        thresholds.append([thresh, res])\n",
    "    thresholds.sort(key=lambda x: x[1], reverse=False)\n",
    "    best_thresh = thresholds[0][0]\n",
    "    \n",
    "    val_pr[val_index]=np.where(prediction['cat_pr']>best_thresh,-33.22,prediction['lgb_pr'])\n",
    "    \n",
    "np.sqrt(mean_squared_error(val_pr,df_train['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_train['hist_purchase_amount_sum']\n",
    "#df_train['hist_purchase_date_uptonow']\n",
    "#df_train['hist_card_id_size']\n",
    "best_pr_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.045606\tvalid_1's binary_logloss: 0.0470904\n",
      "[200]\ttraining's binary_logloss: 0.0426354\tvalid_1's binary_logloss: 0.0449755\n",
      "[300]\ttraining's binary_logloss: 0.0410832\tvalid_1's binary_logloss: 0.0442041\n",
      "[400]\ttraining's binary_logloss: 0.0399629\tvalid_1's binary_logloss: 0.0438532\n",
      "[500]\ttraining's binary_logloss: 0.0390839\tvalid_1's binary_logloss: 0.0436931\n",
      "[600]\ttraining's binary_logloss: 0.0383157\tvalid_1's binary_logloss: 0.0436038\n",
      "[700]\ttraining's binary_logloss: 0.0376569\tvalid_1's binary_logloss: 0.0435639\n",
      "[800]\ttraining's binary_logloss: 0.0370513\tvalid_1's binary_logloss: 0.0435445\n",
      "[900]\ttraining's binary_logloss: 0.0365494\tvalid_1's binary_logloss: 0.0435432\n",
      "[1000]\ttraining's binary_logloss: 0.0361064\tvalid_1's binary_logloss: 0.0435355\n",
      "[1100]\ttraining's binary_logloss: 0.0356762\tvalid_1's binary_logloss: 0.0435396\n",
      "[1200]\ttraining's binary_logloss: 0.0352959\tvalid_1's binary_logloss: 0.0435357\n",
      "[1300]\ttraining's binary_logloss: 0.0349647\tvalid_1's binary_logloss: 0.0435247\n",
      "[1400]\ttraining's binary_logloss: 0.0346411\tvalid_1's binary_logloss: 0.0435209\n",
      "[1500]\ttraining's binary_logloss: 0.0343645\tvalid_1's binary_logloss: 0.0435214\n",
      "[1600]\ttraining's binary_logloss: 0.0340553\tvalid_1's binary_logloss: 0.043528\n",
      "[1700]\ttraining's binary_logloss: 0.0338279\tvalid_1's binary_logloss: 0.043545\n",
      "[1800]\ttraining's binary_logloss: 0.0338279\tvalid_1's binary_logloss: 0.043545\n",
      "[1900]\ttraining's binary_logloss: 0.0338279\tvalid_1's binary_logloss: 0.043545\n",
      "Early stopping, best iteration is:\n",
      "[1509]\ttraining's binary_logloss: 0.0343355\tvalid_1's binary_logloss: 0.0435172\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 1.62678\tvalid_1's rmse: 1.65541\n",
      "[200]\ttraining's rmse: 1.58995\tvalid_1's rmse: 1.62204\n",
      "[300]\ttraining's rmse: 1.57091\tvalid_1's rmse: 1.60608\n",
      "[400]\ttraining's rmse: 1.55918\tvalid_1's rmse: 1.59718\n",
      "[500]\ttraining's rmse: 1.55071\tvalid_1's rmse: 1.59142\n",
      "[600]\ttraining's rmse: 1.54405\tvalid_1's rmse: 1.58731\n",
      "[700]\ttraining's rmse: 1.53848\tvalid_1's rmse: 1.58434\n",
      "[800]\ttraining's rmse: 1.53364\tvalid_1's rmse: 1.58207\n",
      "[900]\ttraining's rmse: 1.52932\tvalid_1's rmse: 1.58028\n",
      "[1000]\ttraining's rmse: 1.52545\tvalid_1's rmse: 1.57895\n",
      "[1100]\ttraining's rmse: 1.52183\tvalid_1's rmse: 1.57792\n",
      "[1200]\ttraining's rmse: 1.51839\tvalid_1's rmse: 1.57708\n",
      "[1300]\ttraining's rmse: 1.51517\tvalid_1's rmse: 1.57634\n",
      "[1400]\ttraining's rmse: 1.51207\tvalid_1's rmse: 1.57576\n",
      "[1500]\ttraining's rmse: 1.50913\tvalid_1's rmse: 1.57532\n",
      "[1600]\ttraining's rmse: 1.50629\tvalid_1's rmse: 1.57496\n",
      "[1700]\ttraining's rmse: 1.50352\tvalid_1's rmse: 1.57466\n",
      "[1800]\ttraining's rmse: 1.50086\tvalid_1's rmse: 1.57431\n",
      "[1900]\ttraining's rmse: 1.49826\tvalid_1's rmse: 1.57404\n",
      "[2000]\ttraining's rmse: 1.49574\tvalid_1's rmse: 1.57386\n",
      "[2100]\ttraining's rmse: 1.49328\tvalid_1's rmse: 1.57371\n",
      "[2200]\ttraining's rmse: 1.49092\tvalid_1's rmse: 1.57354\n",
      "[2300]\ttraining's rmse: 1.48857\tvalid_1's rmse: 1.57342\n",
      "[2400]\ttraining's rmse: 1.48623\tvalid_1's rmse: 1.57328\n",
      "[2500]\ttraining's rmse: 1.48394\tvalid_1's rmse: 1.57316\n",
      "[2600]\ttraining's rmse: 1.48171\tvalid_1's rmse: 1.5731\n",
      "[2700]\ttraining's rmse: 1.47954\tvalid_1's rmse: 1.57301\n",
      "[2800]\ttraining's rmse: 1.47732\tvalid_1's rmse: 1.57291\n",
      "[2900]\ttraining's rmse: 1.47512\tvalid_1's rmse: 1.57281\n",
      "[3000]\ttraining's rmse: 1.47298\tvalid_1's rmse: 1.57275\n",
      "[3100]\ttraining's rmse: 1.47086\tvalid_1's rmse: 1.57268\n",
      "[3200]\ttraining's rmse: 1.46874\tvalid_1's rmse: 1.57264\n",
      "[3300]\ttraining's rmse: 1.4667\tvalid_1's rmse: 1.57259\n",
      "[3400]\ttraining's rmse: 1.46462\tvalid_1's rmse: 1.57257\n",
      "[3500]\ttraining's rmse: 1.4625\tvalid_1's rmse: 1.5725\n",
      "[3600]\ttraining's rmse: 1.46044\tvalid_1's rmse: 1.57241\n",
      "[3700]\ttraining's rmse: 1.45838\tvalid_1's rmse: 1.57232\n",
      "[3800]\ttraining's rmse: 1.45634\tvalid_1's rmse: 1.57223\n",
      "[3900]\ttraining's rmse: 1.45432\tvalid_1's rmse: 1.57218\n",
      "[4000]\ttraining's rmse: 1.45234\tvalid_1's rmse: 1.57216\n",
      "[4100]\ttraining's rmse: 1.45041\tvalid_1's rmse: 1.5721\n",
      "[4200]\ttraining's rmse: 1.44839\tvalid_1's rmse: 1.57211\n",
      "[4300]\ttraining's rmse: 1.44642\tvalid_1's rmse: 1.57214\n",
      "[4400]\ttraining's rmse: 1.44445\tvalid_1's rmse: 1.57212\n",
      "[4500]\ttraining's rmse: 1.4425\tvalid_1's rmse: 1.57206\n",
      "[4600]\ttraining's rmse: 1.44058\tvalid_1's rmse: 1.57209\n",
      "[4700]\ttraining's rmse: 1.43866\tvalid_1's rmse: 1.57205\n",
      "[4800]\ttraining's rmse: 1.43679\tvalid_1's rmse: 1.57202\n",
      "[4900]\ttraining's rmse: 1.43488\tvalid_1's rmse: 1.57205\n",
      "[5000]\ttraining's rmse: 1.43304\tvalid_1's rmse: 1.57203\n",
      "[5100]\ttraining's rmse: 1.43121\tvalid_1's rmse: 1.57194\n",
      "[5200]\ttraining's rmse: 1.4293\tvalid_1's rmse: 1.57195\n",
      "[5300]\ttraining's rmse: 1.42748\tvalid_1's rmse: 1.57191\n",
      "[5400]\ttraining's rmse: 1.42566\tvalid_1's rmse: 1.5719\n",
      "[5500]\ttraining's rmse: 1.42385\tvalid_1's rmse: 1.5719\n",
      "[5600]\ttraining's rmse: 1.42208\tvalid_1's rmse: 1.57191\n",
      "[5700]\ttraining's rmse: 1.42031\tvalid_1's rmse: 1.57192\n",
      "[5800]\ttraining's rmse: 1.41851\tvalid_1's rmse: 1.57192\n",
      "Early stopping, best iteration is:\n",
      "[5453]\ttraining's rmse: 1.4247\tvalid_1's rmse: 1.57188\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0453769\tvalid_1's binary_logloss: 0.0475455\n",
      "[200]\ttraining's binary_logloss: 0.0424635\tvalid_1's binary_logloss: 0.0455167\n",
      "[300]\ttraining's binary_logloss: 0.0409124\tvalid_1's binary_logloss: 0.0448497\n",
      "[400]\ttraining's binary_logloss: 0.0397998\tvalid_1's binary_logloss: 0.0445778\n",
      "[500]\ttraining's binary_logloss: 0.038911\tvalid_1's binary_logloss: 0.0444672\n",
      "[600]\ttraining's binary_logloss: 0.0381387\tvalid_1's binary_logloss: 0.0443927\n",
      "[700]\ttraining's binary_logloss: 0.0374317\tvalid_1's binary_logloss: 0.044359\n",
      "[800]\ttraining's binary_logloss: 0.0368111\tvalid_1's binary_logloss: 0.0443501\n",
      "[900]\ttraining's binary_logloss: 0.0362807\tvalid_1's binary_logloss: 0.0443578\n",
      "[1000]\ttraining's binary_logloss: 0.0357661\tvalid_1's binary_logloss: 0.0443601\n",
      "[1100]\ttraining's binary_logloss: 0.0353132\tvalid_1's binary_logloss: 0.0443765\n",
      "[1200]\ttraining's binary_logloss: 0.0349285\tvalid_1's binary_logloss: 0.0443886\n",
      "Early stopping, best iteration is:\n",
      "[811]\ttraining's binary_logloss: 0.0367511\tvalid_1's binary_logloss: 0.0443442\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 1.63345\tvalid_1's rmse: 1.62799\n",
      "[200]\ttraining's rmse: 1.59645\tvalid_1's rmse: 1.59494\n",
      "[300]\ttraining's rmse: 1.57724\tvalid_1's rmse: 1.57951\n",
      "[400]\ttraining's rmse: 1.56518\tvalid_1's rmse: 1.57112\n",
      "[500]\ttraining's rmse: 1.55653\tvalid_1's rmse: 1.56585\n",
      "[600]\ttraining's rmse: 1.54969\tvalid_1's rmse: 1.5622\n",
      "[700]\ttraining's rmse: 1.54413\tvalid_1's rmse: 1.55961\n",
      "[800]\ttraining's rmse: 1.53931\tvalid_1's rmse: 1.55763\n",
      "[900]\ttraining's rmse: 1.53497\tvalid_1's rmse: 1.55608\n",
      "[1000]\ttraining's rmse: 1.53096\tvalid_1's rmse: 1.5548\n",
      "[1100]\ttraining's rmse: 1.52734\tvalid_1's rmse: 1.55383\n",
      "[1200]\ttraining's rmse: 1.52392\tvalid_1's rmse: 1.55307\n",
      "[1300]\ttraining's rmse: 1.5207\tvalid_1's rmse: 1.55242\n",
      "[1400]\ttraining's rmse: 1.51764\tvalid_1's rmse: 1.55192\n",
      "[1500]\ttraining's rmse: 1.51468\tvalid_1's rmse: 1.55152\n",
      "[1600]\ttraining's rmse: 1.51183\tvalid_1's rmse: 1.55112\n",
      "[1700]\ttraining's rmse: 1.50908\tvalid_1's rmse: 1.55079\n",
      "[1800]\ttraining's rmse: 1.50643\tvalid_1's rmse: 1.55053\n",
      "[1900]\ttraining's rmse: 1.50387\tvalid_1's rmse: 1.55029\n",
      "[2000]\ttraining's rmse: 1.50135\tvalid_1's rmse: 1.55008\n",
      "[2100]\ttraining's rmse: 1.49893\tvalid_1's rmse: 1.54989\n",
      "[2200]\ttraining's rmse: 1.49652\tvalid_1's rmse: 1.54977\n",
      "[2300]\ttraining's rmse: 1.49415\tvalid_1's rmse: 1.54965\n",
      "[2400]\ttraining's rmse: 1.4918\tvalid_1's rmse: 1.5495\n",
      "[2500]\ttraining's rmse: 1.48946\tvalid_1's rmse: 1.54941\n",
      "[2600]\ttraining's rmse: 1.48721\tvalid_1's rmse: 1.54934\n",
      "[2700]\ttraining's rmse: 1.48499\tvalid_1's rmse: 1.54922\n",
      "[2800]\ttraining's rmse: 1.48282\tvalid_1's rmse: 1.54913\n",
      "[2900]\ttraining's rmse: 1.48066\tvalid_1's rmse: 1.54905\n",
      "[3000]\ttraining's rmse: 1.47851\tvalid_1's rmse: 1.54898\n",
      "[3100]\ttraining's rmse: 1.47637\tvalid_1's rmse: 1.54891\n",
      "[3200]\ttraining's rmse: 1.47426\tvalid_1's rmse: 1.54888\n",
      "[3300]\ttraining's rmse: 1.47216\tvalid_1's rmse: 1.54884\n",
      "[3400]\ttraining's rmse: 1.47001\tvalid_1's rmse: 1.54875\n",
      "[3500]\ttraining's rmse: 1.46798\tvalid_1's rmse: 1.54871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3600]\ttraining's rmse: 1.46594\tvalid_1's rmse: 1.54864\n",
      "[3700]\ttraining's rmse: 1.4639\tvalid_1's rmse: 1.54862\n",
      "[3800]\ttraining's rmse: 1.46192\tvalid_1's rmse: 1.5486\n",
      "[3900]\ttraining's rmse: 1.45991\tvalid_1's rmse: 1.54856\n",
      "[4000]\ttraining's rmse: 1.45795\tvalid_1's rmse: 1.54856\n",
      "[4100]\ttraining's rmse: 1.45597\tvalid_1's rmse: 1.54853\n",
      "[4200]\ttraining's rmse: 1.45398\tvalid_1's rmse: 1.54849\n",
      "[4300]\ttraining's rmse: 1.45207\tvalid_1's rmse: 1.54851\n",
      "[4400]\ttraining's rmse: 1.45009\tvalid_1's rmse: 1.54847\n",
      "[4500]\ttraining's rmse: 1.44815\tvalid_1's rmse: 1.54844\n",
      "[4600]\ttraining's rmse: 1.44617\tvalid_1's rmse: 1.54844\n",
      "[4700]\ttraining's rmse: 1.44421\tvalid_1's rmse: 1.54845\n",
      "[4800]\ttraining's rmse: 1.44232\tvalid_1's rmse: 1.54844\n",
      "[4900]\ttraining's rmse: 1.44042\tvalid_1's rmse: 1.54843\n",
      "[5000]\ttraining's rmse: 1.43852\tvalid_1's rmse: 1.54839\n",
      "[5100]\ttraining's rmse: 1.43662\tvalid_1's rmse: 1.54839\n",
      "[5200]\ttraining's rmse: 1.43479\tvalid_1's rmse: 1.54834\n",
      "[5300]\ttraining's rmse: 1.43286\tvalid_1's rmse: 1.54837\n",
      "[5400]\ttraining's rmse: 1.43102\tvalid_1's rmse: 1.5484\n",
      "[5500]\ttraining's rmse: 1.42918\tvalid_1's rmse: 1.54839\n",
      "Early stopping, best iteration is:\n",
      "[5189]\ttraining's rmse: 1.435\tvalid_1's rmse: 1.54833\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0454432\tvalid_1's binary_logloss: 0.0474394\n",
      "[200]\ttraining's binary_logloss: 0.0425619\tvalid_1's binary_logloss: 0.045312\n",
      "[300]\ttraining's binary_logloss: 0.040998\tvalid_1's binary_logloss: 0.04451\n",
      "[400]\ttraining's binary_logloss: 0.0398704\tvalid_1's binary_logloss: 0.0441738\n",
      "[500]\ttraining's binary_logloss: 0.0390032\tvalid_1's binary_logloss: 0.0440127\n",
      "[600]\ttraining's binary_logloss: 0.0382592\tvalid_1's binary_logloss: 0.0439376\n",
      "[700]\ttraining's binary_logloss: 0.0375912\tvalid_1's binary_logloss: 0.043879\n",
      "[800]\ttraining's binary_logloss: 0.0370168\tvalid_1's binary_logloss: 0.0438516\n",
      "[900]\ttraining's binary_logloss: 0.0364815\tvalid_1's binary_logloss: 0.0438281\n",
      "[1000]\ttraining's binary_logloss: 0.035976\tvalid_1's binary_logloss: 0.0438259\n",
      "[1100]\ttraining's binary_logloss: 0.0355521\tvalid_1's binary_logloss: 0.0438268\n",
      "[1200]\ttraining's binary_logloss: 0.0351629\tvalid_1's binary_logloss: 0.0438228\n",
      "[1300]\ttraining's binary_logloss: 0.0348301\tvalid_1's binary_logloss: 0.0438123\n",
      "[1400]\ttraining's binary_logloss: 0.0345278\tvalid_1's binary_logloss: 0.0438163\n",
      "[1500]\ttraining's binary_logloss: 0.0342395\tvalid_1's binary_logloss: 0.0438267\n",
      "[1600]\ttraining's binary_logloss: 0.0340019\tvalid_1's binary_logloss: 0.0438238\n",
      "[1700]\ttraining's binary_logloss: 0.0340019\tvalid_1's binary_logloss: 0.0438238\n",
      "Early stopping, best iteration is:\n",
      "[1338]\ttraining's binary_logloss: 0.0347201\tvalid_1's binary_logloss: 0.0438048\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 1.6272\tvalid_1's rmse: 1.65348\n",
      "[200]\ttraining's rmse: 1.5905\tvalid_1's rmse: 1.62014\n",
      "[300]\ttraining's rmse: 1.57152\tvalid_1's rmse: 1.60416\n",
      "[400]\ttraining's rmse: 1.55967\tvalid_1's rmse: 1.59499\n",
      "[500]\ttraining's rmse: 1.5511\tvalid_1's rmse: 1.58904\n",
      "[600]\ttraining's rmse: 1.54433\tvalid_1's rmse: 1.58498\n",
      "[700]\ttraining's rmse: 1.53878\tvalid_1's rmse: 1.58192\n",
      "[800]\ttraining's rmse: 1.534\tvalid_1's rmse: 1.57968\n",
      "[900]\ttraining's rmse: 1.52968\tvalid_1's rmse: 1.57794\n",
      "[1000]\ttraining's rmse: 1.52575\tvalid_1's rmse: 1.57663\n",
      "[1100]\ttraining's rmse: 1.52215\tvalid_1's rmse: 1.57552\n",
      "[1200]\ttraining's rmse: 1.51876\tvalid_1's rmse: 1.57471\n",
      "[1300]\ttraining's rmse: 1.51552\tvalid_1's rmse: 1.57395\n",
      "[1400]\ttraining's rmse: 1.51246\tvalid_1's rmse: 1.57335\n",
      "[1500]\ttraining's rmse: 1.50952\tvalid_1's rmse: 1.57288\n",
      "[1600]\ttraining's rmse: 1.50669\tvalid_1's rmse: 1.57246\n",
      "[1700]\ttraining's rmse: 1.50394\tvalid_1's rmse: 1.57213\n",
      "[1800]\ttraining's rmse: 1.50129\tvalid_1's rmse: 1.57177\n",
      "[1900]\ttraining's rmse: 1.49876\tvalid_1's rmse: 1.57151\n",
      "[2000]\ttraining's rmse: 1.49629\tvalid_1's rmse: 1.57132\n",
      "[2100]\ttraining's rmse: 1.49389\tvalid_1's rmse: 1.57118\n",
      "[2200]\ttraining's rmse: 1.49148\tvalid_1's rmse: 1.57097\n",
      "[2300]\ttraining's rmse: 1.48919\tvalid_1's rmse: 1.57085\n",
      "[2400]\ttraining's rmse: 1.48698\tvalid_1's rmse: 1.57068\n",
      "[2500]\ttraining's rmse: 1.48475\tvalid_1's rmse: 1.57057\n",
      "[2600]\ttraining's rmse: 1.48248\tvalid_1's rmse: 1.57044\n",
      "[2700]\ttraining's rmse: 1.48029\tvalid_1's rmse: 1.57029\n",
      "[2800]\ttraining's rmse: 1.47814\tvalid_1's rmse: 1.57021\n",
      "[2900]\ttraining's rmse: 1.47605\tvalid_1's rmse: 1.5701\n",
      "[3000]\ttraining's rmse: 1.4739\tvalid_1's rmse: 1.57004\n",
      "[3100]\ttraining's rmse: 1.47179\tvalid_1's rmse: 1.56995\n",
      "[3200]\ttraining's rmse: 1.46968\tvalid_1's rmse: 1.5699\n",
      "[3300]\ttraining's rmse: 1.46758\tvalid_1's rmse: 1.56981\n",
      "[3400]\ttraining's rmse: 1.46552\tvalid_1's rmse: 1.56971\n",
      "[3500]\ttraining's rmse: 1.46347\tvalid_1's rmse: 1.56964\n",
      "[3600]\ttraining's rmse: 1.46143\tvalid_1's rmse: 1.56957\n",
      "[3700]\ttraining's rmse: 1.45943\tvalid_1's rmse: 1.56952\n",
      "[3800]\ttraining's rmse: 1.4575\tvalid_1's rmse: 1.56948\n",
      "[3900]\ttraining's rmse: 1.45556\tvalid_1's rmse: 1.56944\n",
      "[4000]\ttraining's rmse: 1.45364\tvalid_1's rmse: 1.5694\n",
      "[4100]\ttraining's rmse: 1.4517\tvalid_1's rmse: 1.56937\n",
      "[4200]\ttraining's rmse: 1.44978\tvalid_1's rmse: 1.56932\n",
      "[4300]\ttraining's rmse: 1.44782\tvalid_1's rmse: 1.56931\n",
      "[4400]\ttraining's rmse: 1.44587\tvalid_1's rmse: 1.56926\n",
      "[4500]\ttraining's rmse: 1.44396\tvalid_1's rmse: 1.56923\n",
      "[4600]\ttraining's rmse: 1.44206\tvalid_1's rmse: 1.56921\n",
      "[4700]\ttraining's rmse: 1.44022\tvalid_1's rmse: 1.56924\n",
      "[4800]\ttraining's rmse: 1.43837\tvalid_1's rmse: 1.56922\n",
      "[4900]\ttraining's rmse: 1.43651\tvalid_1's rmse: 1.56922\n",
      "Early stopping, best iteration is:\n",
      "[4573]\ttraining's rmse: 1.44258\tvalid_1's rmse: 1.5692\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0452373\tvalid_1's binary_logloss: 0.0477786\n",
      "[200]\ttraining's binary_logloss: 0.0423843\tvalid_1's binary_logloss: 0.0458896\n",
      "[300]\ttraining's binary_logloss: 0.0408541\tvalid_1's binary_logloss: 0.0452066\n",
      "[400]\ttraining's binary_logloss: 0.0397198\tvalid_1's binary_logloss: 0.0449062\n",
      "[500]\ttraining's binary_logloss: 0.0388364\tvalid_1's binary_logloss: 0.0447687\n",
      "[600]\ttraining's binary_logloss: 0.0380823\tvalid_1's binary_logloss: 0.0447076\n",
      "[700]\ttraining's binary_logloss: 0.0374177\tvalid_1's binary_logloss: 0.0446797\n",
      "[800]\ttraining's binary_logloss: 0.0368336\tvalid_1's binary_logloss: 0.044671\n",
      "[900]\ttraining's binary_logloss: 0.0363075\tvalid_1's binary_logloss: 0.0446716\n",
      "[1000]\ttraining's binary_logloss: 0.0358263\tvalid_1's binary_logloss: 0.0446863\n",
      "[1100]\ttraining's binary_logloss: 0.0354053\tvalid_1's binary_logloss: 0.044687\n",
      "[1200]\ttraining's binary_logloss: 0.0350334\tvalid_1's binary_logloss: 0.0446876\n",
      "Early stopping, best iteration is:\n",
      "[833]\ttraining's binary_logloss: 0.0366587\tvalid_1's binary_logloss: 0.0446667\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 1.63189\tvalid_1's rmse: 1.63494\n",
      "[200]\ttraining's rmse: 1.59552\tvalid_1's rmse: 1.60076\n",
      "[300]\ttraining's rmse: 1.57666\tvalid_1's rmse: 1.58422\n",
      "[400]\ttraining's rmse: 1.56487\tvalid_1's rmse: 1.57475\n",
      "[500]\ttraining's rmse: 1.55634\tvalid_1's rmse: 1.56878\n",
      "[600]\ttraining's rmse: 1.54964\tvalid_1's rmse: 1.56438\n",
      "[700]\ttraining's rmse: 1.54407\tvalid_1's rmse: 1.56125\n",
      "[800]\ttraining's rmse: 1.53926\tvalid_1's rmse: 1.55892\n",
      "[900]\ttraining's rmse: 1.53492\tvalid_1's rmse: 1.55702\n",
      "[1000]\ttraining's rmse: 1.53102\tvalid_1's rmse: 1.55552\n",
      "[1100]\ttraining's rmse: 1.52743\tvalid_1's rmse: 1.55444\n",
      "[1200]\ttraining's rmse: 1.52407\tvalid_1's rmse: 1.55352\n",
      "[1300]\ttraining's rmse: 1.52085\tvalid_1's rmse: 1.5528\n",
      "[1400]\ttraining's rmse: 1.51777\tvalid_1's rmse: 1.55216\n",
      "[1500]\ttraining's rmse: 1.51483\tvalid_1's rmse: 1.55165\n",
      "[1600]\ttraining's rmse: 1.51196\tvalid_1's rmse: 1.55123\n",
      "[1700]\ttraining's rmse: 1.50922\tvalid_1's rmse: 1.55086\n",
      "[1800]\ttraining's rmse: 1.50658\tvalid_1's rmse: 1.55053\n",
      "[1900]\ttraining's rmse: 1.50403\tvalid_1's rmse: 1.55032\n",
      "[2000]\ttraining's rmse: 1.50151\tvalid_1's rmse: 1.55013\n",
      "[2100]\ttraining's rmse: 1.49904\tvalid_1's rmse: 1.54993\n",
      "[2200]\ttraining's rmse: 1.49661\tvalid_1's rmse: 1.54982\n",
      "[2300]\ttraining's rmse: 1.49422\tvalid_1's rmse: 1.54966\n",
      "[2400]\ttraining's rmse: 1.4919\tvalid_1's rmse: 1.54955\n",
      "[2500]\ttraining's rmse: 1.48965\tvalid_1's rmse: 1.54944\n",
      "[2600]\ttraining's rmse: 1.48741\tvalid_1's rmse: 1.54934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2700]\ttraining's rmse: 1.48517\tvalid_1's rmse: 1.54921\n",
      "[2800]\ttraining's rmse: 1.48297\tvalid_1's rmse: 1.5491\n",
      "[2900]\ttraining's rmse: 1.48081\tvalid_1's rmse: 1.54898\n",
      "[3000]\ttraining's rmse: 1.47862\tvalid_1's rmse: 1.54888\n",
      "[3100]\ttraining's rmse: 1.4765\tvalid_1's rmse: 1.54874\n",
      "[3200]\ttraining's rmse: 1.47443\tvalid_1's rmse: 1.54863\n",
      "[3300]\ttraining's rmse: 1.47237\tvalid_1's rmse: 1.54859\n",
      "[3400]\ttraining's rmse: 1.47036\tvalid_1's rmse: 1.54854\n",
      "[3500]\ttraining's rmse: 1.46829\tvalid_1's rmse: 1.54853\n",
      "[3600]\ttraining's rmse: 1.46628\tvalid_1's rmse: 1.54848\n",
      "[3700]\ttraining's rmse: 1.46426\tvalid_1's rmse: 1.54844\n",
      "[3800]\ttraining's rmse: 1.46226\tvalid_1's rmse: 1.54841\n",
      "[3900]\ttraining's rmse: 1.46022\tvalid_1's rmse: 1.54837\n",
      "[4000]\ttraining's rmse: 1.45822\tvalid_1's rmse: 1.54834\n",
      "[4100]\ttraining's rmse: 1.45631\tvalid_1's rmse: 1.54832\n",
      "[4200]\ttraining's rmse: 1.45436\tvalid_1's rmse: 1.54836\n",
      "[4300]\ttraining's rmse: 1.4524\tvalid_1's rmse: 1.54834\n",
      "[4400]\ttraining's rmse: 1.45047\tvalid_1's rmse: 1.5483\n",
      "[4500]\ttraining's rmse: 1.44851\tvalid_1's rmse: 1.54827\n",
      "[4600]\ttraining's rmse: 1.44653\tvalid_1's rmse: 1.54826\n",
      "[4700]\ttraining's rmse: 1.44461\tvalid_1's rmse: 1.54821\n",
      "[4800]\ttraining's rmse: 1.44271\tvalid_1's rmse: 1.54821\n",
      "[4900]\ttraining's rmse: 1.44077\tvalid_1's rmse: 1.54821\n",
      "[5000]\ttraining's rmse: 1.4389\tvalid_1's rmse: 1.54818\n",
      "[5100]\ttraining's rmse: 1.43701\tvalid_1's rmse: 1.54818\n",
      "[5200]\ttraining's rmse: 1.43516\tvalid_1's rmse: 1.5482\n",
      "[5300]\ttraining's rmse: 1.43326\tvalid_1's rmse: 1.54821\n",
      "[5400]\ttraining's rmse: 1.43137\tvalid_1's rmse: 1.5482\n",
      "[5500]\ttraining's rmse: 1.42952\tvalid_1's rmse: 1.54822\n",
      "Early stopping, best iteration is:\n",
      "[5104]\ttraining's rmse: 1.43694\tvalid_1's rmse: 1.54817\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0451862\tvalid_1's binary_logloss: 0.0481287\n",
      "[200]\ttraining's binary_logloss: 0.0421436\tvalid_1's binary_logloss: 0.0464453\n",
      "[300]\ttraining's binary_logloss: 0.0405739\tvalid_1's binary_logloss: 0.0459777\n",
      "[400]\ttraining's binary_logloss: 0.0394778\tvalid_1's binary_logloss: 0.0457623\n",
      "[500]\ttraining's binary_logloss: 0.0386175\tvalid_1's binary_logloss: 0.0456529\n",
      "[600]\ttraining's binary_logloss: 0.0378647\tvalid_1's binary_logloss: 0.0456428\n",
      "[700]\ttraining's binary_logloss: 0.0371802\tvalid_1's binary_logloss: 0.0456188\n",
      "[800]\ttraining's binary_logloss: 0.0365658\tvalid_1's binary_logloss: 0.0455997\n",
      "[900]\ttraining's binary_logloss: 0.0360233\tvalid_1's binary_logloss: 0.0456096\n",
      "[1000]\ttraining's binary_logloss: 0.0355532\tvalid_1's binary_logloss: 0.0456313\n",
      "[1100]\ttraining's binary_logloss: 0.0351147\tvalid_1's binary_logloss: 0.045631\n",
      "Early stopping, best iteration is:\n",
      "[799]\ttraining's binary_logloss: 0.0365698\tvalid_1's binary_logloss: 0.0455994\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's rmse: 1.63991\tvalid_1's rmse: 1.60251\n",
      "[200]\ttraining's rmse: 1.60327\tvalid_1's rmse: 1.56815\n",
      "[300]\ttraining's rmse: 1.58431\tvalid_1's rmse: 1.5518\n",
      "[400]\ttraining's rmse: 1.57238\tvalid_1's rmse: 1.5428\n",
      "[500]\ttraining's rmse: 1.56372\tvalid_1's rmse: 1.53693\n",
      "[600]\ttraining's rmse: 1.5569\tvalid_1's rmse: 1.53296\n",
      "[700]\ttraining's rmse: 1.55129\tvalid_1's rmse: 1.53011\n",
      "[800]\ttraining's rmse: 1.54646\tvalid_1's rmse: 1.52802\n",
      "[900]\ttraining's rmse: 1.54209\tvalid_1's rmse: 1.52641\n",
      "[1000]\ttraining's rmse: 1.53809\tvalid_1's rmse: 1.52508\n",
      "[1100]\ttraining's rmse: 1.53441\tvalid_1's rmse: 1.52408\n",
      "[1200]\ttraining's rmse: 1.53097\tvalid_1's rmse: 1.52328\n",
      "[1300]\ttraining's rmse: 1.5277\tvalid_1's rmse: 1.5226\n",
      "[1400]\ttraining's rmse: 1.5246\tvalid_1's rmse: 1.52207\n",
      "[1500]\ttraining's rmse: 1.52164\tvalid_1's rmse: 1.52167\n",
      "[1600]\ttraining's rmse: 1.51878\tvalid_1's rmse: 1.52127\n",
      "[1700]\ttraining's rmse: 1.516\tvalid_1's rmse: 1.52102\n",
      "[1800]\ttraining's rmse: 1.51328\tvalid_1's rmse: 1.52079\n",
      "[1900]\ttraining's rmse: 1.51068\tvalid_1's rmse: 1.52059\n",
      "[2000]\ttraining's rmse: 1.50812\tvalid_1's rmse: 1.52038\n",
      "[2100]\ttraining's rmse: 1.50562\tvalid_1's rmse: 1.52016\n",
      "[2200]\ttraining's rmse: 1.50315\tvalid_1's rmse: 1.51997\n",
      "[2300]\ttraining's rmse: 1.50079\tvalid_1's rmse: 1.51986\n",
      "[2400]\ttraining's rmse: 1.49842\tvalid_1's rmse: 1.5197\n",
      "[2500]\ttraining's rmse: 1.49611\tvalid_1's rmse: 1.51961\n",
      "[2600]\ttraining's rmse: 1.49384\tvalid_1's rmse: 1.51946\n",
      "[2700]\ttraining's rmse: 1.49158\tvalid_1's rmse: 1.51935\n",
      "[2800]\ttraining's rmse: 1.48935\tvalid_1's rmse: 1.51931\n",
      "[2900]\ttraining's rmse: 1.48718\tvalid_1's rmse: 1.51927\n",
      "[3000]\ttraining's rmse: 1.48506\tvalid_1's rmse: 1.51917\n",
      "[3100]\ttraining's rmse: 1.48288\tvalid_1's rmse: 1.51914\n",
      "[3200]\ttraining's rmse: 1.48076\tvalid_1's rmse: 1.51906\n",
      "[3300]\ttraining's rmse: 1.47865\tvalid_1's rmse: 1.51902\n",
      "[3400]\ttraining's rmse: 1.47664\tvalid_1's rmse: 1.51897\n",
      "[3500]\ttraining's rmse: 1.47453\tvalid_1's rmse: 1.51897\n",
      "[3600]\ttraining's rmse: 1.47242\tvalid_1's rmse: 1.51891\n",
      "[3700]\ttraining's rmse: 1.47041\tvalid_1's rmse: 1.51887\n",
      "[3800]\ttraining's rmse: 1.46837\tvalid_1's rmse: 1.51887\n",
      "[3900]\ttraining's rmse: 1.46632\tvalid_1's rmse: 1.51888\n",
      "[4000]\ttraining's rmse: 1.46433\tvalid_1's rmse: 1.51891\n",
      "[4100]\ttraining's rmse: 1.46237\tvalid_1's rmse: 1.51888\n",
      "Early stopping, best iteration is:\n",
      "[3763]\ttraining's rmse: 1.4691\tvalid_1's rmse: 1.51884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.8000297111508057"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best score with catboost feature 3.643498400318252\n",
    "#df_train['outliers'] = 0\n",
    "#df_train.loc[df_train['target'] < -30, 'outliers'] = 1\n",
    "best_pr=val_pr.copy()\n",
    "obj=list(df_train.select_dtypes(include=['object']).columns)[2:]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb=LabelEncoder()\n",
    "for o in obj:\n",
    "    df_train[o]=lb.fit_transform(df_train[o]) \n",
    "df_train_columns = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','target','outliers','Unnamed: 0']]\n",
    "param = {'objective': 'regression_l2', \n",
    "    'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt', \n",
    "            'n_jobs': 4, 'max_depth': 18, \n",
    "            'n_estimators': 6100, \n",
    "            'subsample_freq': 2, \n",
    "            'min_data_per_group': 100, \n",
    "            'max_cat_to_onehot': 4, \n",
    "            'cat_l2': 10.0, \n",
    "            'cat_smooth': 10.0, \n",
    "            'max_cat_threshold': 32, \n",
    "            'metric_freq': 10, \n",
    "            'verbosity': -1, \n",
    "            'colsample_bytree': 0.5, \n",
    "            'learning_rate': 0.0061033234451294376, \n",
    "            'min_child_samples': 20, \n",
    "            'min_child_weight': 9.0, \n",
    "            'min_split_gain': 1e-06, \n",
    "            'num_leaves': 36, \n",
    "            'reg_alpha': 40.0, \n",
    "            'reg_lambda': 13.3, \n",
    "            'subsample': 0.9}\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "rskf=StratifiedKFold(5,shuffle=True,random_state=4590)\n",
    "val_pr=np.zeros(len(df_train))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "#test_pr=np.zeros(len(df_test))\n",
    "for train_index,val_index in rskf.split(df_train,df_train['outliers'].values):\n",
    "    num_round = 10000\n",
    "    train_data=lgb.Dataset(df_train[df_train_columns].loc[train_index],label=df_train['outliers'].loc[train_index])\n",
    "    val_data=lgb.Dataset(df_train[df_train_columns].loc[val_index],label=df_train['outliers'].loc[val_index])\n",
    "    best_pr_val=best_pr[val_index]\n",
    "    params={'n_estimators': 6100, \n",
    "             'n_jobs': 4,\n",
    "        'num_leaves': 31,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'binary',\n",
    "         'max_depth': 12,\n",
    "         'learning_rate': 0.01,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'binary_logloss',\n",
    "         \"lambda_l1\": 30,\n",
    "         \"verbosity\": -1,\n",
    "         \"random_state\": 2333\n",
    "    }\n",
    "    model=lgb.train(params,train_data,num_round,valid_sets=[train_data,val_data],verbose_eval=100,early_stopping_rounds=400)\n",
    "    cat_pr=model.predict(df_train[df_train_columns].loc[val_index],num_iteration=model.best_iteration)\n",
    "    \n",
    "    train_data=lgb.Dataset(df_train[df_train_columns].loc[train_index][df_train['outliers']==0],label=df_train['target'].loc[train_index][df_train['outliers']==0])\n",
    "    val_data=lgb.Dataset(df_train[df_train_columns].loc[val_index][df_train['outliers']==0],label=df_train['target'].loc[val_index][df_train['outliers']==0])\n",
    "    model=lgb.train(param,train_data,num_round,valid_sets=[train_data,val_data],verbose_eval=100,early_stopping_rounds=400)\n",
    "    lgb_pr=model.predict(df_train[df_train_columns].loc[val_index],num_iteration=model.best_iteration)\n",
    "    prediction=pd.DataFrame({'cat_pr':cat_pr,'lgb_pr':lgb_pr,'target':df_train['target'].loc[val_index]})\n",
    "    thresholds = []\n",
    "    for thresh in np.arange(0.01, 1, 0.01):\n",
    "        thresh = np.round(thresh, 2)\n",
    "        pr=np.where(prediction['cat_pr']>thresh,best_pr_val,prediction['lgb_pr'])\n",
    "        res = np.sqrt(mean_squared_error(pr,prediction['target'].values))\n",
    "        thresholds.append([thresh, res])\n",
    "    thresholds.sort(key=lambda x: x[1], reverse=False)\n",
    "    best_thresh = thresholds[0][0]\n",
    "    \n",
    "    val_pr[val_index]=np.where(prediction['cat_pr']>best_thresh,best_pr_val,prediction['lgb_pr'])\n",
    "    \n",
    "np.sqrt(mean_squared_error(val_pr,df_train['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: \t1 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t288\n",
      "Rejected: \t0\n",
      "Iteration: \t2 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t288\n",
      "Rejected: \t0\n",
      "Iteration: \t3 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t288\n",
      "Rejected: \t0\n",
      "Iteration: \t4 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t288\n",
      "Rejected: \t0\n",
      "Iteration: \t5 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t288\n",
      "Rejected: \t0\n",
      "Iteration: \t6 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t288\n",
      "Rejected: \t0\n",
      "Iteration: \t7 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t288\n",
      "Rejected: \t0\n",
      "Iteration: \t8 / 100\n",
      "Confirmed: \t33\n",
      "Tentative: \t31\n",
      "Rejected: \t224\n",
      "Iteration: \t9 / 100\n",
      "Confirmed: \t33\n",
      "Tentative: \t31\n",
      "Rejected: \t224\n",
      "Iteration: \t10 / 100\n",
      "Confirmed: \t33\n",
      "Tentative: \t31\n",
      "Rejected: \t224\n",
      "Iteration: \t11 / 100\n",
      "Confirmed: \t33\n",
      "Tentative: \t31\n",
      "Rejected: \t224\n",
      "Iteration: \t12 / 100\n",
      "Confirmed: \t34\n",
      "Tentative: \t30\n",
      "Rejected: \t224\n",
      "Iteration: \t13 / 100\n",
      "Confirmed: \t34\n",
      "Tentative: \t30\n",
      "Rejected: \t224\n",
      "Iteration: \t14 / 100\n",
      "Confirmed: \t34\n",
      "Tentative: \t30\n",
      "Rejected: \t224\n",
      "Iteration: \t15 / 100\n",
      "Confirmed: \t34\n",
      "Tentative: \t29\n",
      "Rejected: \t225\n",
      "Iteration: \t16 / 100\n",
      "Confirmed: \t36\n",
      "Tentative: \t27\n",
      "Rejected: \t225\n",
      "Iteration: \t17 / 100\n",
      "Confirmed: \t36\n",
      "Tentative: \t27\n",
      "Rejected: \t225\n",
      "Iteration: \t18 / 100\n",
      "Confirmed: \t36\n",
      "Tentative: \t26\n",
      "Rejected: \t226\n",
      "Iteration: \t19 / 100\n",
      "Confirmed: \t38\n",
      "Tentative: \t24\n",
      "Rejected: \t226\n",
      "Iteration: \t20 / 100\n",
      "Confirmed: \t38\n",
      "Tentative: \t24\n",
      "Rejected: \t226\n",
      "Iteration: \t21 / 100\n",
      "Confirmed: \t38\n",
      "Tentative: \t24\n",
      "Rejected: \t226\n",
      "Iteration: \t22 / 100\n",
      "Confirmed: \t40\n",
      "Tentative: \t22\n",
      "Rejected: \t226\n",
      "Iteration: \t23 / 100\n",
      "Confirmed: \t40\n",
      "Tentative: \t22\n",
      "Rejected: \t226\n",
      "Iteration: \t24 / 100\n",
      "Confirmed: \t40\n",
      "Tentative: \t21\n",
      "Rejected: \t227\n",
      "Iteration: \t25 / 100\n",
      "Confirmed: \t40\n",
      "Tentative: \t21\n",
      "Rejected: \t227\n",
      "Iteration: \t26 / 100\n",
      "Confirmed: \t41\n",
      "Tentative: \t20\n",
      "Rejected: \t227\n",
      "Iteration: \t27 / 100\n",
      "Confirmed: \t41\n",
      "Tentative: \t20\n",
      "Rejected: \t227\n",
      "Iteration: \t28 / 100\n",
      "Confirmed: \t41\n",
      "Tentative: \t20\n",
      "Rejected: \t227\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-cc74d283133d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m                        bagging_freq= 1, bagging_fraction= 0.9,bagging_seed= 11, metric= 'rmse', lambda_l1=10)\n\u001b[0;32m     14\u001b[0m \u001b[0mborutaselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBorutaPy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlgbmclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mborutaselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_train_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-fba0997b304e>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \"\"\"\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweak\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-fba0997b304e>\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;31m# add shadow attributes, shuffle them and train estimator, get imps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m             \u001b[0mcur_imp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_shadows_get_imps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_reg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m             \u001b[1;31m# get the threshold of shadow importances we will use for rejection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-fba0997b304e>\u001b[0m in \u001b[0;36m_add_shadows_get_imps\u001b[1;34m(self, X, y, dec_reg)\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[0mx_sha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_shuffle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_sha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[1;31m# get importance of the merged matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         \u001b[0mimp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_imp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_cur\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_sha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m         \u001b[1;31m# separate importances of real and shadow features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[0mimp_sha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_cur_w\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-fba0997b304e>\u001b[0m in \u001b[0;36m_get_imp\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_imp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m             raise ValueError('Please check your X and y variable. The provided'\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    681\u001b[0m                                        \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m                                        \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 683\u001b[1;33m                                        callbacks=callbacks)\n\u001b[0m\u001b[0;32m    684\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    540\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    214\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   1758\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0;32m   1759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1760\u001b[1;33m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[0;32m   1761\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1762\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from boruta import BorutaPy\n",
    "obj=list(df_train.select_dtypes(include=['object']).columns)[2:]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb=LabelEncoder()\n",
    "for o in obj:\n",
    "    df_train[o]=lb.fit_transform(df_train[o]) \n",
    "df_train_columns = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','target','outliers','Unnamed: 0']]\n",
    "df_train=df_train.replace([np.inf, -np.inf], np.nan)\n",
    "df_train=df_train.fillna(0)\n",
    "from lightgbm import LGBMRegressor\n",
    "lgbmclf = LGBMRegressor(boosting_type='gbdt', objective='regression', num_iteration=1000, num_leaves=31,\n",
    "                        max_depth=7, learning_rate=0.01, feature_fraction= 0.9,\n",
    "                       bagging_freq= 1, bagging_fraction= 0.9,bagging_seed= 11, metric= 'rmse', lambda_l1=10)\n",
    "borutaselector = BorutaPy(lgbmclf, n_estimators='auto', verbose=2) \n",
    "borutaselector.fit(df_train[df_train_columns].values, df_train['target'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.utils import check_random_state, check_X_y\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "\n",
    "class BorutaPy(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Improved Python implementation of the Boruta R package.\n",
    "\n",
    "    The improvements of this implementation include:\n",
    "    - Faster run times:\n",
    "        Thanks to scikit-learn's fast implementation of the ensemble methods.\n",
    "    - Scikit-learn like interface:\n",
    "        Use BorutaPy just like any other scikit learner: fit, fit_transform and\n",
    "        transform are all implemented in a similar fashion.\n",
    "    - Modularity:\n",
    "        Any ensemble method could be used: random forest, extra trees\n",
    "        classifier, even gradient boosted trees.\n",
    "    - Two step correction:\n",
    "        The original Boruta code corrects for multiple testing in an overly\n",
    "        conservative way. In this implementation, the Benjamini Hochberg FDR is\n",
    "        used to correct in each iteration across active features. This means\n",
    "        only those features are included in the correction which are still in\n",
    "        the selection process. Following this, each that passed goes through a\n",
    "        regular Bonferroni correction to check for the repeated testing over\n",
    "        the iterations.\n",
    "    - Percentile:\n",
    "        Instead of using the max values of the shadow features the user can\n",
    "        specify which percentile to use. This gives a finer control over this\n",
    "        crucial parameter. For more info, please read about the perc parameter.\n",
    "    - Automatic tree number:\n",
    "        Setting the n_estimator to 'auto' will calculate the number of trees\n",
    "        in each itartion based on the number of features under investigation.\n",
    "        This way more trees are used when the training data has many feautres\n",
    "        and less when most of the features have been rejected.\n",
    "    - Ranking of features:\n",
    "        After fitting BorutaPy it provides the user with ranking of features.\n",
    "        Confirmed ones are 1, Tentatives are 2, and the rejected are ranked\n",
    "        starting from 3, based on their feautre importance history through\n",
    "        the iterations.\n",
    "\n",
    "    We highly recommend using pruned trees with a depth between 3-7.\n",
    "\n",
    "    For more, see the docs of these functions, and the examples below.\n",
    "\n",
    "    Original code and method by: Miron B Kursa, https://m2.icm.edu.pl/boruta/\n",
    "\n",
    "    Boruta is an all relevant feature selection method, while most other are\n",
    "    minimal optimal; this means it tries to find all features carrying\n",
    "    information usable for prediction, rather than finding a possibly compact\n",
    "    subset of features on which some classifier has a minimal error.\n",
    "\n",
    "    Why bother with all relevant feature selection?\n",
    "    When you try to understand the phenomenon that made your data, you should\n",
    "    care about all factors that contribute to it, not just the bluntest signs\n",
    "    of it in context of your methodology (yes, minimal optimal set of features\n",
    "    by definition depends on your classifier choice).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    estimator : object\n",
    "        A supervised learning estimator, with a 'fit' method that returns the\n",
    "        feature_importances_ attribute. Important features must correspond to\n",
    "        high absolute values in the feature_importances_.\n",
    "\n",
    "    n_estimators : int or string, default = 1000\n",
    "        If int sets the number of estimators in the chosen ensemble method.\n",
    "        If 'auto' this is determined automatically based on the size of the\n",
    "        dataset. The other parameters of the used estimators need to be set\n",
    "        with initialisation.\n",
    "\n",
    "    perc : int, default = 100\n",
    "        Instead of the max we use the percentile defined by the user, to pick\n",
    "        our threshold for comparison between shadow and real features. The max\n",
    "        tend to be too stringent. This provides a finer control over this. The\n",
    "        lower perc is the more false positives will be picked as relevant but\n",
    "        also the less relevant features will be left out. The usual trade-off.\n",
    "        The default is essentially the vanilla Boruta corresponding to the max.\n",
    "\n",
    "    alpha : float, default = 0.05\n",
    "        Level at which the corrected p-values will get rejected in both\n",
    "        correction steps.\n",
    "\n",
    "    two_step : Boolean, default = True\n",
    "        If you want to use the original implementation of Boruta with Bonferroni\n",
    "        correction only set this to False.\n",
    "\n",
    "    max_iter : int, default = 100\n",
    "        The number of maximum iterations to perform.\n",
    "\n",
    "    random_state : int, RandomState instance or None; default=None\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "\n",
    "    verbose : int, default=0\n",
    "        Controls verbosity of output:\n",
    "        - 0: no output\n",
    "        - 1: displays iteration number\n",
    "        - 2: which features have been selected already\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    n_features_ : int\n",
    "        The number of selected features.\n",
    "\n",
    "    support_ : array of shape [n_features]\n",
    "\n",
    "        The mask of selected features - only confirmed ones are True.\n",
    "\n",
    "    support_weak_ : array of shape [n_features]\n",
    "\n",
    "        The mask of selected tentative features, which haven't gained enough\n",
    "        support during the max_iter number of iterations..\n",
    "\n",
    "    ranking_ : array of shape [n_features]\n",
    "\n",
    "        The feature ranking, such that ``ranking_[i]`` corresponds to the\n",
    "        ranking position of the i-th feature. Selected (i.e., estimated\n",
    "        best) features are assigned rank 1 and tentative features are assigned\n",
    "        rank 2.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    \n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from boruta import BorutaPy\n",
    "    \n",
    "    # load X and y\n",
    "    # NOTE BorutaPy accepts numpy arrays only, hence the .values attribute\n",
    "    X = pd.read_csv('examples/test_X.csv', index_col=0).values\n",
    "    y = pd.read_csv('examples/test_y.csv', header=None, index_col=0).values\n",
    "    y = y.ravel()\n",
    "    \n",
    "    # define random forest classifier, with utilising all cores and\n",
    "    # sampling in proportion to y labels\n",
    "    rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "    \n",
    "    # define Boruta feature selection method\n",
    "    feat_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=1)\n",
    "    \n",
    "    # find all relevant features - 5 features should be selected\n",
    "    feat_selector.fit(X, y)\n",
    "    \n",
    "    # check selected features - first 5 features are selected\n",
    "    feat_selector.support_\n",
    "    \n",
    "    # check ranking of features\n",
    "    feat_selector.ranking_\n",
    "    \n",
    "    # call transform() on X to filter it down to selected features\n",
    "    X_filtered = feat_selector.transform(X)\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    [1] Kursa M., Rudnicki W., \"Feature Selection with the Boruta Package\"\n",
    "        Journal of Statistical Software, Vol. 36, Issue 11, Sep 2010\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, estimator, n_estimators=1000, perc=100, alpha=0.05,\n",
    "                 two_step=True, max_iter=100, random_state=None, verbose=0):\n",
    "        self.estimator = estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.perc = perc\n",
    "        self.alpha = alpha\n",
    "        self.two_step = two_step\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the Boruta feature selection with the provided estimator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples.\n",
    "\n",
    "        y : array-like, shape = [n_samples]\n",
    "            The target values.\n",
    "        \"\"\"\n",
    "\n",
    "        return self._fit(X, y)\n",
    "\n",
    "    def transform(self, X, weak=False):\n",
    "        \"\"\"\n",
    "        Reduces the input X to the features selected by Boruta.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples.\n",
    "\n",
    "        weak: boolean, default = False\n",
    "            If set to true, the tentative features are also used to reduce X.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X : array-like, shape = [n_samples, n_features_]\n",
    "            The input matrix X's columns are reduced to the features which were\n",
    "            selected by Boruta.\n",
    "        \"\"\"\n",
    "\n",
    "        return self._transform(X, weak)\n",
    "\n",
    "    def fit_transform(self, X, y, weak=False):\n",
    "        \"\"\"\n",
    "        Fits Boruta, then reduces the input X to the selected features.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples.\n",
    "\n",
    "        y : array-like, shape = [n_samples]\n",
    "            The target values.\n",
    "\n",
    "        weak: boolean, default = False\n",
    "            If set to true, the tentative features are also used to reduce X.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X : array-like, shape = [n_samples, n_features_]\n",
    "            The input matrix X's columns are reduced to the features which were\n",
    "            selected by Boruta.\n",
    "        \"\"\"\n",
    "\n",
    "        self._fit(X, y)\n",
    "        return self._transform(X, weak)\n",
    "\n",
    "    def _fit(self, X, y):\n",
    "        # check input params\n",
    "        self._check_params(X, y)\n",
    "        self.random_state = check_random_state(self.random_state)\n",
    "        # setup variables for Boruta\n",
    "        n_sample, n_feat = X.shape\n",
    "        _iter = 1\n",
    "        # holds the decision about each feature:\n",
    "        # 0  - default state = tentative in original code\n",
    "        # 1  - accepted in original code\n",
    "        # -1 - rejected in original code\n",
    "        dec_reg = np.zeros(n_feat, dtype=np.int)\n",
    "        # counts how many times a given feature was more important than\n",
    "        # the best of the shadow features\n",
    "        hit_reg = np.zeros(n_feat, dtype=np.int)\n",
    "        # these record the history of the iterations\n",
    "        imp_history = np.zeros(n_feat, dtype=np.float)\n",
    "        sha_max_history = []\n",
    "\n",
    "        # set n_estimators\n",
    "        if self.n_estimators != 'auto':\n",
    "            self.estimator.set_params(n_estimators=self.n_estimators)\n",
    "\n",
    "        # main feature selection loop\n",
    "        while np.any(dec_reg == 0) and _iter < self.max_iter:\n",
    "            # find optimal number of trees and depth\n",
    "            if self.n_estimators == 'auto':\n",
    "                # number of features that aren't rejected\n",
    "                not_rejected = np.where(dec_reg >= 0)[0].shape[0]\n",
    "                n_tree = self._get_tree_num(not_rejected)\n",
    "                self.estimator.set_params(n_estimators=n_tree)\n",
    "\n",
    "            # make sure we start with a new tree in each iteration\n",
    "            self.estimator.set_params(random_state=self.random_state.get_state()[2])\n",
    "\n",
    "            # add shadow attributes, shuffle them and train estimator, get imps\n",
    "            cur_imp = self._add_shadows_get_imps(X, y, dec_reg)\n",
    "\n",
    "            # get the threshold of shadow importances we will use for rejection\n",
    "            imp_sha_max = np.percentile(cur_imp[1], self.perc)\n",
    "\n",
    "            # record importance history\n",
    "            sha_max_history.append(imp_sha_max)\n",
    "            imp_history = np.vstack((imp_history, cur_imp[0]))\n",
    "\n",
    "            # register which feature is more imp than the max of shadows\n",
    "            hit_reg = self._assign_hits(hit_reg, cur_imp, imp_sha_max)\n",
    "\n",
    "            # based on hit_reg we check if a feature is doing better than\n",
    "            # expected by chance\n",
    "            dec_reg = self._do_tests(dec_reg, hit_reg, _iter)\n",
    "\n",
    "            # print out confirmed features\n",
    "            if self.verbose > 0 and _iter < self.max_iter:\n",
    "                self._print_results(dec_reg, _iter, 0)\n",
    "            if _iter < self.max_iter:\n",
    "                _iter += 1\n",
    "\n",
    "        # we automatically apply R package's rough fix for tentative ones\n",
    "        confirmed = np.where(dec_reg == 1)[0]\n",
    "        tentative = np.where(dec_reg == 0)[0]\n",
    "        # ignore the first row of zeros\n",
    "        tentative_median = np.median(imp_history[1:, tentative], axis=0)\n",
    "        # which tentative to keep\n",
    "        tentative_confirmed = np.where(tentative_median\n",
    "                                       > np.median(sha_max_history))[0]\n",
    "        tentative = tentative[tentative_confirmed]\n",
    "\n",
    "        # basic result variables\n",
    "        self.n_features_ = confirmed.shape[0]\n",
    "        self.support_ = np.zeros(n_feat, dtype=np.bool)\n",
    "        self.support_[confirmed] = 1\n",
    "        self.support_weak_ = np.zeros(n_feat, dtype=np.bool)\n",
    "        self.support_weak_[tentative] = 1\n",
    "\n",
    "        # ranking, confirmed variables are rank 1\n",
    "        self.ranking_ = np.ones(n_feat, dtype=np.int)\n",
    "        # tentative variables are rank 2\n",
    "        self.ranking_[tentative] = 2\n",
    "        # selected = confirmed and tentative\n",
    "        selected = np.hstack((confirmed, tentative))\n",
    "        # all rejected features are sorted by importance history\n",
    "        not_selected = np.setdiff1d(np.arange(n_feat), selected)\n",
    "        # large importance values should rank higher = lower ranks -> *(-1)\n",
    "        imp_history_rejected = imp_history[1:, not_selected] * -1\n",
    "\n",
    "        # update rank for not_selected features\n",
    "        if not_selected.shape[0] > 0:\n",
    "                # calculate ranks in each iteration, then median of ranks across feats\n",
    "                iter_ranks = self._nanrankdata(imp_history_rejected, axis=1)\n",
    "                rank_medians = np.nanmedian(iter_ranks, axis=0)\n",
    "                ranks = self._nanrankdata(rank_medians, axis=0)\n",
    "\n",
    "                # set smallest rank to 3 if there are tentative feats\n",
    "                if tentative.shape[0] > 0:\n",
    "                    ranks = ranks - np.min(ranks) + 3\n",
    "                else:\n",
    "                    # and 2 otherwise\n",
    "                    ranks = ranks - np.min(ranks) + 2\n",
    "                self.ranking_[not_selected] = ranks\n",
    "        else:\n",
    "            # all are selected, thus we set feature supports to True\n",
    "            self.support_ = np.ones(n_feat, dtype=np.bool)\n",
    "\n",
    "        # notify user\n",
    "        if self.verbose > 0:\n",
    "            self._print_results(dec_reg, _iter, 1)\n",
    "        return self\n",
    "\n",
    "    def _transform(self, X, weak=False):\n",
    "        # sanity check\n",
    "        try:\n",
    "            self.ranking_\n",
    "        except AttributeError:\n",
    "            raise ValueError('You need to call the fit(X, y) method first.')\n",
    "\n",
    "        if weak:\n",
    "            X = X[:, self.support_ + self.support_weak_]\n",
    "        else:\n",
    "            X = X[:, self.support_]\n",
    "        return X\n",
    "\n",
    "    def _get_tree_num(self, n_feat):\n",
    "        depth = self.estimator.get_params()['max_depth']\n",
    "        if depth == None:\n",
    "            depth = 10\n",
    "        # how many times a feature should be considered on average\n",
    "        f_repr = 100\n",
    "        # n_feat * 2 because the training matrix is extended with n shadow features\n",
    "        multi = ((n_feat * 2) / (np.sqrt(n_feat * 2) * depth))\n",
    "        n_estimators = int(multi * f_repr)\n",
    "        return n_estimators\n",
    "\n",
    "    def _get_imp(self, X, y):\n",
    "        try:\n",
    "            self.estimator.fit(X, y)\n",
    "        except Exception as e:\n",
    "            raise ValueError('Please check your X and y variable. The provided'\n",
    "                             'estimator cannot be fitted to your data.\\n' + str(e))\n",
    "        try:\n",
    "            imp = self.estimator.feature_importances_\n",
    "        except Exception:\n",
    "            raise ValueError('Only methods with feature_importance_ attribute '\n",
    "                             'are currently supported in BorutaPy.')\n",
    "        return imp\n",
    "\n",
    "    def _get_shuffle(self, seq):\n",
    "        self.random_state.shuffle(seq)\n",
    "        return seq\n",
    "\n",
    "    def _add_shadows_get_imps(self, X, y, dec_reg):\n",
    "        # find features that are tentative still\n",
    "        x_cur_ind = np.where(dec_reg >= 0)[0]\n",
    "        x_cur = np.copy(X[:, x_cur_ind])\n",
    "        x_cur_w = x_cur.shape[1]\n",
    "        # deep copy the matrix for the shadow matrix\n",
    "        x_sha = np.copy(x_cur)\n",
    "        # make sure there's at least 5 columns in the shadow matrix for\n",
    "        while (x_sha.shape[1] < 5):\n",
    "            x_sha = np.hstack((x_sha, x_sha))\n",
    "        # shuffle xSha\n",
    "        x_sha = np.apply_along_axis(self._get_shuffle, 0, x_sha)\n",
    "        # get importance of the merged matrix\n",
    "        imp = self._get_imp(np.hstack((x_cur, x_sha)), y)\n",
    "        # separate importances of real and shadow features\n",
    "        imp_sha = imp[x_cur_w:]\n",
    "        imp_real = np.zeros(X.shape[1])\n",
    "        imp_real[:] = np.nan\n",
    "        imp_real[x_cur_ind] = imp[:x_cur_w]\n",
    "        return imp_real, imp_sha\n",
    "\n",
    "    def _assign_hits(self, hit_reg, cur_imp, imp_sha_max):\n",
    "        # register hits for features that did better than the best of shadows\n",
    "        cur_imp_no_nan = cur_imp[0]\n",
    "        cur_imp_no_nan[np.isnan(cur_imp_no_nan)] = 0\n",
    "        hits = np.where(cur_imp_no_nan > imp_sha_max)[0]\n",
    "        hit_reg[hits] += 1\n",
    "        return hit_reg\n",
    "\n",
    "    def _do_tests(self, dec_reg, hit_reg, _iter):\n",
    "        active_features = np.where(dec_reg >= 0)[0]\n",
    "        hits = hit_reg[active_features]\n",
    "        # get uncorrected p values based on hit_reg\n",
    "        to_accept_ps = sp.stats.binom.sf(hits - 1, _iter, .5).flatten()\n",
    "        to_reject_ps = sp.stats.binom.cdf(hits, _iter, .5).flatten()\n",
    "\n",
    "        if self.two_step:\n",
    "            # two step multicor process\n",
    "            # first we correct for testing several features in each round using FDR\n",
    "            to_accept = self._fdrcorrection(to_accept_ps, alpha=self.alpha)[0]\n",
    "            to_reject = self._fdrcorrection(to_reject_ps, alpha=self.alpha)[0]\n",
    "\n",
    "            # second we correct for testing the same feature over and over again\n",
    "            # using bonferroni\n",
    "            to_accept2 = to_accept_ps <= self.alpha / float(_iter)\n",
    "            to_reject2 = to_reject_ps <= self.alpha / float(_iter)\n",
    "\n",
    "            # combine the two multi corrections, and get indexes\n",
    "            to_accept *= to_accept2\n",
    "            to_reject *= to_reject2\n",
    "        else:\n",
    "            # as in th original Boruta, we simply do bonferroni correction\n",
    "            # with the total n_feat in each iteration\n",
    "            to_accept = to_accept_ps <= self.alpha / float(len(dec_reg))\n",
    "            to_reject = to_reject_ps <= self.alpha / float(len(dec_reg))\n",
    "\n",
    "        # find features which are 0 and have been rejected or accepted\n",
    "        to_accept = np.where((dec_reg[active_features] == 0) * to_accept)[0]\n",
    "        to_reject = np.where((dec_reg[active_features] == 0) * to_reject)[0]\n",
    "\n",
    "        # updating dec_reg\n",
    "        dec_reg[active_features[to_accept]] = 1\n",
    "        dec_reg[active_features[to_reject]] = -1\n",
    "        return dec_reg\n",
    "\n",
    "    def _fdrcorrection(self, pvals, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Benjamini/Hochberg p-value correction for false discovery rate, from\n",
    "        statsmodels package. Included here for decoupling dependency on statsmodels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pvals : array_like\n",
    "            set of p-values of the individual tests.\n",
    "        alpha : float\n",
    "            error rate\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        rejected : array, bool\n",
    "            True if a hypothesis is rejected, False if not\n",
    "        pvalue-corrected : array\n",
    "            pvalues adjusted for multiple hypothesis testing to limit FDR\n",
    "        \"\"\"\n",
    "        pvals = np.asarray(pvals)\n",
    "        pvals_sortind = np.argsort(pvals)\n",
    "        pvals_sorted = np.take(pvals, pvals_sortind)\n",
    "        nobs = len(pvals_sorted)\n",
    "        ecdffactor = np.arange(1, nobs + 1) / float(nobs)\n",
    "\n",
    "        reject = pvals_sorted <= ecdffactor * alpha\n",
    "        if reject.any():\n",
    "            rejectmax = max(np.nonzero(reject)[0])\n",
    "            reject[:rejectmax] = True\n",
    "\n",
    "        pvals_corrected_raw = pvals_sorted / ecdffactor\n",
    "        pvals_corrected = np.minimum.accumulate(pvals_corrected_raw[::-1])[::-1]\n",
    "        pvals_corrected[pvals_corrected > 1] = 1\n",
    "        # reorder p-values and rejection mask to original order of pvals\n",
    "        pvals_corrected_ = np.empty_like(pvals_corrected)\n",
    "        pvals_corrected_[pvals_sortind] = pvals_corrected\n",
    "        reject_ = np.empty_like(reject)\n",
    "        reject_[pvals_sortind] = reject\n",
    "        return reject_, pvals_corrected_\n",
    "\n",
    "    def _nanrankdata(self, X, axis=1):\n",
    "        \"\"\"\n",
    "        Replaces bottleneck's nanrankdata with scipy and numpy alternative.\n",
    "        \"\"\"\n",
    "        ranks = sp.stats.mstats.rankdata(X, axis=axis)\n",
    "        ranks[np.isnan(X)] = np.nan\n",
    "        return ranks\n",
    "\n",
    "    def _check_params(self, X, y):\n",
    "        \"\"\"\n",
    "        Check hyperparameters as well as X and y before proceeding with fit.\n",
    "        \"\"\"\n",
    "        # check X and y are consistent len, X is Array and y is column\n",
    "        X, y = check_X_y(X, y)\n",
    "        if self.perc <= 0 or self.perc > 100:\n",
    "            raise ValueError('The percentile should be between 0 and 100.')\n",
    "\n",
    "        if self.alpha <= 0 or self.alpha > 1:\n",
    "            raise ValueError('Alpha should be between 0 and 1.')\n",
    "\n",
    "    def _print_results(self, dec_reg, _iter, flag):\n",
    "        n_iter = str(_iter) + ' / ' + str(self.max_iter)\n",
    "        n_confirmed = np.where(dec_reg == 1)[0].shape[0]\n",
    "        n_rejected = np.where(dec_reg == -1)[0].shape[0]\n",
    "        cols = ['Iteration: ', 'Confirmed: ', 'Tentative: ', 'Rejected: ']\n",
    "\n",
    "        # still in feature selection\n",
    "        if flag == 0:\n",
    "            n_tentative = np.where(dec_reg == 0)[0].shape[0]\n",
    "            content = map(str, [n_iter, n_confirmed, n_tentative, n_rejected])\n",
    "            if self.verbose == 1:\n",
    "                output = cols[0] + n_iter\n",
    "            elif self.verbose > 1:\n",
    "                output = '\\n'.join([x[0] + '\\t' + x[1] for x in zip(cols, content)])\n",
    "\n",
    "        # Boruta finished running and tentatives have been filtered\n",
    "        else:\n",
    "            n_tentative = np.sum(self.support_weak_)\n",
    "            content = map(str, [n_iter, n_confirmed, n_tentative, n_rejected])\n",
    "            result = '\\n'.join([x[0] + '\\t' + x[1] for x in zip(cols, content)])\n",
    "            output = \"\\n\\nBorutaPy finished running.\\n\\n\" + result\n",
    "        print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | lambda_l1 | lambda_l2 | max_depth | min_child | min_ch... | min_sp... | n_esti... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-2.781   \u001b[0m | \u001b[0m 181.9   \u001b[0m | \u001b[0m 102.6   \u001b[0m | \u001b[0m 4.355   \u001b[0m | \u001b[0m 132.0   \u001b[0m | \u001b[0m 15.41   \u001b[0m | \u001b[0m 0.07727 \u001b[0m | \u001b[0m 4.23e+03\u001b[0m | \u001b[0m 26.78   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-2.782   \u001b[0m | \u001b[0m 199.1   \u001b[0m | \u001b[0m 52.12   \u001b[0m | \u001b[0m 7.318   \u001b[0m | \u001b[0m 58.1    \u001b[0m | \u001b[0m 27.92   \u001b[0m | \u001b[0m 0.04823 \u001b[0m | \u001b[0m 1.254e+0\u001b[0m | \u001b[0m 49.07   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-2.781   \u001b[0m | \u001b[0m 149.2   \u001b[0m | \u001b[0m 27.45   \u001b[0m | \u001b[0m 5.759   \u001b[0m | \u001b[0m 14.91   \u001b[0m | \u001b[0m 36.73   \u001b[0m | \u001b[0m 0.05114 \u001b[0m | \u001b[0m 4.55e+03\u001b[0m | \u001b[0m 44.62   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-2.783   \u001b[0m | \u001b[0m 3.834   \u001b[0m | \u001b[0m 18.0    \u001b[0m | \u001b[0m 19.15   \u001b[0m | \u001b[0m 195.5   \u001b[0m | \u001b[0m 18.29   \u001b[0m | \u001b[0m 0.0208  \u001b[0m | \u001b[0m 6.983e+0\u001b[0m | \u001b[0m 43.9    \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-2.782   \u001b[0m | \u001b[0m 194.2   \u001b[0m | \u001b[0m 198.5   \u001b[0m | \u001b[0m 1.647   \u001b[0m | \u001b[0m 13.4    \u001b[0m | \u001b[0m 7.312   \u001b[0m | \u001b[0m 0.03357 \u001b[0m | \u001b[0m 6.994e+0\u001b[0m | \u001b[0m 24.42   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-2.786   \u001b[0m | \u001b[0m 6.581   \u001b[0m | \u001b[0m 193.0   \u001b[0m | \u001b[0m 2.185   \u001b[0m | \u001b[0m 13.87   \u001b[0m | \u001b[0m 45.05   \u001b[0m | \u001b[0m 0.06237 \u001b[0m | \u001b[0m 394.6   \u001b[0m | \u001b[0m 10.2    \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-2.782   \u001b[0m | \u001b[0m 199.7   \u001b[0m | \u001b[0m 192.1   \u001b[0m | \u001b[0m 19.64   \u001b[0m | \u001b[0m 189.6   \u001b[0m | \u001b[0m 48.71   \u001b[0m | \u001b[0m 0.08373 \u001b[0m | \u001b[0m 4.339e+0\u001b[0m | \u001b[0m 12.34   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-2.783   \u001b[0m | \u001b[0m 194.9   \u001b[0m | \u001b[0m 5.498   \u001b[0m | \u001b[0m 11.71   \u001b[0m | \u001b[0m 180.8   \u001b[0m | \u001b[0m 45.46   \u001b[0m | \u001b[0m 0.07918 \u001b[0m | \u001b[0m 6.945e+0\u001b[0m | \u001b[0m 37.02   \u001b[0m |\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m-2.781   \u001b[0m | \u001b[95m 194.0   \u001b[0m | \u001b[95m 4.27    \u001b[0m | \u001b[95m 4.106   \u001b[0m | \u001b[95m 5.97    \u001b[0m | \u001b[95m 3.62    \u001b[0m | \u001b[95m 0.04477 \u001b[0m | \u001b[95m 3.844e+0\u001b[0m | \u001b[95m 33.55   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-2.782   \u001b[0m | \u001b[0m 197.6   \u001b[0m | \u001b[0m 16.93   \u001b[0m | \u001b[0m 2.215   \u001b[0m | \u001b[0m 198.6   \u001b[0m | \u001b[0m 2.124   \u001b[0m | \u001b[0m 0.07534 \u001b[0m | \u001b[0m 3.011e+0\u001b[0m | \u001b[0m 37.72   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-2.792   \u001b[0m | \u001b[0m 194.3   \u001b[0m | \u001b[0m 13.74   \u001b[0m | \u001b[0m 1.3     \u001b[0m | \u001b[0m 8.455   \u001b[0m | \u001b[0m 44.51   \u001b[0m | \u001b[0m 0.03416 \u001b[0m | \u001b[0m 5.148e+0\u001b[0m | \u001b[0m 18.67   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-2.782   \u001b[0m | \u001b[0m 4.6     \u001b[0m | \u001b[0m 172.4   \u001b[0m | \u001b[0m 0.2403  \u001b[0m | \u001b[0m 7.931   \u001b[0m | \u001b[0m 44.06   \u001b[0m | \u001b[0m 0.07418 \u001b[0m | \u001b[0m 2.598e+0\u001b[0m | \u001b[0m 49.12   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-2.817   \u001b[0m | \u001b[0m 184.0   \u001b[0m | \u001b[0m 191.1   \u001b[0m | \u001b[0m 1.908   \u001b[0m | \u001b[0m 163.4   \u001b[0m | \u001b[0m 29.15   \u001b[0m | \u001b[0m 0.09374 \u001b[0m | \u001b[0m 51.3    \u001b[0m | \u001b[0m 49.19   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-2.783   \u001b[0m | \u001b[0m 4.917   \u001b[0m | \u001b[0m 6.278   \u001b[0m | \u001b[0m 16.46   \u001b[0m | \u001b[0m 198.8   \u001b[0m | \u001b[0m 26.42   \u001b[0m | \u001b[0m 0.05258 \u001b[0m | \u001b[0m 1.29e+03\u001b[0m | \u001b[0m 32.54   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-2.8     \u001b[0m | \u001b[0m 4.207   \u001b[0m | \u001b[0m 5.316   \u001b[0m | \u001b[0m 2.616   \u001b[0m | \u001b[0m 21.35   \u001b[0m | \u001b[0m 46.34   \u001b[0m | \u001b[0m 0.06441 \u001b[0m | \u001b[0m 99.31   \u001b[0m | \u001b[0m 44.66   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-2.782   \u001b[0m | \u001b[0m 0.2954  \u001b[0m | \u001b[0m 196.8   \u001b[0m | \u001b[0m 16.65   \u001b[0m | \u001b[0m 14.38   \u001b[0m | \u001b[0m 3.782   \u001b[0m | \u001b[0m 0.09175 \u001b[0m | \u001b[0m 1.156e+0\u001b[0m | \u001b[0m 49.83   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-2.782   \u001b[0m | \u001b[0m 5.209   \u001b[0m | \u001b[0m 3.431   \u001b[0m | \u001b[0m 0.2913  \u001b[0m | \u001b[0m 187.1   \u001b[0m | \u001b[0m 42.4    \u001b[0m | \u001b[0m 0.09581 \u001b[0m | \u001b[0m 3.857e+0\u001b[0m | \u001b[0m 14.34   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-2.782   \u001b[0m | \u001b[0m 68.06   \u001b[0m | \u001b[0m 1.576   \u001b[0m | \u001b[0m 13.22   \u001b[0m | \u001b[0m 20.35   \u001b[0m | \u001b[0m 47.56   \u001b[0m | \u001b[0m 0.04219 \u001b[0m | \u001b[0m 1.916e+0\u001b[0m | \u001b[0m 10.38   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-2.781   \u001b[0m | \u001b[0m 33.77   \u001b[0m | \u001b[0m 198.5   \u001b[0m | \u001b[0m 4.93    \u001b[0m | \u001b[0m 196.1   \u001b[0m | \u001b[0m 31.23   \u001b[0m | \u001b[0m 0.08675 \u001b[0m | \u001b[0m 6.973e+0\u001b[0m | \u001b[0m 49.34   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-2.782   \u001b[0m | \u001b[0m 26.34   \u001b[0m | \u001b[0m 4.759   \u001b[0m | \u001b[0m 2.519   \u001b[0m | \u001b[0m 20.08   \u001b[0m | \u001b[0m 41.16   \u001b[0m | \u001b[0m 0.05846 \u001b[0m | \u001b[0m 6.995e+0\u001b[0m | \u001b[0m 34.14   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-2.782   \u001b[0m | \u001b[0m 195.8   \u001b[0m | \u001b[0m 45.75   \u001b[0m | \u001b[0m 10.68   \u001b[0m | \u001b[0m 187.9   \u001b[0m | \u001b[0m 44.12   \u001b[0m | \u001b[0m 0.06664 \u001b[0m | \u001b[0m 3.767e+0\u001b[0m | \u001b[0m 49.16   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-2.783   \u001b[0m | \u001b[0m 24.48   \u001b[0m | \u001b[0m 11.25   \u001b[0m | \u001b[0m 1.657   \u001b[0m | \u001b[0m 5.232   \u001b[0m | \u001b[0m 9.863   \u001b[0m | \u001b[0m 0.04904 \u001b[0m | \u001b[0m 909.3   \u001b[0m | \u001b[0m 12.9    \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-2.781   \u001b[0m | \u001b[0m 187.5   \u001b[0m | \u001b[0m 197.5   \u001b[0m | \u001b[0m 2.553   \u001b[0m | \u001b[0m 198.7   \u001b[0m | \u001b[0m 44.77   \u001b[0m | \u001b[0m 0.03741 \u001b[0m | \u001b[0m 2.198e+0\u001b[0m | \u001b[0m 11.31   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-2.792   \u001b[0m | \u001b[0m 19.0    \u001b[0m | \u001b[0m 186.5   \u001b[0m | \u001b[0m 1.071   \u001b[0m | \u001b[0m 6.683   \u001b[0m | \u001b[0m 24.7    \u001b[0m | \u001b[0m 0.0792  \u001b[0m | \u001b[0m 4.261e+0\u001b[0m | \u001b[0m 49.31   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-2.781   \u001b[0m | \u001b[0m 9.139   \u001b[0m | \u001b[0m 189.3   \u001b[0m | \u001b[0m 5.591   \u001b[0m | \u001b[0m 14.92   \u001b[0m | \u001b[0m 49.66   \u001b[0m | \u001b[0m 0.09371 \u001b[0m | \u001b[0m 1.331e+0\u001b[0m | \u001b[0m 20.08   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-2.782   \u001b[0m | \u001b[0m 3.791   \u001b[0m | \u001b[0m 164.8   \u001b[0m | \u001b[0m 12.39   \u001b[0m | \u001b[0m 145.8   \u001b[0m | \u001b[0m 49.66   \u001b[0m | \u001b[0m 0.09032 \u001b[0m | \u001b[0m 6.996e+0\u001b[0m | \u001b[0m 10.85   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-2.782   \u001b[0m | \u001b[0m 191.6   \u001b[0m | \u001b[0m 14.12   \u001b[0m | \u001b[0m 7.086   \u001b[0m | \u001b[0m 199.3   \u001b[0m | \u001b[0m 44.51   \u001b[0m | \u001b[0m 0.06945 \u001b[0m | \u001b[0m 4.614e+0\u001b[0m | \u001b[0m 11.81   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-2.782   \u001b[0m | \u001b[0m 183.4   \u001b[0m | \u001b[0m 3.707   \u001b[0m | \u001b[0m 13.82   \u001b[0m | \u001b[0m 9.018   \u001b[0m | \u001b[0m 44.77   \u001b[0m | \u001b[0m 0.08912 \u001b[0m | \u001b[0m 2.637e+0\u001b[0m | \u001b[0m 23.98   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-2.781   \u001b[0m | \u001b[0m 8.195   \u001b[0m | \u001b[0m 4.781   \u001b[0m | \u001b[0m 2.665   \u001b[0m | \u001b[0m 168.5   \u001b[0m | \u001b[0m 49.49   \u001b[0m | \u001b[0m 0.03552 \u001b[0m | \u001b[0m 2.056e+0\u001b[0m | \u001b[0m 49.81   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-2.782   \u001b[0m | \u001b[0m 27.39   \u001b[0m | \u001b[0m 93.71   \u001b[0m | \u001b[0m 15.16   \u001b[0m | \u001b[0m 192.0   \u001b[0m | \u001b[0m 48.9    \u001b[0m | \u001b[0m 0.06524 \u001b[0m | \u001b[0m 2.769e+0\u001b[0m | \u001b[0m 10.34   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-2.782   \u001b[0m | \u001b[0m 179.2   \u001b[0m | \u001b[0m 172.9   \u001b[0m | \u001b[0m 17.04   \u001b[0m | \u001b[0m 36.05   \u001b[0m | \u001b[0m 48.99   \u001b[0m | \u001b[0m 0.08781 \u001b[0m | \u001b[0m 6.99e+03\u001b[0m | \u001b[0m 46.99   \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-2.782   \u001b[0m | \u001b[0m 1.732   \u001b[0m | \u001b[0m 42.69   \u001b[0m | \u001b[0m 2.957   \u001b[0m | \u001b[0m 18.44   \u001b[0m | \u001b[0m 46.02   \u001b[0m | \u001b[0m 0.03696 \u001b[0m | \u001b[0m 899.4   \u001b[0m | \u001b[0m 49.3    \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-2.782   \u001b[0m | \u001b[0m 191.7   \u001b[0m | \u001b[0m 6.097   \u001b[0m | \u001b[0m-0.4417  \u001b[0m | \u001b[0m 147.1   \u001b[0m | \u001b[0m 44.29   \u001b[0m | \u001b[0m 0.09894 \u001b[0m | \u001b[0m 4.287e+0\u001b[0m | \u001b[0m 13.13   \u001b[0m |\n",
      "=========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "df_train['outliers'] = 0\n",
    "df_train.loc[df_train['target'] < -30, 'outliers'] = 1\n",
    "obj=list(df_train.select_dtypes(include=['object']).columns)[2:]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb=LabelEncoder()\n",
    "for o in obj:\n",
    "    df_train[o]=lb.fit_transform(df_train[o]) \n",
    "df_train_columns = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','target','outliers','Unnamed: 0','Unnamed: 0_x']]\n",
    "def optim_lgb(min_child,n_estimator,max_depth,num_leaves,lambda_l1,lambda_l2,min_split_gain,min_child_weight):\n",
    "    def run_lgb(train_data,val_data,val_x,min_child,n_estimator,max_depth,num_leaves,lambda_l1,lambda_l2,min_split_gain,min_child_weight):\n",
    "        num_round = 10000\n",
    "        params = {\n",
    "                    \"objective\" : \"regression_l2\",\n",
    "                    \"metric\" : \"rmse\", \n",
    "                    \"boosting\": \"gbdt\",\n",
    "                    'n_jobs': 6,\n",
    "                   'learning_rate': 0.008,\n",
    "                    'max_cat_to_onehot': 4, \n",
    "                     'min_child_samples': int(min_child),\n",
    "                    'cat_smooth': 10.0,\n",
    "                    'n_estimators': int(n_estimator),\n",
    "                     'max_depth':int(round(max_depth)),\n",
    "                     'num_leaves':int(num_leaves),\n",
    "                     'lambda_l1':max(lambda_l1, 0),\n",
    "                     'lambda_l2':max(lambda_l2, 0),\n",
    "                     'min_split_gain':min_split_gain,\n",
    "                     'min_child_weight':min_child_weight\n",
    "                }\n",
    "        model=lgb.train(params,train_data,num_round,valid_sets=[train_data,val_data],verbose_eval=False,early_stopping_rounds=400)\n",
    "\n",
    "        pred_val = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "    #    return pred_val\n",
    "        return  pred_val\n",
    "    rskf=StratifiedKFold(5,shuffle=True,random_state=315)\n",
    "    val_pr=np.zeros(len(df_train))\n",
    "    for train_index,val_index in rskf.split(df_train,df_train['outliers'].values):\n",
    "        train_data=lgb.Dataset(df_train[df_train_columns].loc[train_index],label=df_train['target'].loc[train_index])\n",
    "        val_data=lgb.Dataset(df_train[df_train_columns].loc[val_index],label=df_train['target'].loc[val_index])\n",
    "        val_pr[val_index]=run_lgb(train_data,val_data,df_train[df_train_columns].loc[val_index],min_child,n_estimator,max_depth,num_leaves,lambda_l1,lambda_l2,min_split_gain,min_child_weight)\n",
    "    return 1-np.sqrt(mean_squared_error(val_pr,df_train['target']))\n",
    "    \n",
    "from bayes_opt import BayesianOptimization\n",
    "optimizer = BayesianOptimization(optim_lgb,{'min_child':(5,200),'n_estimator':(50,7000),'max_depth':(-1,20),'num_leaves':(10,50),'lambda_l1':(0,200),'lambda_l2':(0,200),\n",
    "'min_split_gain':(0.001, 0.1),'min_child_weight':(2,50)}\n",
    "    \n",
    ")\n",
    "optimizer.maximize(init_points=3,\n",
    "    n_iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 3.8659009\ttest: 3.8797576\tbest: 3.8797576 (0)\ttotal: 3.02s\tremaining: 50m 22s\n",
      "1:\tlearn: 3.8640229\ttest: 3.8781945\tbest: 3.8781945 (1)\ttotal: 6.12s\tremaining: 50m 54s\n",
      "2:\tlearn: 3.8621282\ttest: 3.8766670\tbest: 3.8766670 (2)\ttotal: 9.11s\tremaining: 50m 25s\n",
      "3:\tlearn: 3.8602801\ttest: 3.8751953\tbest: 3.8751953 (3)\ttotal: 11.6s\tremaining: 48m 18s\n",
      "4:\tlearn: 3.8585214\ttest: 3.8737509\tbest: 3.8737509 (4)\ttotal: 14.4s\tremaining: 47m 48s\n",
      "5:\tlearn: 3.8567039\ttest: 3.8723242\tbest: 3.8723242 (5)\ttotal: 17.7s\tremaining: 48m 54s\n",
      "6:\tlearn: 3.8548921\ttest: 3.8709073\tbest: 3.8709073 (6)\ttotal: 21.1s\tremaining: 49m 46s\n",
      "7:\tlearn: 3.8532044\ttest: 3.8695107\tbest: 3.8695107 (7)\ttotal: 23.6s\tremaining: 48m 50s\n",
      "8:\tlearn: 3.8516083\ttest: 3.8680850\tbest: 3.8680850 (8)\ttotal: 26.5s\tremaining: 48m 40s\n",
      "9:\tlearn: 3.8499003\ttest: 3.8667534\tbest: 3.8667534 (9)\ttotal: 29.7s\tremaining: 48m 58s\n",
      "10:\tlearn: 3.8481348\ttest: 3.8654440\tbest: 3.8654440 (10)\ttotal: 33s\tremaining: 49m 28s\n",
      "11:\tlearn: 3.8465372\ttest: 3.8640776\tbest: 3.8640776 (11)\ttotal: 35.7s\tremaining: 48m 56s\n",
      "12:\tlearn: 3.8449506\ttest: 3.8628155\tbest: 3.8628155 (12)\ttotal: 38.3s\tremaining: 48m 31s\n",
      "13:\tlearn: 3.8433535\ttest: 3.8614982\tbest: 3.8614982 (13)\ttotal: 41.1s\tremaining: 48m 16s\n",
      "14:\tlearn: 3.8418549\ttest: 3.8603125\tbest: 3.8603125 (14)\ttotal: 45.2s\tremaining: 49m 29s\n",
      "15:\tlearn: 3.8401323\ttest: 3.8591058\tbest: 3.8591058 (15)\ttotal: 48.9s\tremaining: 50m 9s\n",
      "16:\tlearn: 3.8385585\ttest: 3.8579914\tbest: 3.8579914 (16)\ttotal: 51.5s\tremaining: 49m 39s\n",
      "17:\tlearn: 3.8369408\ttest: 3.8568510\tbest: 3.8568510 (17)\ttotal: 54.3s\tremaining: 49m 20s\n",
      "18:\tlearn: 3.8355162\ttest: 3.8557072\tbest: 3.8557072 (18)\ttotal: 56.7s\tremaining: 48m 49s\n",
      "19:\tlearn: 3.8340461\ttest: 3.8545965\tbest: 3.8545965 (19)\ttotal: 59.3s\tremaining: 48m 23s\n",
      "20:\tlearn: 3.8325951\ttest: 3.8534731\tbest: 3.8534731 (20)\ttotal: 1m 1s\tremaining: 48m\n",
      "21:\tlearn: 3.8311105\ttest: 3.8523986\tbest: 3.8523986 (21)\ttotal: 1m 4s\tremaining: 47m 38s\n",
      "22:\tlearn: 3.8296355\ttest: 3.8513727\tbest: 3.8513727 (22)\ttotal: 1m 6s\tremaining: 47m 20s\n",
      "23:\tlearn: 3.8283053\ttest: 3.8503627\tbest: 3.8503627 (23)\ttotal: 1m 9s\tremaining: 47m 3s\n",
      "24:\tlearn: 3.8269963\ttest: 3.8493141\tbest: 3.8493141 (24)\ttotal: 1m 12s\tremaining: 46m 53s\n",
      "25:\tlearn: 3.8256429\ttest: 3.8483181\tbest: 3.8483181 (25)\ttotal: 1m 15s\tremaining: 46m 59s\n",
      "26:\tlearn: 3.8244018\ttest: 3.8473345\tbest: 3.8473345 (26)\ttotal: 1m 18s\tremaining: 47m 12s\n",
      "27:\tlearn: 3.8228941\ttest: 3.8464079\tbest: 3.8464079 (27)\ttotal: 1m 21s\tremaining: 47m 18s\n",
      "28:\tlearn: 3.8217048\ttest: 3.8454382\tbest: 3.8454382 (28)\ttotal: 1m 25s\tremaining: 47m 33s\n",
      "29:\tlearn: 3.8203032\ttest: 3.8445359\tbest: 3.8445359 (29)\ttotal: 1m 28s\tremaining: 47m 38s\n",
      "30:\tlearn: 3.8190655\ttest: 3.8436614\tbest: 3.8436614 (30)\ttotal: 1m 31s\tremaining: 47m 53s\n",
      "31:\tlearn: 3.8178226\ttest: 3.8427287\tbest: 3.8427287 (31)\ttotal: 1m 34s\tremaining: 47m 40s\n",
      "32:\tlearn: 3.8166153\ttest: 3.8418432\tbest: 3.8418432 (32)\ttotal: 1m 36s\tremaining: 47m 21s\n",
      "33:\tlearn: 3.8154287\ttest: 3.8409414\tbest: 3.8409414 (33)\ttotal: 1m 40s\tremaining: 47m 45s\n",
      "34:\tlearn: 3.8142737\ttest: 3.8401046\tbest: 3.8401046 (34)\ttotal: 1m 44s\tremaining: 47m 57s\n",
      "35:\tlearn: 3.8131432\ttest: 3.8393087\tbest: 3.8393087 (35)\ttotal: 1m 47s\tremaining: 47m 49s\n",
      "36:\tlearn: 3.8120394\ttest: 3.8384927\tbest: 3.8384927 (36)\ttotal: 1m 50s\tremaining: 47m 46s\n",
      "37:\tlearn: 3.8109246\ttest: 3.8376759\tbest: 3.8376759 (37)\ttotal: 1m 53s\tremaining: 47m 52s\n",
      "38:\tlearn: 3.8098532\ttest: 3.8368929\tbest: 3.8368929 (38)\ttotal: 1m 56s\tremaining: 47m 46s\n",
      "39:\tlearn: 3.8088394\ttest: 3.8360647\tbest: 3.8360647 (39)\ttotal: 1m 59s\tremaining: 47m 41s\n",
      "40:\tlearn: 3.8076503\ttest: 3.8352864\tbest: 3.8352864 (40)\ttotal: 2m 3s\tremaining: 48m 12s\n",
      "41:\tlearn: 3.8066642\ttest: 3.8345620\tbest: 3.8345620 (41)\ttotal: 2m 6s\tremaining: 48m 3s\n",
      "42:\tlearn: 3.8055328\ttest: 3.8337782\tbest: 3.8337782 (42)\ttotal: 2m 8s\tremaining: 47m 48s\n",
      "43:\tlearn: 3.8046256\ttest: 3.8330217\tbest: 3.8330217 (43)\ttotal: 2m 11s\tremaining: 47m 42s\n",
      "44:\tlearn: 3.8036085\ttest: 3.8323444\tbest: 3.8323444 (44)\ttotal: 2m 14s\tremaining: 47m 26s\n",
      "45:\tlearn: 3.8024294\ttest: 3.8316377\tbest: 3.8316377 (45)\ttotal: 2m 16s\tremaining: 47m 14s\n",
      "46:\tlearn: 3.8014233\ttest: 3.8309399\tbest: 3.8309399 (46)\ttotal: 2m 19s\tremaining: 47m\n",
      "47:\tlearn: 3.8006080\ttest: 3.8302841\tbest: 3.8302841 (47)\ttotal: 2m 22s\tremaining: 46m 56s\n",
      "48:\tlearn: 3.7994232\ttest: 3.8296934\tbest: 3.8296934 (48)\ttotal: 2m 24s\tremaining: 46m 52s\n",
      "49:\tlearn: 3.7981617\ttest: 3.8291614\tbest: 3.8291614 (49)\ttotal: 2m 27s\tremaining: 46m 38s\n",
      "50:\tlearn: 3.7972229\ttest: 3.8285251\tbest: 3.8285251 (50)\ttotal: 2m 29s\tremaining: 46m 26s\n",
      "51:\tlearn: 3.7963977\ttest: 3.8278626\tbest: 3.8278626 (51)\ttotal: 2m 32s\tremaining: 46m 13s\n",
      "52:\tlearn: 3.7953382\ttest: 3.8272196\tbest: 3.8272196 (52)\ttotal: 2m 34s\tremaining: 46m 1s\n",
      "53:\tlearn: 3.7943068\ttest: 3.8265450\tbest: 3.8265450 (53)\ttotal: 2m 36s\tremaining: 45m 48s\n",
      "54:\tlearn: 3.7933727\ttest: 3.8259232\tbest: 3.8259232 (54)\ttotal: 2m 39s\tremaining: 45m 39s\n",
      "55:\tlearn: 3.7923413\ttest: 3.8253889\tbest: 3.8253889 (55)\ttotal: 2m 41s\tremaining: 45m 28s\n",
      "56:\tlearn: 3.7914852\ttest: 3.8247681\tbest: 3.8247681 (56)\ttotal: 2m 44s\tremaining: 45m 22s\n",
      "57:\tlearn: 3.7906627\ttest: 3.8241634\tbest: 3.8241634 (57)\ttotal: 2m 46s\tremaining: 45m 10s\n",
      "58:\tlearn: 3.7898460\ttest: 3.8236505\tbest: 3.8236505 (58)\ttotal: 2m 49s\tremaining: 45m 9s\n",
      "59:\tlearn: 3.7888229\ttest: 3.8231664\tbest: 3.8231664 (59)\ttotal: 2m 52s\tremaining: 44m 59s\n",
      "60:\tlearn: 3.7878947\ttest: 3.8226909\tbest: 3.8226909 (60)\ttotal: 2m 54s\tremaining: 44m 52s\n",
      "61:\tlearn: 3.7869280\ttest: 3.8221952\tbest: 3.8221952 (61)\ttotal: 2m 57s\tremaining: 44m 51s\n",
      "62:\tlearn: 3.7858602\ttest: 3.8217429\tbest: 3.8217429 (62)\ttotal: 3m\tremaining: 44m 50s\n",
      "63:\tlearn: 3.7848936\ttest: 3.8213124\tbest: 3.8213124 (63)\ttotal: 3m 4s\tremaining: 44m 59s\n",
      "64:\tlearn: 3.7841795\ttest: 3.8207840\tbest: 3.8207840 (64)\ttotal: 3m 7s\tremaining: 44m 57s\n",
      "65:\tlearn: 3.7831602\ttest: 3.8203279\tbest: 3.8203279 (65)\ttotal: 3m 11s\tremaining: 45m 5s\n",
      "66:\tlearn: 3.7823842\ttest: 3.8198851\tbest: 3.8198851 (66)\ttotal: 3m 14s\tremaining: 45m 4s\n",
      "67:\tlearn: 3.7814602\ttest: 3.8194933\tbest: 3.8194933 (67)\ttotal: 3m 18s\tremaining: 45m 17s\n",
      "68:\tlearn: 3.7807300\ttest: 3.8190394\tbest: 3.8190394 (68)\ttotal: 3m 22s\tremaining: 45m 28s\n",
      "69:\tlearn: 3.7800100\ttest: 3.8186095\tbest: 3.8186095 (69)\ttotal: 3m 25s\tremaining: 45m 24s\n",
      "70:\tlearn: 3.7791808\ttest: 3.8181786\tbest: 3.8181786 (70)\ttotal: 3m 27s\tremaining: 45m 16s\n",
      "71:\tlearn: 3.7783473\ttest: 3.8177300\tbest: 3.8177300 (71)\ttotal: 3m 30s\tremaining: 45m 9s\n",
      "72:\tlearn: 3.7773821\ttest: 3.8173640\tbest: 3.8173640 (72)\ttotal: 3m 33s\tremaining: 45m 6s\n",
      "73:\tlearn: 3.7765054\ttest: 3.8169948\tbest: 3.8169948 (73)\ttotal: 3m 36s\tremaining: 45m 13s\n",
      "74:\tlearn: 3.7758916\ttest: 3.8165649\tbest: 3.8165649 (74)\ttotal: 3m 40s\tremaining: 45m 24s\n",
      "75:\tlearn: 3.7750977\ttest: 3.8162064\tbest: 3.8162064 (75)\ttotal: 3m 43s\tremaining: 45m 21s\n",
      "76:\tlearn: 3.7744022\ttest: 3.8158343\tbest: 3.8158343 (76)\ttotal: 3m 46s\tremaining: 45m 18s\n",
      "77:\tlearn: 3.7736300\ttest: 3.8155064\tbest: 3.8155064 (77)\ttotal: 3m 49s\tremaining: 45m 10s\n",
      "78:\tlearn: 3.7730079\ttest: 3.8151194\tbest: 3.8151194 (78)\ttotal: 3m 52s\tremaining: 45m 6s\n",
      "79:\tlearn: 3.7721572\ttest: 3.8148485\tbest: 3.8148485 (79)\ttotal: 3m 54s\tremaining: 44m 57s\n",
      "80:\tlearn: 3.7715357\ttest: 3.8144890\tbest: 3.8144890 (80)\ttotal: 3m 57s\tremaining: 44m 49s\n",
      "81:\tlearn: 3.7708598\ttest: 3.8141357\tbest: 3.8141357 (81)\ttotal: 4m\tremaining: 44m 48s\n",
      "82:\tlearn: 3.7702441\ttest: 3.8137887\tbest: 3.8137887 (82)\ttotal: 4m 2s\tremaining: 44m 39s\n",
      "83:\tlearn: 3.7695554\ttest: 3.8134409\tbest: 3.8134409 (83)\ttotal: 4m 4s\tremaining: 44m 30s\n",
      "84:\tlearn: 3.7688385\ttest: 3.8131089\tbest: 3.8131089 (84)\ttotal: 4m 7s\tremaining: 44m 29s\n",
      "85:\tlearn: 3.7682741\ttest: 3.8127450\tbest: 3.8127450 (85)\ttotal: 4m 10s\tremaining: 44m 27s\n",
      "86:\tlearn: 3.7676518\ttest: 3.8123880\tbest: 3.8123880 (86)\ttotal: 4m 13s\tremaining: 44m 22s\n",
      "87:\tlearn: 3.7667612\ttest: 3.8121236\tbest: 3.8121236 (87)\ttotal: 4m 16s\tremaining: 44m 15s\n",
      "88:\tlearn: 3.7658727\ttest: 3.8118043\tbest: 3.8118043 (88)\ttotal: 4m 18s\tremaining: 44m 7s\n",
      "89:\tlearn: 3.7650630\ttest: 3.8115181\tbest: 3.8115181 (89)\ttotal: 4m 21s\tremaining: 43m 59s\n",
      "90:\tlearn: 3.7644985\ttest: 3.8111822\tbest: 3.8111822 (90)\ttotal: 4m 23s\tremaining: 43m 56s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91:\tlearn: 3.7639024\ttest: 3.8109117\tbest: 3.8109117 (91)\ttotal: 4m 26s\tremaining: 43m 55s\n",
      "92:\tlearn: 3.7631093\ttest: 3.8106243\tbest: 3.8106243 (92)\ttotal: 4m 29s\tremaining: 43m 47s\n",
      "93:\tlearn: 3.7623956\ttest: 3.8103903\tbest: 3.8103903 (93)\ttotal: 4m 32s\tremaining: 43m 45s\n",
      "94:\tlearn: 3.7616474\ttest: 3.8100588\tbest: 3.8100588 (94)\ttotal: 4m 35s\tremaining: 43m 48s\n",
      "95:\tlearn: 3.7610226\ttest: 3.8097469\tbest: 3.8097469 (95)\ttotal: 4m 38s\tremaining: 43m 40s\n",
      "96:\tlearn: 3.7603253\ttest: 3.8095000\tbest: 3.8095000 (96)\ttotal: 4m 40s\tremaining: 43m 35s\n",
      "97:\tlearn: 3.7597873\ttest: 3.8092605\tbest: 3.8092605 (97)\ttotal: 4m 43s\tremaining: 43m 27s\n",
      "98:\tlearn: 3.7590375\ttest: 3.8090170\tbest: 3.8090170 (98)\ttotal: 4m 45s\tremaining: 43m 20s\n",
      "99:\tlearn: 3.7585347\ttest: 3.8087155\tbest: 3.8087155 (99)\ttotal: 4m 48s\tremaining: 43m 12s\n",
      "100:\tlearn: 3.7578420\ttest: 3.8083986\tbest: 3.8083986 (100)\ttotal: 4m 51s\tremaining: 43m 17s\n",
      "101:\tlearn: 3.7572971\ttest: 3.8081076\tbest: 3.8081076 (101)\ttotal: 4m 55s\tremaining: 43m 19s\n",
      "102:\tlearn: 3.7567088\ttest: 3.8078915\tbest: 3.8078915 (102)\ttotal: 4m 57s\tremaining: 43m 14s\n",
      "103:\tlearn: 3.7558723\ttest: 3.8077159\tbest: 3.8077159 (103)\ttotal: 5m\tremaining: 43m 7s\n",
      "104:\tlearn: 3.7551758\ttest: 3.8075685\tbest: 3.8075685 (104)\ttotal: 5m 2s\tremaining: 43m\n",
      "105:\tlearn: 3.7545590\ttest: 3.8073527\tbest: 3.8073527 (105)\ttotal: 5m 5s\tremaining: 42m 56s\n",
      "106:\tlearn: 3.7538182\ttest: 3.8071068\tbest: 3.8071068 (106)\ttotal: 5m 8s\tremaining: 42m 53s\n",
      "107:\tlearn: 3.7531842\ttest: 3.8068712\tbest: 3.8068712 (107)\ttotal: 5m 12s\tremaining: 42m 58s\n",
      "108:\tlearn: 3.7526103\ttest: 3.8066437\tbest: 3.8066437 (108)\ttotal: 5m 15s\tremaining: 43m 3s\n",
      "109:\tlearn: 3.7520050\ttest: 3.8064457\tbest: 3.8064457 (109)\ttotal: 5m 19s\tremaining: 43m 4s\n",
      "110:\tlearn: 3.7513086\ttest: 3.8063040\tbest: 3.8063040 (110)\ttotal: 5m 22s\tremaining: 43m 6s\n",
      "111:\tlearn: 3.7506386\ttest: 3.8060892\tbest: 3.8060892 (111)\ttotal: 5m 25s\tremaining: 43m 4s\n",
      "112:\tlearn: 3.7499910\ttest: 3.8058989\tbest: 3.8058989 (112)\ttotal: 5m 28s\tremaining: 43m\n",
      "113:\tlearn: 3.7495785\ttest: 3.8057251\tbest: 3.8057251 (113)\ttotal: 5m 31s\tremaining: 42m 58s\n",
      "114:\tlearn: 3.7488701\ttest: 3.8056000\tbest: 3.8056000 (114)\ttotal: 5m 35s\tremaining: 43m\n",
      "115:\tlearn: 3.7483397\ttest: 3.8054310\tbest: 3.8054310 (115)\ttotal: 5m 38s\tremaining: 42m 56s\n",
      "116:\tlearn: 3.7479156\ttest: 3.8052011\tbest: 3.8052011 (116)\ttotal: 5m 40s\tremaining: 42m 50s\n",
      "117:\tlearn: 3.7474253\ttest: 3.8049735\tbest: 3.8049735 (117)\ttotal: 5m 43s\tremaining: 42m 49s\n",
      "118:\tlearn: 3.7469471\ttest: 3.8048170\tbest: 3.8048170 (118)\ttotal: 5m 47s\tremaining: 42m 50s\n",
      "119:\tlearn: 3.7465399\ttest: 3.8046620\tbest: 3.8046620 (119)\ttotal: 5m 50s\tremaining: 42m 49s\n",
      "120:\tlearn: 3.7461730\ttest: 3.8044654\tbest: 3.8044654 (120)\ttotal: 5m 54s\tremaining: 42m 54s\n",
      "121:\tlearn: 3.7457262\ttest: 3.8042864\tbest: 3.8042864 (121)\ttotal: 5m 57s\tremaining: 42m 53s\n",
      "122:\tlearn: 3.7452157\ttest: 3.8041276\tbest: 3.8041276 (122)\ttotal: 6m 1s\tremaining: 42m 54s\n",
      "123:\tlearn: 3.7447567\ttest: 3.8039397\tbest: 3.8039397 (123)\ttotal: 6m 4s\tremaining: 42m 54s\n",
      "124:\tlearn: 3.7442043\ttest: 3.8037716\tbest: 3.8037716 (124)\ttotal: 6m 7s\tremaining: 42m 51s\n",
      "125:\tlearn: 3.7438133\ttest: 3.8036139\tbest: 3.8036139 (125)\ttotal: 6m 10s\tremaining: 42m 49s\n",
      "126:\tlearn: 3.7432162\ttest: 3.8035082\tbest: 3.8035082 (126)\ttotal: 6m 13s\tremaining: 42m 46s\n",
      "127:\tlearn: 3.7425774\ttest: 3.8033598\tbest: 3.8033598 (127)\ttotal: 6m 16s\tremaining: 42m 41s\n",
      "128:\tlearn: 3.7419069\ttest: 3.8032335\tbest: 3.8032335 (128)\ttotal: 6m 19s\tremaining: 42m 39s\n",
      "129:\tlearn: 3.7410962\ttest: 3.8030955\tbest: 3.8030955 (129)\ttotal: 6m 21s\tremaining: 42m 34s\n",
      "130:\tlearn: 3.7406075\ttest: 3.8029101\tbest: 3.8029101 (130)\ttotal: 6m 24s\tremaining: 42m 29s\n",
      "131:\tlearn: 3.7402076\ttest: 3.8027815\tbest: 3.8027815 (131)\ttotal: 6m 26s\tremaining: 42m 24s\n",
      "132:\tlearn: 3.7397830\ttest: 3.8026136\tbest: 3.8026136 (132)\ttotal: 6m 29s\tremaining: 42m 19s\n",
      "133:\tlearn: 3.7394090\ttest: 3.8024180\tbest: 3.8024180 (133)\ttotal: 6m 32s\tremaining: 42m 19s\n",
      "134:\tlearn: 3.7390064\ttest: 3.8023075\tbest: 3.8023075 (134)\ttotal: 6m 35s\tremaining: 42m 14s\n",
      "135:\tlearn: 3.7385089\ttest: 3.8021877\tbest: 3.8021877 (135)\ttotal: 6m 38s\tremaining: 42m 10s\n",
      "136:\tlearn: 3.7380285\ttest: 3.8021045\tbest: 3.8021045 (136)\ttotal: 6m 40s\tremaining: 42m 4s\n",
      "137:\tlearn: 3.7376482\ttest: 3.8019818\tbest: 3.8019818 (137)\ttotal: 6m 43s\tremaining: 42m\n",
      "138:\tlearn: 3.7370525\ttest: 3.8018619\tbest: 3.8018619 (138)\ttotal: 6m 45s\tremaining: 41m 54s\n",
      "139:\tlearn: 3.7365508\ttest: 3.8016746\tbest: 3.8016746 (139)\ttotal: 6m 48s\tremaining: 41m 48s\n",
      "140:\tlearn: 3.7361564\ttest: 3.8015728\tbest: 3.8015728 (140)\ttotal: 6m 50s\tremaining: 41m 42s\n",
      "141:\tlearn: 3.7355246\ttest: 3.8014063\tbest: 3.8014063 (141)\ttotal: 6m 53s\tremaining: 41m 36s\n",
      "142:\tlearn: 3.7350272\ttest: 3.8012624\tbest: 3.8012624 (142)\ttotal: 6m 55s\tremaining: 41m 31s\n",
      "143:\tlearn: 3.7346018\ttest: 3.8011463\tbest: 3.8011463 (143)\ttotal: 6m 58s\tremaining: 41m 25s\n",
      "144:\tlearn: 3.7341254\ttest: 3.8010444\tbest: 3.8010444 (144)\ttotal: 7m\tremaining: 41m 20s\n",
      "145:\tlearn: 3.7335452\ttest: 3.8008455\tbest: 3.8008455 (145)\ttotal: 7m 3s\tremaining: 41m 14s\n",
      "146:\tlearn: 3.7330652\ttest: 3.8007669\tbest: 3.8007669 (146)\ttotal: 7m 5s\tremaining: 41m 10s\n",
      "147:\tlearn: 3.7326023\ttest: 3.8006396\tbest: 3.8006396 (147)\ttotal: 7m 8s\tremaining: 41m 9s\n",
      "148:\tlearn: 3.7320762\ttest: 3.8005568\tbest: 3.8005568 (148)\ttotal: 7m 11s\tremaining: 41m 6s\n",
      "149:\tlearn: 3.7316568\ttest: 3.8004448\tbest: 3.8004448 (149)\ttotal: 7m 14s\tremaining: 41m 4s\n",
      "150:\tlearn: 3.7311972\ttest: 3.8003489\tbest: 3.8003489 (150)\ttotal: 7m 17s\tremaining: 41m 1s\n",
      "151:\tlearn: 3.7307806\ttest: 3.8003207\tbest: 3.8003207 (151)\ttotal: 7m 20s\tremaining: 40m 55s\n",
      "152:\tlearn: 3.7304773\ttest: 3.8002083\tbest: 3.8002083 (152)\ttotal: 7m 22s\tremaining: 40m 49s\n",
      "153:\tlearn: 3.7301336\ttest: 3.8000933\tbest: 3.8000933 (153)\ttotal: 7m 25s\tremaining: 40m 46s\n",
      "154:\tlearn: 3.7298126\ttest: 3.7999823\tbest: 3.7999823 (154)\ttotal: 7m 28s\tremaining: 40m 43s\n",
      "155:\tlearn: 3.7292872\ttest: 3.7998792\tbest: 3.7998792 (155)\ttotal: 7m 31s\tremaining: 40m 40s\n",
      "156:\tlearn: 3.7289899\ttest: 3.7998132\tbest: 3.7998132 (156)\ttotal: 7m 34s\tremaining: 40m 38s\n",
      "157:\tlearn: 3.7287084\ttest: 3.7997223\tbest: 3.7997223 (157)\ttotal: 7m 37s\tremaining: 40m 39s\n",
      "158:\tlearn: 3.7282576\ttest: 3.7996512\tbest: 3.7996512 (158)\ttotal: 7m 40s\tremaining: 40m 38s\n",
      "159:\tlearn: 3.7277311\ttest: 3.7995982\tbest: 3.7995982 (159)\ttotal: 7m 43s\tremaining: 40m 34s\n",
      "160:\tlearn: 3.7274386\ttest: 3.7995056\tbest: 3.7995056 (160)\ttotal: 7m 46s\tremaining: 40m 30s\n",
      "161:\tlearn: 3.7271464\ttest: 3.7993999\tbest: 3.7993999 (161)\ttotal: 7m 50s\tremaining: 40m 35s\n",
      "162:\tlearn: 3.7265793\ttest: 3.7993553\tbest: 3.7993553 (162)\ttotal: 7m 54s\tremaining: 40m 35s\n",
      "163:\tlearn: 3.7261444\ttest: 3.7992280\tbest: 3.7992280 (163)\ttotal: 7m 57s\tremaining: 40m 32s\n",
      "164:\tlearn: 3.7257529\ttest: 3.7991504\tbest: 3.7991504 (164)\ttotal: 8m\tremaining: 40m 32s\n",
      "165:\tlearn: 3.7249461\ttest: 3.7991181\tbest: 3.7991181 (165)\ttotal: 8m 3s\tremaining: 40m 30s\n",
      "166:\tlearn: 3.7245012\ttest: 3.7990689\tbest: 3.7990689 (166)\ttotal: 8m 6s\tremaining: 40m 27s\n",
      "167:\tlearn: 3.7240772\ttest: 3.7989839\tbest: 3.7989839 (167)\ttotal: 8m 9s\tremaining: 40m 24s\n",
      "168:\tlearn: 3.7236938\ttest: 3.7988664\tbest: 3.7988664 (168)\ttotal: 8m 12s\tremaining: 40m 20s\n",
      "169:\tlearn: 3.7233416\ttest: 3.7987788\tbest: 3.7987788 (169)\ttotal: 8m 15s\tremaining: 40m 20s\n",
      "170:\tlearn: 3.7228824\ttest: 3.7987369\tbest: 3.7987369 (170)\ttotal: 8m 18s\tremaining: 40m 16s\n",
      "171:\tlearn: 3.7225668\ttest: 3.7986873\tbest: 3.7986873 (171)\ttotal: 8m 21s\tremaining: 40m 13s\n",
      "172:\tlearn: 3.7222764\ttest: 3.7986260\tbest: 3.7986260 (172)\ttotal: 8m 24s\tremaining: 40m 10s\n",
      "173:\tlearn: 3.7216823\ttest: 3.7986657\tbest: 3.7986260 (172)\ttotal: 8m 27s\tremaining: 40m 7s\n",
      "174:\tlearn: 3.7209905\ttest: 3.7985682\tbest: 3.7985682 (174)\ttotal: 8m 30s\tremaining: 40m 4s\n",
      "175:\tlearn: 3.7207508\ttest: 3.7985418\tbest: 3.7985418 (175)\ttotal: 8m 33s\tremaining: 40m 1s\n",
      "176:\tlearn: 3.7204182\ttest: 3.7984614\tbest: 3.7984614 (176)\ttotal: 8m 35s\tremaining: 39m 58s\n",
      "177:\tlearn: 3.7198890\ttest: 3.7984125\tbest: 3.7984125 (177)\ttotal: 8m 39s\tremaining: 39m 56s\n",
      "178:\tlearn: 3.7195667\ttest: 3.7983933\tbest: 3.7983933 (178)\ttotal: 8m 41s\tremaining: 39m 54s\n",
      "179:\tlearn: 3.7193258\ttest: 3.7983345\tbest: 3.7983345 (179)\ttotal: 8m 44s\tremaining: 39m 50s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180:\tlearn: 3.7187373\ttest: 3.7983086\tbest: 3.7983086 (180)\ttotal: 8m 47s\tremaining: 39m 47s\n",
      "181:\tlearn: 3.7182584\ttest: 3.7983000\tbest: 3.7983000 (181)\ttotal: 8m 50s\tremaining: 39m 43s\n",
      "182:\tlearn: 3.7177564\ttest: 3.7982647\tbest: 3.7982647 (182)\ttotal: 8m 53s\tremaining: 39m 40s\n",
      "183:\tlearn: 3.7173367\ttest: 3.7982335\tbest: 3.7982335 (183)\ttotal: 8m 56s\tremaining: 39m 38s\n",
      "184:\tlearn: 3.7168999\ttest: 3.7981963\tbest: 3.7981963 (184)\ttotal: 8m 59s\tremaining: 39m 35s\n",
      "185:\tlearn: 3.7164324\ttest: 3.7982236\tbest: 3.7981963 (184)\ttotal: 9m 1s\tremaining: 39m 29s\n",
      "186:\tlearn: 3.7159333\ttest: 3.7981561\tbest: 3.7981561 (186)\ttotal: 9m 3s\tremaining: 39m 25s\n",
      "187:\tlearn: 3.7156745\ttest: 3.7980789\tbest: 3.7980789 (187)\ttotal: 9m 6s\tremaining: 39m 20s\n",
      "188:\tlearn: 3.7152308\ttest: 3.7980433\tbest: 3.7980433 (188)\ttotal: 9m 9s\tremaining: 39m 16s\n",
      "189:\tlearn: 3.7147610\ttest: 3.7980341\tbest: 3.7980341 (189)\ttotal: 9m 11s\tremaining: 39m 11s\n",
      "190:\tlearn: 3.7142246\ttest: 3.7979831\tbest: 3.7979831 (190)\ttotal: 9m 14s\tremaining: 39m 7s\n",
      "191:\tlearn: 3.7139442\ttest: 3.7979494\tbest: 3.7979494 (191)\ttotal: 9m 16s\tremaining: 39m 3s\n",
      "192:\tlearn: 3.7136894\ttest: 3.7979191\tbest: 3.7979191 (192)\ttotal: 9m 19s\tremaining: 38m 58s\n",
      "193:\tlearn: 3.7133611\ttest: 3.7978958\tbest: 3.7978958 (193)\ttotal: 9m 21s\tremaining: 38m 53s\n",
      "194:\tlearn: 3.7129910\ttest: 3.7978580\tbest: 3.7978580 (194)\ttotal: 9m 24s\tremaining: 38m 49s\n",
      "195:\tlearn: 3.7125758\ttest: 3.7978188\tbest: 3.7978188 (195)\ttotal: 9m 26s\tremaining: 38m 45s\n",
      "196:\tlearn: 3.7121777\ttest: 3.7978144\tbest: 3.7978144 (196)\ttotal: 9m 29s\tremaining: 38m 42s\n",
      "197:\tlearn: 3.7117399\ttest: 3.7977607\tbest: 3.7977607 (197)\ttotal: 9m 32s\tremaining: 38m 39s\n",
      "198:\tlearn: 3.7111900\ttest: 3.7977798\tbest: 3.7977607 (197)\ttotal: 9m 35s\tremaining: 38m 37s\n",
      "199:\tlearn: 3.7108960\ttest: 3.7977268\tbest: 3.7977268 (199)\ttotal: 9m 38s\tremaining: 38m 33s\n",
      "200:\tlearn: 3.7105278\ttest: 3.7977329\tbest: 3.7977268 (199)\ttotal: 9m 40s\tremaining: 38m 29s\n",
      "201:\tlearn: 3.7102058\ttest: 3.7976875\tbest: 3.7976875 (201)\ttotal: 9m 43s\tremaining: 38m 24s\n",
      "202:\tlearn: 3.7096356\ttest: 3.7977011\tbest: 3.7976875 (201)\ttotal: 9m 46s\tremaining: 38m 20s\n",
      "203:\tlearn: 3.7092118\ttest: 3.7977122\tbest: 3.7976875 (201)\ttotal: 9m 48s\tremaining: 38m 16s\n",
      "204:\tlearn: 3.7088501\ttest: 3.7976883\tbest: 3.7976875 (201)\ttotal: 9m 50s\tremaining: 38m 11s\n",
      "205:\tlearn: 3.7085782\ttest: 3.7976574\tbest: 3.7976574 (205)\ttotal: 9m 53s\tremaining: 38m 7s\n",
      "206:\tlearn: 3.7081735\ttest: 3.7976564\tbest: 3.7976564 (206)\ttotal: 9m 55s\tremaining: 38m 2s\n",
      "207:\tlearn: 3.7080046\ttest: 3.7976142\tbest: 3.7976142 (207)\ttotal: 9m 58s\tremaining: 37m 57s\n",
      "208:\tlearn: 3.7075676\ttest: 3.7975852\tbest: 3.7975852 (208)\ttotal: 10m\tremaining: 37m 53s\n",
      "209:\tlearn: 3.7073110\ttest: 3.7975517\tbest: 3.7975517 (209)\ttotal: 10m 3s\tremaining: 37m 51s\n",
      "210:\tlearn: 3.7069735\ttest: 3.7975235\tbest: 3.7975235 (210)\ttotal: 10m 7s\tremaining: 37m 50s\n",
      "211:\tlearn: 3.7066931\ttest: 3.7974862\tbest: 3.7974862 (211)\ttotal: 10m 10s\tremaining: 37m 49s\n",
      "212:\tlearn: 3.7063722\ttest: 3.7974397\tbest: 3.7974397 (212)\ttotal: 10m 13s\tremaining: 37m 46s\n",
      "213:\tlearn: 3.7058294\ttest: 3.7973680\tbest: 3.7973680 (213)\ttotal: 10m 16s\tremaining: 37m 44s\n",
      "214:\tlearn: 3.7053940\ttest: 3.7973703\tbest: 3.7973680 (213)\ttotal: 10m 18s\tremaining: 37m 39s\n",
      "215:\tlearn: 3.7049503\ttest: 3.7973820\tbest: 3.7973680 (213)\ttotal: 10m 21s\tremaining: 37m 35s\n",
      "216:\tlearn: 3.7045510\ttest: 3.7973642\tbest: 3.7973642 (216)\ttotal: 10m 24s\tremaining: 37m 32s\n",
      "217:\tlearn: 3.7042425\ttest: 3.7973038\tbest: 3.7973038 (217)\ttotal: 10m 27s\tremaining: 37m 30s\n",
      "218:\tlearn: 3.7038794\ttest: 3.7972816\tbest: 3.7972816 (218)\ttotal: 10m 29s\tremaining: 37m 25s\n",
      "219:\tlearn: 3.7035427\ttest: 3.7972737\tbest: 3.7972737 (219)\ttotal: 10m 32s\tremaining: 37m 21s\n",
      "220:\tlearn: 3.7030217\ttest: 3.7973016\tbest: 3.7972737 (219)\ttotal: 10m 34s\tremaining: 37m 17s\n",
      "221:\tlearn: 3.7022879\ttest: 3.7973459\tbest: 3.7972737 (219)\ttotal: 10m 37s\tremaining: 37m 12s\n",
      "222:\tlearn: 3.7019693\ttest: 3.7972901\tbest: 3.7972737 (219)\ttotal: 10m 39s\tremaining: 37m 8s\n",
      "223:\tlearn: 3.7017224\ttest: 3.7972386\tbest: 3.7972386 (223)\ttotal: 10m 42s\tremaining: 37m 4s\n",
      "224:\tlearn: 3.7014478\ttest: 3.7972560\tbest: 3.7972386 (223)\ttotal: 10m 44s\tremaining: 37m\n",
      "225:\tlearn: 3.7009740\ttest: 3.7972911\tbest: 3.7972386 (223)\ttotal: 10m 47s\tremaining: 36m 56s\n",
      "226:\tlearn: 3.7005528\ttest: 3.7972952\tbest: 3.7972386 (223)\ttotal: 10m 49s\tremaining: 36m 52s\n",
      "227:\tlearn: 3.7000039\ttest: 3.7972938\tbest: 3.7972386 (223)\ttotal: 10m 52s\tremaining: 36m 48s\n",
      "228:\tlearn: 3.6994615\ttest: 3.7972523\tbest: 3.7972386 (223)\ttotal: 10m 54s\tremaining: 36m 44s\n",
      "229:\tlearn: 3.6991412\ttest: 3.7972551\tbest: 3.7972386 (223)\ttotal: 10m 57s\tremaining: 36m 39s\n",
      "230:\tlearn: 3.6987277\ttest: 3.7972365\tbest: 3.7972365 (230)\ttotal: 10m 59s\tremaining: 36m 35s\n",
      "231:\tlearn: 3.6982013\ttest: 3.7972103\tbest: 3.7972103 (231)\ttotal: 11m 2s\tremaining: 36m 32s\n",
      "232:\tlearn: 3.6979890\ttest: 3.7971445\tbest: 3.7971445 (232)\ttotal: 11m 4s\tremaining: 36m 28s\n",
      "233:\tlearn: 3.6977793\ttest: 3.7971371\tbest: 3.7971371 (233)\ttotal: 11m 7s\tremaining: 36m 25s\n",
      "234:\tlearn: 3.6975357\ttest: 3.7971061\tbest: 3.7971061 (234)\ttotal: 11m 10s\tremaining: 36m 22s\n",
      "235:\tlearn: 3.6969277\ttest: 3.7970843\tbest: 3.7970843 (235)\ttotal: 11m 13s\tremaining: 36m 20s\n",
      "236:\tlearn: 3.6967151\ttest: 3.7970669\tbest: 3.7970669 (236)\ttotal: 11m 17s\tremaining: 36m 19s\n",
      "237:\tlearn: 3.6962453\ttest: 3.7970729\tbest: 3.7970669 (236)\ttotal: 11m 20s\tremaining: 36m 19s\n",
      "238:\tlearn: 3.6957796\ttest: 3.7970900\tbest: 3.7970669 (236)\ttotal: 11m 23s\tremaining: 36m 16s\n",
      "239:\tlearn: 3.6954192\ttest: 3.7971035\tbest: 3.7970669 (236)\ttotal: 11m 26s\tremaining: 36m 13s\n",
      "240:\tlearn: 3.6951337\ttest: 3.7971047\tbest: 3.7970669 (236)\ttotal: 11m 29s\tremaining: 36m 10s\n",
      "241:\tlearn: 3.6948345\ttest: 3.7971059\tbest: 3.7970669 (236)\ttotal: 11m 31s\tremaining: 36m 6s\n",
      "242:\tlearn: 3.6942466\ttest: 3.7971323\tbest: 3.7970669 (236)\ttotal: 11m 34s\tremaining: 36m 3s\n",
      "243:\tlearn: 3.6939703\ttest: 3.7971151\tbest: 3.7970669 (236)\ttotal: 11m 37s\tremaining: 36m 1s\n",
      "244:\tlearn: 3.6937712\ttest: 3.7970880\tbest: 3.7970669 (236)\ttotal: 11m 40s\tremaining: 35m 58s\n",
      "245:\tlearn: 3.6935340\ttest: 3.7970634\tbest: 3.7970634 (245)\ttotal: 11m 42s\tremaining: 35m 53s\n",
      "246:\tlearn: 3.6930922\ttest: 3.7970802\tbest: 3.7970634 (245)\ttotal: 11m 45s\tremaining: 35m 49s\n",
      "247:\tlearn: 3.6927151\ttest: 3.7970779\tbest: 3.7970634 (245)\ttotal: 11m 47s\tremaining: 35m 45s\n",
      "248:\tlearn: 3.6925112\ttest: 3.7970796\tbest: 3.7970634 (245)\ttotal: 11m 50s\tremaining: 35m 43s\n",
      "249:\tlearn: 3.6922261\ttest: 3.7970620\tbest: 3.7970620 (249)\ttotal: 11m 53s\tremaining: 35m 39s\n",
      "250:\tlearn: 3.6920340\ttest: 3.7970120\tbest: 3.7970120 (250)\ttotal: 11m 56s\tremaining: 35m 38s\n",
      "251:\tlearn: 3.6918518\ttest: 3.7969683\tbest: 3.7969683 (251)\ttotal: 11m 59s\tremaining: 35m 36s\n",
      "252:\tlearn: 3.6915507\ttest: 3.7969632\tbest: 3.7969632 (252)\ttotal: 12m 2s\tremaining: 35m 32s\n",
      "253:\tlearn: 3.6912628\ttest: 3.7969884\tbest: 3.7969632 (252)\ttotal: 12m 4s\tremaining: 35m 28s\n",
      "254:\tlearn: 3.6908026\ttest: 3.7969165\tbest: 3.7969165 (254)\ttotal: 12m 7s\tremaining: 35m 24s\n",
      "255:\tlearn: 3.6905164\ttest: 3.7969250\tbest: 3.7969165 (254)\ttotal: 12m 9s\tremaining: 35m 21s\n",
      "256:\tlearn: 3.6901350\ttest: 3.7969300\tbest: 3.7969165 (254)\ttotal: 12m 12s\tremaining: 35m 18s\n",
      "257:\tlearn: 3.6895570\ttest: 3.7969275\tbest: 3.7969165 (254)\ttotal: 12m 15s\tremaining: 35m 16s\n",
      "258:\tlearn: 3.6888898\ttest: 3.7968972\tbest: 3.7968972 (258)\ttotal: 12m 19s\tremaining: 35m 14s\n",
      "259:\tlearn: 3.6885178\ttest: 3.7968818\tbest: 3.7968818 (259)\ttotal: 12m 22s\tremaining: 35m 12s\n",
      "260:\tlearn: 3.6882784\ttest: 3.7968863\tbest: 3.7968818 (259)\ttotal: 12m 24s\tremaining: 35m 8s\n",
      "261:\tlearn: 3.6876925\ttest: 3.7969403\tbest: 3.7968818 (259)\ttotal: 12m 27s\tremaining: 35m 4s\n",
      "262:\tlearn: 3.6874596\ttest: 3.7969449\tbest: 3.7968818 (259)\ttotal: 12m 30s\tremaining: 35m 2s\n",
      "263:\tlearn: 3.6869236\ttest: 3.7969113\tbest: 3.7968818 (259)\ttotal: 12m 32s\tremaining: 34m 58s\n",
      "264:\tlearn: 3.6866555\ttest: 3.7969044\tbest: 3.7968818 (259)\ttotal: 12m 35s\tremaining: 34m 54s\n",
      "265:\tlearn: 3.6862552\ttest: 3.7969440\tbest: 3.7968818 (259)\ttotal: 12m 37s\tremaining: 34m 50s\n",
      "266:\tlearn: 3.6856138\ttest: 3.7969790\tbest: 3.7968818 (259)\ttotal: 12m 39s\tremaining: 34m 46s\n",
      "267:\tlearn: 3.6852575\ttest: 3.7970196\tbest: 3.7968818 (259)\ttotal: 12m 42s\tremaining: 34m 42s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268:\tlearn: 3.6849368\ttest: 3.7969617\tbest: 3.7968818 (259)\ttotal: 12m 44s\tremaining: 34m 38s\n",
      "269:\tlearn: 3.6844927\ttest: 3.7969367\tbest: 3.7968818 (259)\ttotal: 12m 47s\tremaining: 34m 34s\n",
      "270:\tlearn: 3.6842522\ttest: 3.7969321\tbest: 3.7968818 (259)\ttotal: 12m 49s\tremaining: 34m 30s\n",
      "271:\tlearn: 3.6839278\ttest: 3.7969319\tbest: 3.7968818 (259)\ttotal: 12m 52s\tremaining: 34m 26s\n",
      "272:\tlearn: 3.6834607\ttest: 3.7969239\tbest: 3.7968818 (259)\ttotal: 12m 54s\tremaining: 34m 22s\n",
      "273:\tlearn: 3.6830732\ttest: 3.7969283\tbest: 3.7968818 (259)\ttotal: 12m 56s\tremaining: 34m 18s\n",
      "274:\tlearn: 3.6826005\ttest: 3.7969761\tbest: 3.7968818 (259)\ttotal: 12m 59s\tremaining: 34m 15s\n",
      "275:\tlearn: 3.6818560\ttest: 3.7969637\tbest: 3.7968818 (259)\ttotal: 13m 2s\tremaining: 34m 12s\n",
      "276:\tlearn: 3.6814687\ttest: 3.7969936\tbest: 3.7968818 (259)\ttotal: 13m 4s\tremaining: 34m 8s\n",
      "277:\tlearn: 3.6810916\ttest: 3.7969663\tbest: 3.7968818 (259)\ttotal: 13m 7s\tremaining: 34m 4s\n",
      "278:\tlearn: 3.6809183\ttest: 3.7969905\tbest: 3.7968818 (259)\ttotal: 13m 9s\tremaining: 34m\n",
      "279:\tlearn: 3.6802860\ttest: 3.7970059\tbest: 3.7968818 (259)\ttotal: 13m 12s\tremaining: 33m 56s\n",
      "280:\tlearn: 3.6799479\ttest: 3.7969828\tbest: 3.7968818 (259)\ttotal: 13m 14s\tremaining: 33m 52s\n",
      "281:\tlearn: 3.6796308\ttest: 3.7969526\tbest: 3.7968818 (259)\ttotal: 13m 16s\tremaining: 33m 49s\n",
      "282:\tlearn: 3.6794264\ttest: 3.7969231\tbest: 3.7968818 (259)\ttotal: 13m 19s\tremaining: 33m 45s\n",
      "283:\tlearn: 3.6788854\ttest: 3.7969881\tbest: 3.7968818 (259)\ttotal: 13m 21s\tremaining: 33m 41s\n",
      "284:\tlearn: 3.6784634\ttest: 3.7970224\tbest: 3.7968818 (259)\ttotal: 13m 24s\tremaining: 33m 38s\n",
      "285:\tlearn: 3.6782885\ttest: 3.7970233\tbest: 3.7968818 (259)\ttotal: 13m 26s\tremaining: 33m 34s\n",
      "286:\tlearn: 3.6781226\ttest: 3.7969944\tbest: 3.7968818 (259)\ttotal: 13m 29s\tremaining: 33m 30s\n",
      "287:\tlearn: 3.6779213\ttest: 3.7969870\tbest: 3.7968818 (259)\ttotal: 13m 31s\tremaining: 33m 26s\n",
      "288:\tlearn: 3.6777207\ttest: 3.7969835\tbest: 3.7968818 (259)\ttotal: 13m 34s\tremaining: 33m 22s\n",
      "289:\tlearn: 3.6774569\ttest: 3.7970054\tbest: 3.7968818 (259)\ttotal: 13m 36s\tremaining: 33m 19s\n",
      "290:\tlearn: 3.6771613\ttest: 3.7969979\tbest: 3.7968818 (259)\ttotal: 13m 38s\tremaining: 33m 15s\n",
      "291:\tlearn: 3.6769511\ttest: 3.7970307\tbest: 3.7968818 (259)\ttotal: 13m 41s\tremaining: 33m 11s\n",
      "292:\tlearn: 3.6766597\ttest: 3.7970245\tbest: 3.7968818 (259)\ttotal: 13m 44s\tremaining: 33m 9s\n",
      "293:\tlearn: 3.6762367\ttest: 3.7970034\tbest: 3.7968818 (259)\ttotal: 13m 46s\tremaining: 33m 5s\n",
      "294:\tlearn: 3.6758773\ttest: 3.7969755\tbest: 3.7968818 (259)\ttotal: 13m 49s\tremaining: 33m 1s\n",
      "295:\tlearn: 3.6756873\ttest: 3.7969580\tbest: 3.7968818 (259)\ttotal: 13m 51s\tremaining: 32m 57s\n",
      "296:\tlearn: 3.6751583\ttest: 3.7969343\tbest: 3.7968818 (259)\ttotal: 13m 54s\tremaining: 32m 55s\n",
      "297:\tlearn: 3.6749272\ttest: 3.7969328\tbest: 3.7968818 (259)\ttotal: 13m 57s\tremaining: 32m 52s\n",
      "298:\tlearn: 3.6742004\ttest: 3.7969038\tbest: 3.7968818 (259)\ttotal: 13m 59s\tremaining: 32m 48s\n",
      "299:\tlearn: 3.6737765\ttest: 3.7968829\tbest: 3.7968818 (259)\ttotal: 14m 3s\tremaining: 32m 47s\n",
      "300:\tlearn: 3.6734314\ttest: 3.7969261\tbest: 3.7968818 (259)\ttotal: 14m 6s\tremaining: 32m 45s\n",
      "301:\tlearn: 3.6731033\ttest: 3.7969190\tbest: 3.7968818 (259)\ttotal: 14m 9s\tremaining: 32m 43s\n",
      "302:\tlearn: 3.6727912\ttest: 3.7969066\tbest: 3.7968818 (259)\ttotal: 14m 11s\tremaining: 32m 39s\n",
      "303:\tlearn: 3.6721726\ttest: 3.7969922\tbest: 3.7968818 (259)\ttotal: 14m 14s\tremaining: 32m 36s\n",
      "304:\tlearn: 3.6718592\ttest: 3.7969780\tbest: 3.7968818 (259)\ttotal: 14m 17s\tremaining: 32m 32s\n",
      "305:\tlearn: 3.6712906\ttest: 3.7969908\tbest: 3.7968818 (259)\ttotal: 14m 19s\tremaining: 32m 30s\n",
      "306:\tlearn: 3.6710813\ttest: 3.7970079\tbest: 3.7968818 (259)\ttotal: 14m 23s\tremaining: 32m 28s\n",
      "307:\tlearn: 3.6706054\ttest: 3.7970588\tbest: 3.7968818 (259)\ttotal: 14m 26s\tremaining: 32m 27s\n",
      "308:\tlearn: 3.6701026\ttest: 3.7970943\tbest: 3.7968818 (259)\ttotal: 14m 29s\tremaining: 32m 24s\n",
      "309:\tlearn: 3.6696415\ttest: 3.7971028\tbest: 3.7968818 (259)\ttotal: 14m 32s\tremaining: 32m 21s\n",
      "310:\tlearn: 3.6695233\ttest: 3.7970858\tbest: 3.7968818 (259)\ttotal: 14m 35s\tremaining: 32m 19s\n",
      "311:\tlearn: 3.6690011\ttest: 3.7971306\tbest: 3.7968818 (259)\ttotal: 14m 37s\tremaining: 32m 15s\n",
      "312:\tlearn: 3.6684930\ttest: 3.7971361\tbest: 3.7968818 (259)\ttotal: 14m 40s\tremaining: 32m 12s\n",
      "313:\tlearn: 3.6681583\ttest: 3.7971401\tbest: 3.7968818 (259)\ttotal: 14m 43s\tremaining: 32m 9s\n",
      "314:\tlearn: 3.6677228\ttest: 3.7972077\tbest: 3.7968818 (259)\ttotal: 14m 45s\tremaining: 32m 6s\n",
      "315:\tlearn: 3.6675582\ttest: 3.7971858\tbest: 3.7968818 (259)\ttotal: 14m 48s\tremaining: 32m 2s\n",
      "316:\tlearn: 3.6672359\ttest: 3.7972001\tbest: 3.7968818 (259)\ttotal: 14m 50s\tremaining: 31m 59s\n",
      "317:\tlearn: 3.6668505\ttest: 3.7972056\tbest: 3.7968818 (259)\ttotal: 14m 53s\tremaining: 31m 55s\n",
      "318:\tlearn: 3.6665047\ttest: 3.7972008\tbest: 3.7968818 (259)\ttotal: 14m 55s\tremaining: 31m 51s\n",
      "319:\tlearn: 3.6660367\ttest: 3.7972038\tbest: 3.7968818 (259)\ttotal: 14m 58s\tremaining: 31m 48s\n",
      "320:\tlearn: 3.6657453\ttest: 3.7971741\tbest: 3.7968818 (259)\ttotal: 15m\tremaining: 31m 44s\n",
      "321:\tlearn: 3.6654022\ttest: 3.7972343\tbest: 3.7968818 (259)\ttotal: 15m 2s\tremaining: 31m 41s\n",
      "322:\tlearn: 3.6652995\ttest: 3.7972429\tbest: 3.7968818 (259)\ttotal: 15m 5s\tremaining: 31m 37s\n",
      "323:\tlearn: 3.6649498\ttest: 3.7972649\tbest: 3.7968818 (259)\ttotal: 15m 7s\tremaining: 31m 33s\n",
      "324:\tlearn: 3.6646741\ttest: 3.7972765\tbest: 3.7968818 (259)\ttotal: 15m 10s\tremaining: 31m 30s\n",
      "325:\tlearn: 3.6642062\ttest: 3.7972616\tbest: 3.7968818 (259)\ttotal: 15m 12s\tremaining: 31m 26s\n",
      "326:\tlearn: 3.6639170\ttest: 3.7972603\tbest: 3.7968818 (259)\ttotal: 15m 15s\tremaining: 31m 23s\n",
      "327:\tlearn: 3.6634699\ttest: 3.7972558\tbest: 3.7968818 (259)\ttotal: 15m 17s\tremaining: 31m 19s\n",
      "328:\tlearn: 3.6632093\ttest: 3.7972592\tbest: 3.7968818 (259)\ttotal: 15m 20s\tremaining: 31m 16s\n",
      "329:\tlearn: 3.6629838\ttest: 3.7972203\tbest: 3.7968818 (259)\ttotal: 15m 22s\tremaining: 31m 12s\n",
      "330:\tlearn: 3.6626352\ttest: 3.7972343\tbest: 3.7968818 (259)\ttotal: 15m 24s\tremaining: 31m 9s\n",
      "331:\tlearn: 3.6622052\ttest: 3.7972675\tbest: 3.7968818 (259)\ttotal: 15m 27s\tremaining: 31m 5s\n",
      "332:\tlearn: 3.6618320\ttest: 3.7972557\tbest: 3.7968818 (259)\ttotal: 15m 29s\tremaining: 31m 2s\n",
      "333:\tlearn: 3.6614596\ttest: 3.7972341\tbest: 3.7968818 (259)\ttotal: 15m 32s\tremaining: 30m 58s\n",
      "334:\tlearn: 3.6608952\ttest: 3.7972582\tbest: 3.7968818 (259)\ttotal: 15m 34s\tremaining: 30m 55s\n",
      "335:\tlearn: 3.6606474\ttest: 3.7972516\tbest: 3.7968818 (259)\ttotal: 15m 37s\tremaining: 30m 51s\n",
      "336:\tlearn: 3.6602601\ttest: 3.7972448\tbest: 3.7968818 (259)\ttotal: 15m 39s\tremaining: 30m 48s\n",
      "337:\tlearn: 3.6596469\ttest: 3.7972889\tbest: 3.7968818 (259)\ttotal: 15m 42s\tremaining: 30m 46s\n",
      "338:\tlearn: 3.6590642\ttest: 3.7973282\tbest: 3.7968818 (259)\ttotal: 15m 45s\tremaining: 30m 43s\n",
      "339:\tlearn: 3.6588178\ttest: 3.7973285\tbest: 3.7968818 (259)\ttotal: 15m 47s\tremaining: 30m 39s\n",
      "340:\tlearn: 3.6582904\ttest: 3.7973502\tbest: 3.7968818 (259)\ttotal: 15m 50s\tremaining: 30m 36s\n",
      "341:\tlearn: 3.6578459\ttest: 3.7974248\tbest: 3.7968818 (259)\ttotal: 15m 52s\tremaining: 30m 33s\n",
      "342:\tlearn: 3.6576943\ttest: 3.7974188\tbest: 3.7968818 (259)\ttotal: 15m 55s\tremaining: 30m 30s\n",
      "343:\tlearn: 3.6575296\ttest: 3.7974241\tbest: 3.7968818 (259)\ttotal: 15m 58s\tremaining: 30m 27s\n",
      "344:\tlearn: 3.6570873\ttest: 3.7973972\tbest: 3.7968818 (259)\ttotal: 16m\tremaining: 30m 24s\n",
      "345:\tlearn: 3.6567685\ttest: 3.7974148\tbest: 3.7968818 (259)\ttotal: 16m 3s\tremaining: 30m 20s\n",
      "346:\tlearn: 3.6564439\ttest: 3.7974043\tbest: 3.7968818 (259)\ttotal: 16m 6s\tremaining: 30m 17s\n",
      "347:\tlearn: 3.6561200\ttest: 3.7974224\tbest: 3.7968818 (259)\ttotal: 16m 8s\tremaining: 30m 14s\n",
      "348:\tlearn: 3.6556810\ttest: 3.7974583\tbest: 3.7968818 (259)\ttotal: 16m 11s\tremaining: 30m 11s\n",
      "349:\tlearn: 3.6550630\ttest: 3.7975120\tbest: 3.7968818 (259)\ttotal: 16m 13s\tremaining: 30m 8s\n",
      "350:\tlearn: 3.6544011\ttest: 3.7975364\tbest: 3.7968818 (259)\ttotal: 16m 16s\tremaining: 30m 4s\n",
      "351:\tlearn: 3.6541460\ttest: 3.7975353\tbest: 3.7968818 (259)\ttotal: 16m 19s\tremaining: 30m 2s\n",
      "352:\tlearn: 3.6536070\ttest: 3.7975915\tbest: 3.7968818 (259)\ttotal: 16m 21s\tremaining: 29m 59s\n",
      "353:\tlearn: 3.6531935\ttest: 3.7976060\tbest: 3.7968818 (259)\ttotal: 16m 24s\tremaining: 29m 56s\n",
      "354:\tlearn: 3.6528746\ttest: 3.7976091\tbest: 3.7968818 (259)\ttotal: 16m 27s\tremaining: 29m 53s\n",
      "355:\tlearn: 3.6526695\ttest: 3.7976135\tbest: 3.7968818 (259)\ttotal: 16m 30s\tremaining: 29m 51s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356:\tlearn: 3.6520480\ttest: 3.7976651\tbest: 3.7968818 (259)\ttotal: 16m 32s\tremaining: 29m 47s\n",
      "357:\tlearn: 3.6519018\ttest: 3.7976375\tbest: 3.7968818 (259)\ttotal: 16m 35s\tremaining: 29m 44s\n",
      "358:\tlearn: 3.6516275\ttest: 3.7976519\tbest: 3.7968818 (259)\ttotal: 16m 37s\tremaining: 29m 41s\n",
      "359:\tlearn: 3.6514410\ttest: 3.7976682\tbest: 3.7968818 (259)\ttotal: 16m 40s\tremaining: 29m 38s\n",
      "360:\tlearn: 3.6510910\ttest: 3.7976304\tbest: 3.7968818 (259)\ttotal: 16m 42s\tremaining: 29m 34s\n",
      "361:\tlearn: 3.6506622\ttest: 3.7976219\tbest: 3.7968818 (259)\ttotal: 16m 45s\tremaining: 29m 31s\n",
      "362:\tlearn: 3.6503036\ttest: 3.7977040\tbest: 3.7968818 (259)\ttotal: 16m 47s\tremaining: 29m 28s\n",
      "363:\tlearn: 3.6500584\ttest: 3.7977158\tbest: 3.7968818 (259)\ttotal: 16m 50s\tremaining: 29m 24s\n",
      "364:\tlearn: 3.6498771\ttest: 3.7977073\tbest: 3.7968818 (259)\ttotal: 16m 52s\tremaining: 29m 21s\n",
      "365:\tlearn: 3.6494810\ttest: 3.7977415\tbest: 3.7968818 (259)\ttotal: 16m 55s\tremaining: 29m 18s\n",
      "366:\tlearn: 3.6490925\ttest: 3.7977614\tbest: 3.7968818 (259)\ttotal: 16m 57s\tremaining: 29m 15s\n",
      "367:\tlearn: 3.6488061\ttest: 3.7977673\tbest: 3.7968818 (259)\ttotal: 17m\tremaining: 29m 11s\n",
      "368:\tlearn: 3.6486433\ttest: 3.7977824\tbest: 3.7968818 (259)\ttotal: 17m 3s\tremaining: 29m 9s\n",
      "369:\tlearn: 3.6482866\ttest: 3.7978382\tbest: 3.7968818 (259)\ttotal: 17m 5s\tremaining: 29m 6s\n",
      "370:\tlearn: 3.6479415\ttest: 3.7978234\tbest: 3.7968818 (259)\ttotal: 17m 8s\tremaining: 29m 2s\n",
      "371:\tlearn: 3.6476718\ttest: 3.7978451\tbest: 3.7968818 (259)\ttotal: 17m 10s\tremaining: 28m 59s\n",
      "372:\tlearn: 3.6470086\ttest: 3.7978011\tbest: 3.7968818 (259)\ttotal: 17m 12s\tremaining: 28m 56s\n",
      "373:\tlearn: 3.6466420\ttest: 3.7978254\tbest: 3.7968818 (259)\ttotal: 17m 15s\tremaining: 28m 53s\n",
      "374:\tlearn: 3.6464502\ttest: 3.7978315\tbest: 3.7968818 (259)\ttotal: 17m 17s\tremaining: 28m 49s\n",
      "375:\tlearn: 3.6460265\ttest: 3.7978607\tbest: 3.7968818 (259)\ttotal: 17m 20s\tremaining: 28m 47s\n",
      "376:\tlearn: 3.6458552\ttest: 3.7978461\tbest: 3.7968818 (259)\ttotal: 17m 23s\tremaining: 28m 44s\n",
      "377:\tlearn: 3.6452227\ttest: 3.7978592\tbest: 3.7968818 (259)\ttotal: 17m 26s\tremaining: 28m 42s\n",
      "378:\tlearn: 3.6449451\ttest: 3.7978801\tbest: 3.7968818 (259)\ttotal: 17m 29s\tremaining: 28m 39s\n",
      "379:\tlearn: 3.6448149\ttest: 3.7978756\tbest: 3.7968818 (259)\ttotal: 17m 31s\tremaining: 28m 36s\n",
      "380:\tlearn: 3.6443809\ttest: 3.7978907\tbest: 3.7968818 (259)\ttotal: 17m 34s\tremaining: 28m 33s\n",
      "381:\tlearn: 3.6437800\ttest: 3.7979061\tbest: 3.7968818 (259)\ttotal: 17m 37s\tremaining: 28m 31s\n",
      "382:\tlearn: 3.6435930\ttest: 3.7979054\tbest: 3.7968818 (259)\ttotal: 17m 41s\tremaining: 28m 29s\n",
      "383:\tlearn: 3.6433045\ttest: 3.7979231\tbest: 3.7968818 (259)\ttotal: 17m 43s\tremaining: 28m 26s\n",
      "384:\tlearn: 3.6430412\ttest: 3.7979210\tbest: 3.7968818 (259)\ttotal: 17m 46s\tremaining: 28m 23s\n",
      "385:\tlearn: 3.6427404\ttest: 3.7979499\tbest: 3.7968818 (259)\ttotal: 17m 49s\tremaining: 28m 21s\n",
      "386:\tlearn: 3.6423293\ttest: 3.7979926\tbest: 3.7968818 (259)\ttotal: 17m 52s\tremaining: 28m 18s\n",
      "387:\tlearn: 3.6419702\ttest: 3.7979928\tbest: 3.7968818 (259)\ttotal: 17m 55s\tremaining: 28m 15s\n",
      "388:\tlearn: 3.6415712\ttest: 3.7980055\tbest: 3.7968818 (259)\ttotal: 17m 57s\tremaining: 28m 12s\n",
      "389:\tlearn: 3.6413441\ttest: 3.7980607\tbest: 3.7968818 (259)\ttotal: 18m\tremaining: 28m 9s\n",
      "390:\tlearn: 3.6410678\ttest: 3.7980459\tbest: 3.7968818 (259)\ttotal: 18m 2s\tremaining: 28m 6s\n",
      "391:\tlearn: 3.6405970\ttest: 3.7980647\tbest: 3.7968818 (259)\ttotal: 18m 5s\tremaining: 28m 3s\n",
      "392:\tlearn: 3.6402422\ttest: 3.7980685\tbest: 3.7968818 (259)\ttotal: 18m 7s\tremaining: 28m\n",
      "393:\tlearn: 3.6399465\ttest: 3.7981170\tbest: 3.7968818 (259)\ttotal: 18m 10s\tremaining: 27m 57s\n",
      "394:\tlearn: 3.6398423\ttest: 3.7981158\tbest: 3.7968818 (259)\ttotal: 18m 12s\tremaining: 27m 53s\n",
      "395:\tlearn: 3.6395294\ttest: 3.7981498\tbest: 3.7968818 (259)\ttotal: 18m 15s\tremaining: 27m 50s\n",
      "396:\tlearn: 3.6392995\ttest: 3.7981718\tbest: 3.7968818 (259)\ttotal: 18m 17s\tremaining: 27m 47s\n",
      "397:\tlearn: 3.6388597\ttest: 3.7982043\tbest: 3.7968818 (259)\ttotal: 18m 20s\tremaining: 27m 45s\n",
      "398:\tlearn: 3.6386340\ttest: 3.7982011\tbest: 3.7968818 (259)\ttotal: 18m 23s\tremaining: 27m 41s\n",
      "399:\tlearn: 3.6382366\ttest: 3.7982259\tbest: 3.7968818 (259)\ttotal: 18m 25s\tremaining: 27m 38s\n",
      "400:\tlearn: 3.6377764\ttest: 3.7982621\tbest: 3.7968818 (259)\ttotal: 18m 28s\tremaining: 27m 35s\n",
      "401:\tlearn: 3.6375383\ttest: 3.7983104\tbest: 3.7968818 (259)\ttotal: 18m 31s\tremaining: 27m 33s\n",
      "402:\tlearn: 3.6371903\ttest: 3.7983160\tbest: 3.7968818 (259)\ttotal: 18m 33s\tremaining: 27m 29s\n",
      "403:\tlearn: 3.6367427\ttest: 3.7983571\tbest: 3.7968818 (259)\ttotal: 18m 36s\tremaining: 27m 26s\n",
      "404:\tlearn: 3.6365279\ttest: 3.7983567\tbest: 3.7968818 (259)\ttotal: 18m 38s\tremaining: 27m 23s\n",
      "405:\tlearn: 3.6362224\ttest: 3.7983966\tbest: 3.7968818 (259)\ttotal: 18m 41s\tremaining: 27m 20s\n",
      "406:\tlearn: 3.6358336\ttest: 3.7983668\tbest: 3.7968818 (259)\ttotal: 18m 43s\tremaining: 27m 17s\n",
      "407:\tlearn: 3.6356383\ttest: 3.7983762\tbest: 3.7968818 (259)\ttotal: 18m 46s\tremaining: 27m 14s\n",
      "408:\tlearn: 3.6352653\ttest: 3.7983801\tbest: 3.7968818 (259)\ttotal: 18m 48s\tremaining: 27m 11s\n",
      "409:\tlearn: 3.6349877\ttest: 3.7983874\tbest: 3.7968818 (259)\ttotal: 18m 51s\tremaining: 27m 8s\n",
      "410:\tlearn: 3.6344529\ttest: 3.7983711\tbest: 3.7968818 (259)\ttotal: 18m 53s\tremaining: 27m 4s\n",
      "411:\tlearn: 3.6342465\ttest: 3.7983721\tbest: 3.7968818 (259)\ttotal: 18m 56s\tremaining: 27m 1s\n",
      "412:\tlearn: 3.6337492\ttest: 3.7983812\tbest: 3.7968818 (259)\ttotal: 18m 58s\tremaining: 26m 58s\n",
      "413:\tlearn: 3.6335364\ttest: 3.7983938\tbest: 3.7968818 (259)\ttotal: 19m 1s\tremaining: 26m 55s\n",
      "414:\tlearn: 3.6332658\ttest: 3.7983888\tbest: 3.7968818 (259)\ttotal: 19m 3s\tremaining: 26m 52s\n",
      "415:\tlearn: 3.6329027\ttest: 3.7984130\tbest: 3.7968818 (259)\ttotal: 19m 6s\tremaining: 26m 49s\n",
      "416:\tlearn: 3.6325947\ttest: 3.7984200\tbest: 3.7968818 (259)\ttotal: 19m 10s\tremaining: 26m 48s\n",
      "417:\tlearn: 3.6323064\ttest: 3.7984157\tbest: 3.7968818 (259)\ttotal: 19m 12s\tremaining: 26m 45s\n",
      "418:\tlearn: 3.6319625\ttest: 3.7984760\tbest: 3.7968818 (259)\ttotal: 19m 15s\tremaining: 26m 42s\n",
      "419:\tlearn: 3.6316809\ttest: 3.7984420\tbest: 3.7968818 (259)\ttotal: 19m 18s\tremaining: 26m 39s\n",
      "420:\tlearn: 3.6312467\ttest: 3.7984726\tbest: 3.7968818 (259)\ttotal: 19m 21s\tremaining: 26m 37s\n",
      "421:\tlearn: 3.6307975\ttest: 3.7984919\tbest: 3.7968818 (259)\ttotal: 19m 24s\tremaining: 26m 34s\n",
      "422:\tlearn: 3.6306045\ttest: 3.7985038\tbest: 3.7968818 (259)\ttotal: 19m 26s\tremaining: 26m 31s\n",
      "423:\tlearn: 3.6304361\ttest: 3.7985154\tbest: 3.7968818 (259)\ttotal: 19m 29s\tremaining: 26m 28s\n",
      "424:\tlearn: 3.6302207\ttest: 3.7984992\tbest: 3.7968818 (259)\ttotal: 19m 31s\tremaining: 26m 25s\n",
      "425:\tlearn: 3.6298688\ttest: 3.7985089\tbest: 3.7968818 (259)\ttotal: 19m 34s\tremaining: 26m 22s\n",
      "426:\tlearn: 3.6297500\ttest: 3.7985099\tbest: 3.7968818 (259)\ttotal: 19m 36s\tremaining: 26m 19s\n",
      "427:\tlearn: 3.6294013\ttest: 3.7985053\tbest: 3.7968818 (259)\ttotal: 19m 39s\tremaining: 26m 16s\n",
      "428:\tlearn: 3.6290409\ttest: 3.7985692\tbest: 3.7968818 (259)\ttotal: 19m 42s\tremaining: 26m 13s\n",
      "429:\tlearn: 3.6285145\ttest: 3.7986173\tbest: 3.7968818 (259)\ttotal: 19m 44s\tremaining: 26m 10s\n",
      "430:\tlearn: 3.6280665\ttest: 3.7986884\tbest: 3.7968818 (259)\ttotal: 19m 47s\tremaining: 26m 7s\n",
      "431:\tlearn: 3.6278576\ttest: 3.7987224\tbest: 3.7968818 (259)\ttotal: 19m 50s\tremaining: 26m 4s\n",
      "432:\tlearn: 3.6275433\ttest: 3.7987527\tbest: 3.7968818 (259)\ttotal: 19m 52s\tremaining: 26m 2s\n",
      "433:\tlearn: 3.6271673\ttest: 3.7987405\tbest: 3.7968818 (259)\ttotal: 19m 55s\tremaining: 25m 59s\n",
      "434:\tlearn: 3.6267877\ttest: 3.7987341\tbest: 3.7968818 (259)\ttotal: 19m 58s\tremaining: 25m 56s\n",
      "435:\tlearn: 3.6262593\ttest: 3.7987722\tbest: 3.7968818 (259)\ttotal: 20m\tremaining: 25m 53s\n",
      "436:\tlearn: 3.6258832\ttest: 3.7988273\tbest: 3.7968818 (259)\ttotal: 20m 3s\tremaining: 25m 50s\n",
      "437:\tlearn: 3.6255035\ttest: 3.7988279\tbest: 3.7968818 (259)\ttotal: 20m 6s\tremaining: 25m 48s\n",
      "438:\tlearn: 3.6251760\ttest: 3.7987786\tbest: 3.7968818 (259)\ttotal: 20m 10s\tremaining: 25m 46s\n",
      "439:\tlearn: 3.6248466\ttest: 3.7987478\tbest: 3.7968818 (259)\ttotal: 20m 13s\tremaining: 25m 44s\n",
      "440:\tlearn: 3.6244598\ttest: 3.7987823\tbest: 3.7968818 (259)\ttotal: 20m 16s\tremaining: 25m 42s\n",
      "441:\tlearn: 3.6242575\ttest: 3.7987991\tbest: 3.7968818 (259)\ttotal: 20m 20s\tremaining: 25m 40s\n",
      "442:\tlearn: 3.6238967\ttest: 3.7987594\tbest: 3.7968818 (259)\ttotal: 20m 22s\tremaining: 25m 37s\n",
      "443:\tlearn: 3.6236084\ttest: 3.7987003\tbest: 3.7968818 (259)\ttotal: 20m 25s\tremaining: 25m 35s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444:\tlearn: 3.6231768\ttest: 3.7987757\tbest: 3.7968818 (259)\ttotal: 20m 29s\tremaining: 25m 32s\n",
      "445:\tlearn: 3.6225879\ttest: 3.7987873\tbest: 3.7968818 (259)\ttotal: 20m 31s\tremaining: 25m 30s\n",
      "446:\tlearn: 3.6221082\ttest: 3.7988173\tbest: 3.7968818 (259)\ttotal: 20m 34s\tremaining: 25m 27s\n",
      "447:\tlearn: 3.6219190\ttest: 3.7988479\tbest: 3.7968818 (259)\ttotal: 20m 37s\tremaining: 25m 24s\n",
      "448:\tlearn: 3.6214442\ttest: 3.7989078\tbest: 3.7968818 (259)\ttotal: 20m 39s\tremaining: 25m 21s\n",
      "449:\tlearn: 3.6210104\ttest: 3.7989112\tbest: 3.7968818 (259)\ttotal: 20m 42s\tremaining: 25m 18s\n",
      "450:\tlearn: 3.6207397\ttest: 3.7989044\tbest: 3.7968818 (259)\ttotal: 20m 44s\tremaining: 25m 15s\n",
      "451:\tlearn: 3.6204061\ttest: 3.7988854\tbest: 3.7968818 (259)\ttotal: 20m 47s\tremaining: 25m 11s\n",
      "452:\tlearn: 3.6201428\ttest: 3.7989149\tbest: 3.7968818 (259)\ttotal: 20m 49s\tremaining: 25m 8s\n",
      "453:\tlearn: 3.6198669\ttest: 3.7989241\tbest: 3.7968818 (259)\ttotal: 20m 52s\tremaining: 25m 6s\n",
      "454:\tlearn: 3.6196342\ttest: 3.7989169\tbest: 3.7968818 (259)\ttotal: 20m 55s\tremaining: 25m 3s\n",
      "455:\tlearn: 3.6193032\ttest: 3.7989276\tbest: 3.7968818 (259)\ttotal: 20m 57s\tremaining: 25m\n",
      "456:\tlearn: 3.6188428\ttest: 3.7989303\tbest: 3.7968818 (259)\ttotal: 21m\tremaining: 24m 57s\n",
      "457:\tlearn: 3.6184807\ttest: 3.7989788\tbest: 3.7968818 (259)\ttotal: 21m 2s\tremaining: 24m 54s\n",
      "458:\tlearn: 3.6179865\ttest: 3.7989424\tbest: 3.7968818 (259)\ttotal: 21m 5s\tremaining: 24m 51s\n",
      "459:\tlearn: 3.6178597\ttest: 3.7989433\tbest: 3.7968818 (259)\ttotal: 21m 7s\tremaining: 24m 48s\n",
      "460:\tlearn: 3.6175267\ttest: 3.7989631\tbest: 3.7968818 (259)\ttotal: 21m 10s\tremaining: 24m 45s\n",
      "461:\tlearn: 3.6169003\ttest: 3.7989944\tbest: 3.7968818 (259)\ttotal: 21m 13s\tremaining: 24m 42s\n",
      "462:\tlearn: 3.6165942\ttest: 3.7989881\tbest: 3.7968818 (259)\ttotal: 21m 15s\tremaining: 24m 39s\n",
      "463:\tlearn: 3.6159210\ttest: 3.7990413\tbest: 3.7968818 (259)\ttotal: 21m 18s\tremaining: 24m 36s\n",
      "464:\tlearn: 3.6154847\ttest: 3.7990167\tbest: 3.7968818 (259)\ttotal: 21m 20s\tremaining: 24m 33s\n",
      "465:\tlearn: 3.6151338\ttest: 3.7990192\tbest: 3.7968818 (259)\ttotal: 21m 23s\tremaining: 24m 30s\n",
      "466:\tlearn: 3.6147512\ttest: 3.7990335\tbest: 3.7968818 (259)\ttotal: 21m 25s\tremaining: 24m 27s\n",
      "467:\tlearn: 3.6143990\ttest: 3.7990415\tbest: 3.7968818 (259)\ttotal: 21m 28s\tremaining: 24m 24s\n",
      "468:\tlearn: 3.6139903\ttest: 3.7990850\tbest: 3.7968818 (259)\ttotal: 21m 30s\tremaining: 24m 21s\n",
      "469:\tlearn: 3.6135948\ttest: 3.7990356\tbest: 3.7968818 (259)\ttotal: 21m 33s\tremaining: 24m 18s\n",
      "470:\tlearn: 3.6132924\ttest: 3.7990678\tbest: 3.7968818 (259)\ttotal: 21m 36s\tremaining: 24m 16s\n",
      "471:\tlearn: 3.6130870\ttest: 3.7990479\tbest: 3.7968818 (259)\ttotal: 21m 39s\tremaining: 24m 13s\n",
      "472:\tlearn: 3.6127909\ttest: 3.7990470\tbest: 3.7968818 (259)\ttotal: 21m 42s\tremaining: 24m 11s\n",
      "473:\tlearn: 3.6126673\ttest: 3.7990441\tbest: 3.7968818 (259)\ttotal: 21m 45s\tremaining: 24m 8s\n",
      "474:\tlearn: 3.6121090\ttest: 3.7990947\tbest: 3.7968818 (259)\ttotal: 21m 47s\tremaining: 24m 5s\n",
      "475:\tlearn: 3.6117580\ttest: 3.7990557\tbest: 3.7968818 (259)\ttotal: 21m 51s\tremaining: 24m 3s\n",
      "476:\tlearn: 3.6113593\ttest: 3.7990990\tbest: 3.7968818 (259)\ttotal: 21m 54s\tremaining: 24m 1s\n",
      "477:\tlearn: 3.6109675\ttest: 3.7990889\tbest: 3.7968818 (259)\ttotal: 21m 57s\tremaining: 23m 58s\n",
      "478:\tlearn: 3.6105780\ttest: 3.7991341\tbest: 3.7968818 (259)\ttotal: 22m\tremaining: 23m 56s\n",
      "479:\tlearn: 3.6100797\ttest: 3.7991567\tbest: 3.7968818 (259)\ttotal: 22m 3s\tremaining: 23m 53s\n",
      "480:\tlearn: 3.6097489\ttest: 3.7991803\tbest: 3.7968818 (259)\ttotal: 22m 5s\tremaining: 23m 50s\n",
      "481:\tlearn: 3.6094261\ttest: 3.7992386\tbest: 3.7968818 (259)\ttotal: 22m 8s\tremaining: 23m 47s\n",
      "482:\tlearn: 3.6093176\ttest: 3.7992339\tbest: 3.7968818 (259)\ttotal: 22m 12s\tremaining: 23m 45s\n",
      "483:\tlearn: 3.6086836\ttest: 3.7993245\tbest: 3.7968818 (259)\ttotal: 22m 15s\tremaining: 23m 43s\n",
      "484:\tlearn: 3.6081671\ttest: 3.7993223\tbest: 3.7968818 (259)\ttotal: 22m 18s\tremaining: 23m 40s\n",
      "485:\tlearn: 3.6078018\ttest: 3.7993072\tbest: 3.7968818 (259)\ttotal: 22m 20s\tremaining: 23m 38s\n",
      "486:\tlearn: 3.6075390\ttest: 3.7993661\tbest: 3.7968818 (259)\ttotal: 22m 23s\tremaining: 23m 35s\n",
      "487:\tlearn: 3.6072472\ttest: 3.7993987\tbest: 3.7968818 (259)\ttotal: 22m 26s\tremaining: 23m 32s\n",
      "488:\tlearn: 3.6069611\ttest: 3.7993972\tbest: 3.7968818 (259)\ttotal: 22m 29s\tremaining: 23m 29s\n",
      "489:\tlearn: 3.6063216\ttest: 3.7994600\tbest: 3.7968818 (259)\ttotal: 22m 31s\tremaining: 23m 27s\n",
      "490:\tlearn: 3.6058824\ttest: 3.7995112\tbest: 3.7968818 (259)\ttotal: 22m 34s\tremaining: 23m 24s\n",
      "491:\tlearn: 3.6056563\ttest: 3.7995158\tbest: 3.7968818 (259)\ttotal: 22m 37s\tremaining: 23m 21s\n",
      "492:\tlearn: 3.6053828\ttest: 3.7995101\tbest: 3.7968818 (259)\ttotal: 22m 40s\tremaining: 23m 18s\n",
      "493:\tlearn: 3.6051138\ttest: 3.7995111\tbest: 3.7968818 (259)\ttotal: 22m 43s\tremaining: 23m 16s\n",
      "494:\tlearn: 3.6047032\ttest: 3.7996049\tbest: 3.7968818 (259)\ttotal: 22m 46s\tremaining: 23m 13s\n",
      "495:\tlearn: 3.6046054\ttest: 3.7995983\tbest: 3.7968818 (259)\ttotal: 22m 48s\tremaining: 23m 10s\n",
      "496:\tlearn: 3.6042403\ttest: 3.7996095\tbest: 3.7968818 (259)\ttotal: 22m 52s\tremaining: 23m 8s\n",
      "497:\tlearn: 3.6038693\ttest: 3.7996455\tbest: 3.7968818 (259)\ttotal: 22m 55s\tremaining: 23m 6s\n",
      "498:\tlearn: 3.6034575\ttest: 3.7996631\tbest: 3.7968818 (259)\ttotal: 22m 58s\tremaining: 23m 4s\n",
      "499:\tlearn: 3.6033180\ttest: 3.7996597\tbest: 3.7968818 (259)\ttotal: 23m 1s\tremaining: 23m 1s\n",
      "500:\tlearn: 3.6030394\ttest: 3.7996805\tbest: 3.7968818 (259)\ttotal: 23m 4s\tremaining: 22m 58s\n",
      "501:\tlearn: 3.6028251\ttest: 3.7996508\tbest: 3.7968818 (259)\ttotal: 23m 6s\tremaining: 22m 55s\n",
      "502:\tlearn: 3.6022673\ttest: 3.7996475\tbest: 3.7968818 (259)\ttotal: 23m 9s\tremaining: 22m 52s\n",
      "503:\tlearn: 3.6021799\ttest: 3.7996565\tbest: 3.7968818 (259)\ttotal: 23m 12s\tremaining: 22m 49s\n",
      "504:\tlearn: 3.6019726\ttest: 3.7996438\tbest: 3.7968818 (259)\ttotal: 23m 14s\tremaining: 22m 47s\n",
      "505:\tlearn: 3.6016925\ttest: 3.7996254\tbest: 3.7968818 (259)\ttotal: 23m 17s\tremaining: 22m 44s\n",
      "506:\tlearn: 3.6014543\ttest: 3.7996157\tbest: 3.7968818 (259)\ttotal: 23m 19s\tremaining: 22m 41s\n",
      "507:\tlearn: 3.6012598\ttest: 3.7995857\tbest: 3.7968818 (259)\ttotal: 23m 22s\tremaining: 22m 38s\n",
      "508:\tlearn: 3.6010800\ttest: 3.7996237\tbest: 3.7968818 (259)\ttotal: 23m 24s\tremaining: 22m 35s\n",
      "509:\tlearn: 3.6006817\ttest: 3.7996202\tbest: 3.7968818 (259)\ttotal: 23m 27s\tremaining: 22m 32s\n",
      "510:\tlearn: 3.6003177\ttest: 3.7996544\tbest: 3.7968818 (259)\ttotal: 23m 30s\tremaining: 22m 29s\n",
      "511:\tlearn: 3.6001679\ttest: 3.7996407\tbest: 3.7968818 (259)\ttotal: 23m 33s\tremaining: 22m 27s\n",
      "512:\tlearn: 3.5997752\ttest: 3.7996271\tbest: 3.7968818 (259)\ttotal: 23m 36s\tremaining: 22m 24s\n",
      "513:\tlearn: 3.5993087\ttest: 3.7996569\tbest: 3.7968818 (259)\ttotal: 23m 39s\tremaining: 22m 21s\n",
      "514:\tlearn: 3.5991131\ttest: 3.7996944\tbest: 3.7968818 (259)\ttotal: 23m 41s\tremaining: 22m 19s\n",
      "515:\tlearn: 3.5987663\ttest: 3.7997508\tbest: 3.7968818 (259)\ttotal: 23m 45s\tremaining: 22m 16s\n",
      "516:\tlearn: 3.5984031\ttest: 3.7997499\tbest: 3.7968818 (259)\ttotal: 23m 48s\tremaining: 22m 14s\n",
      "517:\tlearn: 3.5982218\ttest: 3.7997248\tbest: 3.7968818 (259)\ttotal: 23m 50s\tremaining: 22m 11s\n",
      "518:\tlearn: 3.5979831\ttest: 3.7997029\tbest: 3.7968818 (259)\ttotal: 23m 53s\tremaining: 22m 8s\n",
      "519:\tlearn: 3.5976975\ttest: 3.7996922\tbest: 3.7968818 (259)\ttotal: 23m 56s\tremaining: 22m 6s\n",
      "520:\tlearn: 3.5972952\ttest: 3.7997494\tbest: 3.7968818 (259)\ttotal: 23m 59s\tremaining: 22m 3s\n",
      "521:\tlearn: 3.5970731\ttest: 3.7997424\tbest: 3.7968818 (259)\ttotal: 24m 1s\tremaining: 22m\n",
      "522:\tlearn: 3.5967217\ttest: 3.7997825\tbest: 3.7968818 (259)\ttotal: 24m 4s\tremaining: 21m 57s\n",
      "523:\tlearn: 3.5963059\ttest: 3.7998282\tbest: 3.7968818 (259)\ttotal: 24m 7s\tremaining: 21m 54s\n",
      "524:\tlearn: 3.5960796\ttest: 3.7998434\tbest: 3.7968818 (259)\ttotal: 24m 9s\tremaining: 21m 51s\n",
      "525:\tlearn: 3.5955583\ttest: 3.7998509\tbest: 3.7968818 (259)\ttotal: 24m 12s\tremaining: 21m 48s\n",
      "526:\tlearn: 3.5952853\ttest: 3.7998361\tbest: 3.7968818 (259)\ttotal: 24m 14s\tremaining: 21m 45s\n",
      "527:\tlearn: 3.5948840\ttest: 3.7998697\tbest: 3.7968818 (259)\ttotal: 24m 17s\tremaining: 21m 42s\n",
      "528:\tlearn: 3.5945825\ttest: 3.7999036\tbest: 3.7968818 (259)\ttotal: 24m 19s\tremaining: 21m 39s\n",
      "529:\tlearn: 3.5943266\ttest: 3.7999252\tbest: 3.7968818 (259)\ttotal: 24m 22s\tremaining: 21m 36s\n",
      "530:\tlearn: 3.5940803\ttest: 3.7999866\tbest: 3.7968818 (259)\ttotal: 24m 24s\tremaining: 21m 33s\n",
      "531:\tlearn: 3.5938414\ttest: 3.8000002\tbest: 3.7968818 (259)\ttotal: 24m 27s\tremaining: 21m 30s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532:\tlearn: 3.5936042\ttest: 3.8000032\tbest: 3.7968818 (259)\ttotal: 24m 29s\tremaining: 21m 27s\n",
      "533:\tlearn: 3.5933537\ttest: 3.8000171\tbest: 3.7968818 (259)\ttotal: 24m 32s\tremaining: 21m 24s\n",
      "534:\tlearn: 3.5932008\ttest: 3.8000334\tbest: 3.7968818 (259)\ttotal: 24m 34s\tremaining: 21m 21s\n",
      "535:\tlearn: 3.5930076\ttest: 3.7999873\tbest: 3.7968818 (259)\ttotal: 24m 36s\tremaining: 21m 18s\n",
      "536:\tlearn: 3.5927575\ttest: 3.7999865\tbest: 3.7968818 (259)\ttotal: 24m 39s\tremaining: 21m 15s\n",
      "537:\tlearn: 3.5925474\ttest: 3.7999477\tbest: 3.7968818 (259)\ttotal: 24m 41s\tremaining: 21m 12s\n",
      "538:\tlearn: 3.5922262\ttest: 3.7999764\tbest: 3.7968818 (259)\ttotal: 24m 44s\tremaining: 21m 9s\n",
      "539:\tlearn: 3.5918873\ttest: 3.8000191\tbest: 3.7968818 (259)\ttotal: 24m 46s\tremaining: 21m 6s\n",
      "540:\tlearn: 3.5916441\ttest: 3.8000244\tbest: 3.7968818 (259)\ttotal: 24m 49s\tremaining: 21m 3s\n",
      "541:\tlearn: 3.5911563\ttest: 3.8000756\tbest: 3.7968818 (259)\ttotal: 24m 51s\tremaining: 21m\n",
      "542:\tlearn: 3.5910167\ttest: 3.8000749\tbest: 3.7968818 (259)\ttotal: 24m 54s\tremaining: 20m 57s\n",
      "543:\tlearn: 3.5904005\ttest: 3.8000945\tbest: 3.7968818 (259)\ttotal: 24m 56s\tremaining: 20m 54s\n",
      "544:\tlearn: 3.5901469\ttest: 3.8000971\tbest: 3.7968818 (259)\ttotal: 24m 59s\tremaining: 20m 51s\n",
      "545:\tlearn: 3.5899014\ttest: 3.8001512\tbest: 3.7968818 (259)\ttotal: 25m 1s\tremaining: 20m 48s\n",
      "546:\tlearn: 3.5896356\ttest: 3.8001390\tbest: 3.7968818 (259)\ttotal: 25m 4s\tremaining: 20m 45s\n",
      "547:\tlearn: 3.5893040\ttest: 3.8001443\tbest: 3.7968818 (259)\ttotal: 25m 6s\tremaining: 20m 42s\n",
      "548:\tlearn: 3.5889366\ttest: 3.8001431\tbest: 3.7968818 (259)\ttotal: 25m 9s\tremaining: 20m 39s\n",
      "549:\tlearn: 3.5885135\ttest: 3.8001577\tbest: 3.7968818 (259)\ttotal: 25m 11s\tremaining: 20m 36s\n",
      "550:\tlearn: 3.5882104\ttest: 3.8001616\tbest: 3.7968818 (259)\ttotal: 25m 14s\tremaining: 20m 33s\n",
      "551:\tlearn: 3.5880702\ttest: 3.8001621\tbest: 3.7968818 (259)\ttotal: 25m 16s\tremaining: 20m 30s\n",
      "552:\tlearn: 3.5877248\ttest: 3.8001622\tbest: 3.7968818 (259)\ttotal: 25m 18s\tremaining: 20m 27s\n",
      "553:\tlearn: 3.5873979\ttest: 3.8001680\tbest: 3.7968818 (259)\ttotal: 25m 21s\tremaining: 20m 24s\n",
      "554:\tlearn: 3.5870229\ttest: 3.8001883\tbest: 3.7968818 (259)\ttotal: 25m 23s\tremaining: 20m 21s\n",
      "555:\tlearn: 3.5867756\ttest: 3.8002218\tbest: 3.7968818 (259)\ttotal: 25m 26s\tremaining: 20m 18s\n",
      "556:\tlearn: 3.5866370\ttest: 3.8002346\tbest: 3.7968818 (259)\ttotal: 25m 28s\tremaining: 20m 15s\n",
      "557:\tlearn: 3.5863984\ttest: 3.8002515\tbest: 3.7968818 (259)\ttotal: 25m 31s\tremaining: 20m 12s\n",
      "558:\tlearn: 3.5859160\ttest: 3.8002777\tbest: 3.7968818 (259)\ttotal: 25m 33s\tremaining: 20m 9s\n",
      "559:\tlearn: 3.5855968\ttest: 3.8002631\tbest: 3.7968818 (259)\ttotal: 25m 35s\tremaining: 20m 6s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 3.796881836\n",
      "bestIteration = 259\n",
      "\n",
      "Shrink model to first 260 iterations.\n",
      "0:\tlearn: 3.8689582\ttest: 3.8673872\tbest: 3.8673872 (0)\ttotal: 2.38s\tremaining: 39m 38s\n",
      "1:\tlearn: 3.8668729\ttest: 3.8657906\tbest: 3.8657906 (1)\ttotal: 4.81s\tremaining: 40m 2s\n",
      "2:\tlearn: 3.8650476\ttest: 3.8641419\tbest: 3.8641419 (2)\ttotal: 7.23s\tremaining: 40m 1s\n",
      "3:\tlearn: 3.8632498\ttest: 3.8626295\tbest: 3.8626295 (3)\ttotal: 9.61s\tremaining: 39m 53s\n",
      "4:\tlearn: 3.8613437\ttest: 3.8610004\tbest: 3.8610004 (4)\ttotal: 12s\tremaining: 39m 49s\n",
      "5:\tlearn: 3.8594708\ttest: 3.8595224\tbest: 3.8595224 (5)\ttotal: 14.5s\tremaining: 39m 59s\n",
      "6:\tlearn: 3.8578400\ttest: 3.8580605\tbest: 3.8580605 (6)\ttotal: 16.9s\tremaining: 39m 52s\n",
      "7:\tlearn: 3.8561188\ttest: 3.8566961\tbest: 3.8566961 (7)\ttotal: 19.3s\tremaining: 39m 48s\n",
      "8:\tlearn: 3.8544187\ttest: 3.8552892\tbest: 3.8552892 (8)\ttotal: 21.7s\tremaining: 39m 45s\n",
      "9:\tlearn: 3.8526702\ttest: 3.8539507\tbest: 3.8539507 (9)\ttotal: 24.1s\tremaining: 39m 44s\n",
      "10:\tlearn: 3.8510718\ttest: 3.8525747\tbest: 3.8525747 (10)\ttotal: 26.5s\tremaining: 39m 41s\n",
      "11:\tlearn: 3.8495062\ttest: 3.8512248\tbest: 3.8512248 (11)\ttotal: 28.9s\tremaining: 39m 40s\n",
      "12:\tlearn: 3.8478734\ttest: 3.8499329\tbest: 3.8499329 (12)\ttotal: 31.3s\tremaining: 39m 38s\n",
      "13:\tlearn: 3.8462444\ttest: 3.8487576\tbest: 3.8487576 (13)\ttotal: 34.1s\tremaining: 39m 59s\n",
      "14:\tlearn: 3.8447439\ttest: 3.8474869\tbest: 3.8474869 (14)\ttotal: 36.7s\tremaining: 40m 10s\n",
      "15:\tlearn: 3.8430227\ttest: 3.8462523\tbest: 3.8462523 (15)\ttotal: 39.2s\tremaining: 40m 9s\n",
      "16:\tlearn: 3.8414549\ttest: 3.8450650\tbest: 3.8450650 (16)\ttotal: 41.6s\tremaining: 40m 5s\n",
      "17:\tlearn: 3.8399017\ttest: 3.8438548\tbest: 3.8438548 (17)\ttotal: 44s\tremaining: 40m 2s\n",
      "18:\tlearn: 3.8383841\ttest: 3.8426600\tbest: 3.8426600 (18)\ttotal: 46.5s\tremaining: 39m 58s\n",
      "19:\tlearn: 3.8370444\ttest: 3.8415543\tbest: 3.8415543 (19)\ttotal: 48.9s\tremaining: 39m 54s\n",
      "20:\tlearn: 3.8355162\ttest: 3.8404129\tbest: 3.8404129 (20)\ttotal: 51.3s\tremaining: 39m 50s\n",
      "21:\tlearn: 3.8340693\ttest: 3.8393450\tbest: 3.8393450 (21)\ttotal: 53.7s\tremaining: 39m 47s\n",
      "22:\tlearn: 3.8326101\ttest: 3.8382410\tbest: 3.8382410 (22)\ttotal: 56.1s\tremaining: 39m 43s\n",
      "23:\tlearn: 3.8311305\ttest: 3.8372316\tbest: 3.8372316 (23)\ttotal: 58.5s\tremaining: 39m 40s\n",
      "24:\tlearn: 3.8297526\ttest: 3.8361880\tbest: 3.8361880 (24)\ttotal: 1m\tremaining: 39m 35s\n",
      "25:\tlearn: 3.8283290\ttest: 3.8351257\tbest: 3.8351257 (25)\ttotal: 1m 3s\tremaining: 39m 31s\n",
      "26:\tlearn: 3.8270503\ttest: 3.8340922\tbest: 3.8340922 (26)\ttotal: 1m 5s\tremaining: 39m 28s\n",
      "27:\tlearn: 3.8258374\ttest: 3.8330656\tbest: 3.8330656 (27)\ttotal: 1m 8s\tremaining: 39m 24s\n",
      "28:\tlearn: 3.8246738\ttest: 3.8320380\tbest: 3.8320380 (28)\ttotal: 1m 10s\tremaining: 39m 20s\n",
      "29:\tlearn: 3.8233102\ttest: 3.8311072\tbest: 3.8311072 (29)\ttotal: 1m 12s\tremaining: 39m 18s\n",
      "30:\tlearn: 3.8220471\ttest: 3.8301740\tbest: 3.8301740 (30)\ttotal: 1m 15s\tremaining: 39m 15s\n",
      "31:\tlearn: 3.8208539\ttest: 3.8291966\tbest: 3.8291966 (31)\ttotal: 1m 17s\tremaining: 39m 11s\n",
      "32:\tlearn: 3.8195862\ttest: 3.8282878\tbest: 3.8282878 (32)\ttotal: 1m 20s\tremaining: 39m 8s\n",
      "33:\tlearn: 3.8183987\ttest: 3.8274057\tbest: 3.8274057 (33)\ttotal: 1m 22s\tremaining: 39m 4s\n",
      "34:\tlearn: 3.8171886\ttest: 3.8265533\tbest: 3.8265533 (34)\ttotal: 1m 24s\tremaining: 39m 1s\n",
      "35:\tlearn: 3.8159929\ttest: 3.8256712\tbest: 3.8256712 (35)\ttotal: 1m 27s\tremaining: 38m 57s\n",
      "36:\tlearn: 3.8147761\ttest: 3.8247893\tbest: 3.8247893 (36)\ttotal: 1m 29s\tremaining: 38m 54s\n",
      "37:\tlearn: 3.8136484\ttest: 3.8239796\tbest: 3.8239796 (37)\ttotal: 1m 32s\tremaining: 38m 51s\n",
      "38:\tlearn: 3.8125810\ttest: 3.8231854\tbest: 3.8231854 (38)\ttotal: 1m 34s\tremaining: 38m 52s\n",
      "39:\tlearn: 3.8114591\ttest: 3.8223688\tbest: 3.8223688 (39)\ttotal: 1m 37s\tremaining: 38m 53s\n",
      "40:\tlearn: 3.8103075\ttest: 3.8216072\tbest: 3.8216072 (40)\ttotal: 1m 39s\tremaining: 38m 52s\n",
      "41:\tlearn: 3.8091851\ttest: 3.8208372\tbest: 3.8208372 (41)\ttotal: 1m 42s\tremaining: 38m 51s\n",
      "42:\tlearn: 3.8080758\ttest: 3.8200655\tbest: 3.8200655 (42)\ttotal: 1m 44s\tremaining: 38m 53s\n",
      "43:\tlearn: 3.8071386\ttest: 3.8193138\tbest: 3.8193138 (43)\ttotal: 1m 47s\tremaining: 38m 55s\n",
      "44:\tlearn: 3.8059316\ttest: 3.8186146\tbest: 3.8186146 (44)\ttotal: 1m 50s\tremaining: 38m 57s\n",
      "45:\tlearn: 3.8050142\ttest: 3.8178826\tbest: 3.8178826 (45)\ttotal: 1m 52s\tremaining: 39m\n",
      "46:\tlearn: 3.8039976\ttest: 3.8171969\tbest: 3.8171969 (46)\ttotal: 1m 55s\tremaining: 39m\n",
      "47:\tlearn: 3.8031547\ttest: 3.8164703\tbest: 3.8164703 (47)\ttotal: 1m 57s\tremaining: 38m 56s\n",
      "48:\tlearn: 3.8020306\ttest: 3.8157291\tbest: 3.8157291 (48)\ttotal: 2m\tremaining: 38m 53s\n",
      "49:\tlearn: 3.8010225\ttest: 3.8151055\tbest: 3.8151055 (49)\ttotal: 2m 2s\tremaining: 38m 50s\n",
      "50:\tlearn: 3.7999205\ttest: 3.8144179\tbest: 3.8144179 (50)\ttotal: 2m 5s\tremaining: 38m 46s\n",
      "51:\tlearn: 3.7989705\ttest: 3.8137660\tbest: 3.8137660 (51)\ttotal: 2m 7s\tremaining: 38m 43s\n",
      "52:\tlearn: 3.7980842\ttest: 3.8131002\tbest: 3.8131002 (52)\ttotal: 2m 9s\tremaining: 38m 40s\n",
      "53:\tlearn: 3.7971524\ttest: 3.8124153\tbest: 3.8124153 (53)\ttotal: 2m 12s\tremaining: 38m 37s\n",
      "54:\tlearn: 3.7963236\ttest: 3.8118244\tbest: 3.8118244 (54)\ttotal: 2m 14s\tremaining: 38m 34s\n",
      "55:\tlearn: 3.7953880\ttest: 3.8112723\tbest: 3.8112723 (55)\ttotal: 2m 17s\tremaining: 38m 30s\n",
      "56:\tlearn: 3.7946407\ttest: 3.8106551\tbest: 3.8106551 (56)\ttotal: 2m 19s\tremaining: 38m 27s\n",
      "57:\tlearn: 3.7938264\ttest: 3.8100954\tbest: 3.8100954 (57)\ttotal: 2m 21s\tremaining: 38m 23s\n",
      "58:\tlearn: 3.7930296\ttest: 3.8095360\tbest: 3.8095360 (58)\ttotal: 2m 24s\tremaining: 38m 20s\n",
      "59:\tlearn: 3.7921247\ttest: 3.8089744\tbest: 3.8089744 (59)\ttotal: 2m 26s\tremaining: 38m 17s\n",
      "60:\tlearn: 3.7912336\ttest: 3.8085103\tbest: 3.8085103 (60)\ttotal: 2m 29s\tremaining: 38m 13s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61:\tlearn: 3.7904866\ttest: 3.8079669\tbest: 3.8079669 (61)\ttotal: 2m 31s\tremaining: 38m 10s\n",
      "62:\tlearn: 3.7896632\ttest: 3.8074834\tbest: 3.8074834 (62)\ttotal: 2m 33s\tremaining: 38m 7s\n",
      "63:\tlearn: 3.7889301\ttest: 3.8069891\tbest: 3.8069891 (63)\ttotal: 2m 36s\tremaining: 38m 4s\n",
      "64:\tlearn: 3.7882888\ttest: 3.8064430\tbest: 3.8064430 (64)\ttotal: 2m 38s\tremaining: 38m 1s\n",
      "65:\tlearn: 3.7871736\ttest: 3.8058795\tbest: 3.8058795 (65)\ttotal: 2m 41s\tremaining: 37m 58s\n",
      "66:\tlearn: 3.7864992\ttest: 3.8053900\tbest: 3.8053900 (66)\ttotal: 2m 43s\tremaining: 37m 55s\n",
      "67:\tlearn: 3.7857135\ttest: 3.8049400\tbest: 3.8049400 (67)\ttotal: 2m 45s\tremaining: 37m 52s\n",
      "68:\tlearn: 3.7849447\ttest: 3.8044646\tbest: 3.8044646 (68)\ttotal: 2m 48s\tremaining: 37m 49s\n",
      "69:\tlearn: 3.7839804\ttest: 3.8040461\tbest: 3.8040461 (69)\ttotal: 2m 50s\tremaining: 37m 46s\n",
      "70:\tlearn: 3.7832795\ttest: 3.8035915\tbest: 3.8035915 (70)\ttotal: 2m 53s\tremaining: 37m 43s\n",
      "71:\tlearn: 3.7825048\ttest: 3.8031293\tbest: 3.8031293 (71)\ttotal: 2m 55s\tremaining: 37m 40s\n",
      "72:\tlearn: 3.7817834\ttest: 3.8026826\tbest: 3.8026826 (72)\ttotal: 2m 57s\tremaining: 37m 37s\n",
      "73:\tlearn: 3.7810006\ttest: 3.8021930\tbest: 3.8021930 (73)\ttotal: 3m\tremaining: 37m 34s\n",
      "74:\tlearn: 3.7803670\ttest: 3.8017932\tbest: 3.8017932 (74)\ttotal: 3m 2s\tremaining: 37m 31s\n",
      "75:\tlearn: 3.7796630\ttest: 3.8013651\tbest: 3.8013651 (75)\ttotal: 3m 4s\tremaining: 37m 29s\n",
      "76:\tlearn: 3.7788586\ttest: 3.8010036\tbest: 3.8010036 (76)\ttotal: 3m 7s\tremaining: 37m 26s\n",
      "77:\tlearn: 3.7783643\ttest: 3.8006127\tbest: 3.8006127 (77)\ttotal: 3m 9s\tremaining: 37m 24s\n",
      "78:\tlearn: 3.7775388\ttest: 3.8002327\tbest: 3.8002327 (78)\ttotal: 3m 12s\tremaining: 37m 24s\n",
      "79:\tlearn: 3.7768283\ttest: 3.7998907\tbest: 3.7998907 (79)\ttotal: 3m 14s\tremaining: 37m 22s\n",
      "80:\tlearn: 3.7759694\ttest: 3.7995931\tbest: 3.7995931 (80)\ttotal: 3m 17s\tremaining: 37m 20s\n",
      "81:\tlearn: 3.7751603\ttest: 3.7992246\tbest: 3.7992246 (81)\ttotal: 3m 20s\tremaining: 37m 19s\n",
      "82:\tlearn: 3.7746529\ttest: 3.7988525\tbest: 3.7988525 (82)\ttotal: 3m 22s\tremaining: 37m 18s\n",
      "83:\tlearn: 3.7740199\ttest: 3.7984946\tbest: 3.7984946 (83)\ttotal: 3m 25s\tremaining: 37m 17s\n",
      "84:\tlearn: 3.7731238\ttest: 3.7981440\tbest: 3.7981440 (84)\ttotal: 3m 27s\tremaining: 37m 14s\n",
      "85:\tlearn: 3.7725284\ttest: 3.7977784\tbest: 3.7977784 (85)\ttotal: 3m 29s\tremaining: 37m 11s\n",
      "86:\tlearn: 3.7719431\ttest: 3.7973921\tbest: 3.7973921 (86)\ttotal: 3m 32s\tremaining: 37m 8s\n",
      "87:\tlearn: 3.7713316\ttest: 3.7970752\tbest: 3.7970752 (87)\ttotal: 3m 34s\tremaining: 37m 5s\n",
      "88:\tlearn: 3.7706476\ttest: 3.7967070\tbest: 3.7967070 (88)\ttotal: 3m 37s\tremaining: 37m 2s\n",
      "89:\tlearn: 3.7699831\ttest: 3.7963924\tbest: 3.7963924 (89)\ttotal: 3m 39s\tremaining: 37m\n",
      "90:\tlearn: 3.7690945\ttest: 3.7960691\tbest: 3.7960691 (90)\ttotal: 3m 42s\tremaining: 36m 58s\n",
      "91:\tlearn: 3.7684207\ttest: 3.7957743\tbest: 3.7957743 (91)\ttotal: 3m 44s\tremaining: 36m 55s\n",
      "92:\tlearn: 3.7678672\ttest: 3.7954791\tbest: 3.7954791 (92)\ttotal: 3m 46s\tremaining: 36m 52s\n",
      "93:\tlearn: 3.7672699\ttest: 3.7951895\tbest: 3.7951895 (93)\ttotal: 3m 49s\tremaining: 36m 50s\n",
      "94:\tlearn: 3.7664824\ttest: 3.7949154\tbest: 3.7949154 (94)\ttotal: 3m 51s\tremaining: 36m 47s\n",
      "95:\tlearn: 3.7658319\ttest: 3.7945518\tbest: 3.7945518 (95)\ttotal: 3m 54s\tremaining: 36m 45s\n",
      "96:\tlearn: 3.7652380\ttest: 3.7942680\tbest: 3.7942680 (96)\ttotal: 3m 56s\tremaining: 36m 42s\n",
      "97:\tlearn: 3.7645259\ttest: 3.7940321\tbest: 3.7940321 (97)\ttotal: 3m 58s\tremaining: 36m 39s\n",
      "98:\tlearn: 3.7639391\ttest: 3.7937969\tbest: 3.7937969 (98)\ttotal: 4m 1s\tremaining: 36m 36s\n",
      "99:\tlearn: 3.7633250\ttest: 3.7935730\tbest: 3.7935730 (99)\ttotal: 4m 3s\tremaining: 36m 33s\n",
      "100:\tlearn: 3.7628666\ttest: 3.7932864\tbest: 3.7932864 (100)\ttotal: 4m 6s\tremaining: 36m 31s\n",
      "101:\tlearn: 3.7623188\ttest: 3.7930391\tbest: 3.7930391 (101)\ttotal: 4m 8s\tremaining: 36m 28s\n",
      "102:\tlearn: 3.7617496\ttest: 3.7928200\tbest: 3.7928200 (102)\ttotal: 4m 10s\tremaining: 36m 25s\n",
      "103:\tlearn: 3.7612970\ttest: 3.7926138\tbest: 3.7926138 (103)\ttotal: 4m 13s\tremaining: 36m 23s\n",
      "104:\tlearn: 3.7607620\ttest: 3.7923468\tbest: 3.7923468 (104)\ttotal: 4m 15s\tremaining: 36m 20s\n",
      "105:\tlearn: 3.7603110\ttest: 3.7921220\tbest: 3.7921220 (105)\ttotal: 4m 18s\tremaining: 36m 17s\n",
      "106:\tlearn: 3.7596918\ttest: 3.7918483\tbest: 3.7918483 (106)\ttotal: 4m 20s\tremaining: 36m 14s\n",
      "107:\tlearn: 3.7589795\ttest: 3.7916428\tbest: 3.7916428 (107)\ttotal: 4m 22s\tremaining: 36m 11s\n",
      "108:\tlearn: 3.7583571\ttest: 3.7913552\tbest: 3.7913552 (108)\ttotal: 4m 25s\tremaining: 36m 9s\n",
      "109:\tlearn: 3.7578640\ttest: 3.7911513\tbest: 3.7911513 (109)\ttotal: 4m 27s\tremaining: 36m 6s\n",
      "110:\tlearn: 3.7573885\ttest: 3.7909423\tbest: 3.7909423 (110)\ttotal: 4m 30s\tremaining: 36m 3s\n",
      "111:\tlearn: 3.7569088\ttest: 3.7907533\tbest: 3.7907533 (111)\ttotal: 4m 32s\tremaining: 36m\n",
      "112:\tlearn: 3.7562829\ttest: 3.7904905\tbest: 3.7904905 (112)\ttotal: 4m 34s\tremaining: 35m 57s\n",
      "113:\tlearn: 3.7558772\ttest: 3.7902465\tbest: 3.7902465 (113)\ttotal: 4m 37s\tremaining: 35m 55s\n",
      "114:\tlearn: 3.7552794\ttest: 3.7901130\tbest: 3.7901130 (114)\ttotal: 4m 39s\tremaining: 35m 52s\n",
      "115:\tlearn: 3.7547402\ttest: 3.7898617\tbest: 3.7898617 (115)\ttotal: 4m 42s\tremaining: 35m 50s\n",
      "116:\tlearn: 3.7541273\ttest: 3.7897347\tbest: 3.7897347 (116)\ttotal: 4m 44s\tremaining: 35m 47s\n",
      "117:\tlearn: 3.7536611\ttest: 3.7895174\tbest: 3.7895174 (117)\ttotal: 4m 46s\tremaining: 35m 44s\n",
      "118:\tlearn: 3.7530865\ttest: 3.7893022\tbest: 3.7893022 (118)\ttotal: 4m 49s\tremaining: 35m 41s\n",
      "119:\tlearn: 3.7526179\ttest: 3.7891070\tbest: 3.7891070 (119)\ttotal: 4m 51s\tremaining: 35m 39s\n",
      "120:\tlearn: 3.7520651\ttest: 3.7889271\tbest: 3.7889271 (120)\ttotal: 4m 54s\tremaining: 35m 36s\n",
      "121:\tlearn: 3.7515800\ttest: 3.7887544\tbest: 3.7887544 (121)\ttotal: 4m 56s\tremaining: 35m 35s\n",
      "122:\tlearn: 3.7510975\ttest: 3.7885390\tbest: 3.7885390 (122)\ttotal: 4m 59s\tremaining: 35m 34s\n",
      "123:\tlearn: 3.7506300\ttest: 3.7883898\tbest: 3.7883898 (123)\ttotal: 5m 2s\tremaining: 35m 39s\n",
      "124:\tlearn: 3.7501471\ttest: 3.7882213\tbest: 3.7882213 (124)\ttotal: 5m 5s\tremaining: 35m 40s\n",
      "125:\tlearn: 3.7495971\ttest: 3.7880399\tbest: 3.7880399 (125)\ttotal: 5m 8s\tremaining: 35m 39s\n",
      "126:\tlearn: 3.7490516\ttest: 3.7878557\tbest: 3.7878557 (126)\ttotal: 5m 11s\tremaining: 35m 39s\n",
      "127:\tlearn: 3.7485422\ttest: 3.7876730\tbest: 3.7876730 (127)\ttotal: 5m 13s\tremaining: 35m 37s\n",
      "128:\tlearn: 3.7481295\ttest: 3.7875133\tbest: 3.7875133 (128)\ttotal: 5m 16s\tremaining: 35m 36s\n",
      "129:\tlearn: 3.7476186\ttest: 3.7874177\tbest: 3.7874177 (129)\ttotal: 5m 19s\tremaining: 35m 37s\n",
      "130:\tlearn: 3.7470121\ttest: 3.7871879\tbest: 3.7871879 (130)\ttotal: 5m 22s\tremaining: 35m 41s\n",
      "131:\tlearn: 3.7465267\ttest: 3.7870340\tbest: 3.7870340 (131)\ttotal: 5m 26s\tremaining: 35m 44s\n",
      "132:\tlearn: 3.7461044\ttest: 3.7869083\tbest: 3.7869083 (132)\ttotal: 5m 29s\tremaining: 35m 46s\n",
      "133:\tlearn: 3.7455393\ttest: 3.7868005\tbest: 3.7868005 (133)\ttotal: 5m 32s\tremaining: 35m 50s\n",
      "134:\tlearn: 3.7450892\ttest: 3.7867160\tbest: 3.7867160 (134)\ttotal: 5m 36s\tremaining: 35m 55s\n",
      "135:\tlearn: 3.7446699\ttest: 3.7865679\tbest: 3.7865679 (135)\ttotal: 5m 40s\tremaining: 36m\n",
      "136:\tlearn: 3.7442505\ttest: 3.7863834\tbest: 3.7863834 (136)\ttotal: 5m 43s\tremaining: 36m 4s\n",
      "137:\tlearn: 3.7436725\ttest: 3.7863158\tbest: 3.7863158 (137)\ttotal: 5m 47s\tremaining: 36m 9s\n",
      "138:\tlearn: 3.7432754\ttest: 3.7861861\tbest: 3.7861861 (138)\ttotal: 5m 51s\tremaining: 36m 16s\n",
      "139:\tlearn: 3.7429362\ttest: 3.7860758\tbest: 3.7860758 (139)\ttotal: 5m 54s\tremaining: 36m 14s\n",
      "140:\tlearn: 3.7425830\ttest: 3.7859363\tbest: 3.7859363 (140)\ttotal: 5m 56s\tremaining: 36m 14s\n",
      "141:\tlearn: 3.7422166\ttest: 3.7857628\tbest: 3.7857628 (141)\ttotal: 5m 59s\tremaining: 36m 13s\n",
      "142:\tlearn: 3.7417215\ttest: 3.7856353\tbest: 3.7856353 (142)\ttotal: 6m 2s\tremaining: 36m 13s\n",
      "143:\tlearn: 3.7414315\ttest: 3.7854687\tbest: 3.7854687 (143)\ttotal: 6m 5s\tremaining: 36m 13s\n",
      "144:\tlearn: 3.7408637\ttest: 3.7854285\tbest: 3.7854285 (144)\ttotal: 6m 8s\tremaining: 36m 13s\n",
      "145:\tlearn: 3.7404961\ttest: 3.7853211\tbest: 3.7853211 (145)\ttotal: 6m 12s\tremaining: 36m 16s\n",
      "146:\tlearn: 3.7400346\ttest: 3.7852329\tbest: 3.7852329 (146)\ttotal: 6m 15s\tremaining: 36m 20s\n",
      "147:\tlearn: 3.7396634\ttest: 3.7851065\tbest: 3.7851065 (147)\ttotal: 6m 18s\tremaining: 36m 19s\n",
      "148:\tlearn: 3.7391313\ttest: 3.7849563\tbest: 3.7849563 (148)\ttotal: 6m 21s\tremaining: 36m 17s\n",
      "149:\tlearn: 3.7386185\ttest: 3.7848168\tbest: 3.7848168 (149)\ttotal: 6m 23s\tremaining: 36m 15s\n",
      "150:\tlearn: 3.7382141\ttest: 3.7847444\tbest: 3.7847444 (150)\ttotal: 6m 26s\tremaining: 36m 13s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151:\tlearn: 3.7380152\ttest: 3.7846325\tbest: 3.7846325 (151)\ttotal: 6m 29s\tremaining: 36m 11s\n",
      "152:\tlearn: 3.7376300\ttest: 3.7845661\tbest: 3.7845661 (152)\ttotal: 6m 31s\tremaining: 36m 8s\n",
      "153:\tlearn: 3.7369636\ttest: 3.7845515\tbest: 3.7845515 (153)\ttotal: 6m 34s\tremaining: 36m 5s\n",
      "154:\tlearn: 3.7365584\ttest: 3.7843943\tbest: 3.7843943 (154)\ttotal: 6m 36s\tremaining: 36m 3s\n",
      "155:\tlearn: 3.7359303\ttest: 3.7842942\tbest: 3.7842942 (155)\ttotal: 6m 39s\tremaining: 36m\n",
      "156:\tlearn: 3.7356617\ttest: 3.7841788\tbest: 3.7841788 (156)\ttotal: 6m 41s\tremaining: 35m 57s\n",
      "157:\tlearn: 3.7353700\ttest: 3.7840663\tbest: 3.7840663 (157)\ttotal: 6m 44s\tremaining: 35m 54s\n",
      "158:\tlearn: 3.7349877\ttest: 3.7839911\tbest: 3.7839911 (158)\ttotal: 6m 46s\tremaining: 35m 51s\n",
      "159:\tlearn: 3.7346952\ttest: 3.7838400\tbest: 3.7838400 (159)\ttotal: 6m 49s\tremaining: 35m 48s\n",
      "160:\tlearn: 3.7342598\ttest: 3.7837470\tbest: 3.7837470 (160)\ttotal: 6m 51s\tremaining: 35m 45s\n",
      "161:\tlearn: 3.7339608\ttest: 3.7836466\tbest: 3.7836466 (161)\ttotal: 6m 54s\tremaining: 35m 42s\n",
      "162:\tlearn: 3.7332571\ttest: 3.7836148\tbest: 3.7836148 (162)\ttotal: 6m 56s\tremaining: 35m 39s\n",
      "163:\tlearn: 3.7327326\ttest: 3.7834945\tbest: 3.7834945 (163)\ttotal: 6m 58s\tremaining: 35m 35s\n",
      "164:\tlearn: 3.7323401\ttest: 3.7834353\tbest: 3.7834353 (164)\ttotal: 7m 1s\tremaining: 35m 32s\n",
      "165:\tlearn: 3.7318824\ttest: 3.7833862\tbest: 3.7833862 (165)\ttotal: 7m 3s\tremaining: 35m 29s\n",
      "166:\tlearn: 3.7313612\ttest: 3.7832700\tbest: 3.7832700 (166)\ttotal: 7m 6s\tremaining: 35m 26s\n",
      "167:\tlearn: 3.7308542\ttest: 3.7831819\tbest: 3.7831819 (167)\ttotal: 7m 8s\tremaining: 35m 23s\n",
      "168:\tlearn: 3.7304684\ttest: 3.7831096\tbest: 3.7831096 (168)\ttotal: 7m 11s\tremaining: 35m 20s\n",
      "169:\tlearn: 3.7299549\ttest: 3.7830933\tbest: 3.7830933 (169)\ttotal: 7m 13s\tremaining: 35m 17s\n",
      "170:\tlearn: 3.7294580\ttest: 3.7830290\tbest: 3.7830290 (170)\ttotal: 7m 16s\tremaining: 35m 14s\n",
      "171:\tlearn: 3.7291700\ttest: 3.7829725\tbest: 3.7829725 (171)\ttotal: 7m 18s\tremaining: 35m 11s\n",
      "172:\tlearn: 3.7287843\ttest: 3.7828889\tbest: 3.7828889 (172)\ttotal: 7m 20s\tremaining: 35m 7s\n",
      "173:\tlearn: 3.7283604\ttest: 3.7828348\tbest: 3.7828348 (173)\ttotal: 7m 23s\tremaining: 35m 4s\n",
      "174:\tlearn: 3.7277469\ttest: 3.7827832\tbest: 3.7827832 (174)\ttotal: 7m 25s\tremaining: 35m 1s\n",
      "175:\tlearn: 3.7273819\ttest: 3.7827176\tbest: 3.7827176 (175)\ttotal: 7m 28s\tremaining: 34m 58s\n",
      "176:\tlearn: 3.7269778\ttest: 3.7826456\tbest: 3.7826456 (176)\ttotal: 7m 30s\tremaining: 34m 55s\n",
      "177:\tlearn: 3.7266731\ttest: 3.7825671\tbest: 3.7825671 (177)\ttotal: 7m 33s\tremaining: 34m 52s\n",
      "178:\tlearn: 3.7263525\ttest: 3.7825183\tbest: 3.7825183 (178)\ttotal: 7m 35s\tremaining: 34m 49s\n",
      "179:\tlearn: 3.7260715\ttest: 3.7824403\tbest: 3.7824403 (179)\ttotal: 7m 37s\tremaining: 34m 46s\n",
      "180:\tlearn: 3.7255806\ttest: 3.7824059\tbest: 3.7824059 (180)\ttotal: 7m 40s\tremaining: 34m 43s\n",
      "181:\tlearn: 3.7249645\ttest: 3.7823524\tbest: 3.7823524 (181)\ttotal: 7m 42s\tremaining: 34m 40s\n",
      "182:\tlearn: 3.7245930\ttest: 3.7822900\tbest: 3.7822900 (182)\ttotal: 7m 45s\tremaining: 34m 37s\n",
      "183:\tlearn: 3.7242935\ttest: 3.7822272\tbest: 3.7822272 (183)\ttotal: 7m 47s\tremaining: 34m 34s\n",
      "184:\tlearn: 3.7239367\ttest: 3.7821483\tbest: 3.7821483 (184)\ttotal: 7m 50s\tremaining: 34m 31s\n",
      "185:\tlearn: 3.7235050\ttest: 3.7820413\tbest: 3.7820413 (185)\ttotal: 7m 52s\tremaining: 34m 28s\n",
      "186:\tlearn: 3.7230901\ttest: 3.7820132\tbest: 3.7820132 (186)\ttotal: 7m 55s\tremaining: 34m 25s\n",
      "187:\tlearn: 3.7225977\ttest: 3.7819557\tbest: 3.7819557 (187)\ttotal: 7m 57s\tremaining: 34m 22s\n",
      "188:\tlearn: 3.7220101\ttest: 3.7818781\tbest: 3.7818781 (188)\ttotal: 7m 59s\tremaining: 34m 18s\n",
      "189:\tlearn: 3.7214309\ttest: 3.7818305\tbest: 3.7818305 (189)\ttotal: 8m 2s\tremaining: 34m 16s\n",
      "190:\tlearn: 3.7210416\ttest: 3.7817636\tbest: 3.7817636 (190)\ttotal: 8m 4s\tremaining: 34m 14s\n",
      "191:\tlearn: 3.7206687\ttest: 3.7817067\tbest: 3.7817067 (191)\ttotal: 8m 7s\tremaining: 34m 11s\n",
      "192:\tlearn: 3.7201530\ttest: 3.7816486\tbest: 3.7816486 (192)\ttotal: 8m 9s\tremaining: 34m 8s\n",
      "193:\tlearn: 3.7197295\ttest: 3.7815815\tbest: 3.7815815 (193)\ttotal: 8m 12s\tremaining: 34m 5s\n",
      "194:\tlearn: 3.7193523\ttest: 3.7815175\tbest: 3.7815175 (194)\ttotal: 8m 14s\tremaining: 34m 2s\n",
      "195:\tlearn: 3.7189263\ttest: 3.7814704\tbest: 3.7814704 (195)\ttotal: 8m 17s\tremaining: 33m 59s\n",
      "196:\tlearn: 3.7186425\ttest: 3.7813993\tbest: 3.7813993 (196)\ttotal: 8m 19s\tremaining: 33m 56s\n",
      "197:\tlearn: 3.7182220\ttest: 3.7813191\tbest: 3.7813191 (197)\ttotal: 8m 22s\tremaining: 33m 53s\n",
      "198:\tlearn: 3.7178309\ttest: 3.7812815\tbest: 3.7812815 (198)\ttotal: 8m 24s\tremaining: 33m 50s\n",
      "199:\tlearn: 3.7175399\ttest: 3.7812174\tbest: 3.7812174 (199)\ttotal: 8m 27s\tremaining: 33m 47s\n",
      "200:\tlearn: 3.7173153\ttest: 3.7812004\tbest: 3.7812004 (200)\ttotal: 8m 29s\tremaining: 33m 45s\n",
      "201:\tlearn: 3.7168652\ttest: 3.7812036\tbest: 3.7812004 (200)\ttotal: 8m 31s\tremaining: 33m 42s\n",
      "202:\tlearn: 3.7163549\ttest: 3.7811185\tbest: 3.7811185 (202)\ttotal: 8m 34s\tremaining: 33m 39s\n",
      "203:\tlearn: 3.7159217\ttest: 3.7810878\tbest: 3.7810878 (203)\ttotal: 8m 36s\tremaining: 33m 36s\n",
      "204:\tlearn: 3.7154840\ttest: 3.7810955\tbest: 3.7810878 (203)\ttotal: 8m 39s\tremaining: 33m 33s\n",
      "205:\tlearn: 3.7151353\ttest: 3.7810392\tbest: 3.7810392 (205)\ttotal: 8m 41s\tremaining: 33m 30s\n",
      "206:\tlearn: 3.7148278\ttest: 3.7810168\tbest: 3.7810168 (206)\ttotal: 8m 44s\tremaining: 33m 27s\n",
      "207:\tlearn: 3.7145814\ttest: 3.7809554\tbest: 3.7809554 (207)\ttotal: 8m 46s\tremaining: 33m 24s\n",
      "208:\tlearn: 3.7144145\ttest: 3.7809118\tbest: 3.7809118 (208)\ttotal: 8m 48s\tremaining: 33m 21s\n",
      "209:\tlearn: 3.7138485\ttest: 3.7808901\tbest: 3.7808901 (209)\ttotal: 8m 51s\tremaining: 33m 18s\n",
      "210:\tlearn: 3.7133807\ttest: 3.7808195\tbest: 3.7808195 (210)\ttotal: 8m 53s\tremaining: 33m 15s\n",
      "211:\tlearn: 3.7131349\ttest: 3.7807741\tbest: 3.7807741 (211)\ttotal: 8m 56s\tremaining: 33m 12s\n",
      "212:\tlearn: 3.7125691\ttest: 3.7807574\tbest: 3.7807574 (212)\ttotal: 8m 58s\tremaining: 33m 10s\n",
      "213:\tlearn: 3.7120242\ttest: 3.7807598\tbest: 3.7807574 (212)\ttotal: 9m 1s\tremaining: 33m 7s\n",
      "214:\tlearn: 3.7116160\ttest: 3.7807220\tbest: 3.7807220 (214)\ttotal: 9m 3s\tremaining: 33m 4s\n",
      "215:\tlearn: 3.7113109\ttest: 3.7807295\tbest: 3.7807220 (214)\ttotal: 9m 5s\tremaining: 33m 1s\n",
      "216:\tlearn: 3.7110188\ttest: 3.7806408\tbest: 3.7806408 (216)\ttotal: 9m 8s\tremaining: 32m 58s\n",
      "217:\tlearn: 3.7106093\ttest: 3.7806223\tbest: 3.7806223 (217)\ttotal: 9m 10s\tremaining: 32m 55s\n",
      "218:\tlearn: 3.7102853\ttest: 3.7805449\tbest: 3.7805449 (218)\ttotal: 9m 13s\tremaining: 32m 53s\n",
      "219:\tlearn: 3.7098683\ttest: 3.7805400\tbest: 3.7805400 (219)\ttotal: 9m 15s\tremaining: 32m 50s\n",
      "220:\tlearn: 3.7093333\ttest: 3.7805092\tbest: 3.7805092 (220)\ttotal: 9m 18s\tremaining: 32m 48s\n",
      "221:\tlearn: 3.7089332\ttest: 3.7804762\tbest: 3.7804762 (221)\ttotal: 9m 20s\tremaining: 32m 45s\n",
      "222:\tlearn: 3.7086221\ttest: 3.7804123\tbest: 3.7804123 (222)\ttotal: 9m 23s\tremaining: 32m 43s\n",
      "223:\tlearn: 3.7083110\ttest: 3.7803838\tbest: 3.7803838 (223)\ttotal: 9m 25s\tremaining: 32m 40s\n",
      "224:\tlearn: 3.7080497\ttest: 3.7803639\tbest: 3.7803639 (224)\ttotal: 9m 28s\tremaining: 32m 37s\n",
      "225:\tlearn: 3.7077625\ttest: 3.7803738\tbest: 3.7803639 (224)\ttotal: 9m 30s\tremaining: 32m 34s\n",
      "226:\tlearn: 3.7070197\ttest: 3.7803754\tbest: 3.7803639 (224)\ttotal: 9m 33s\tremaining: 32m 31s\n",
      "227:\tlearn: 3.7066995\ttest: 3.7803583\tbest: 3.7803583 (227)\ttotal: 9m 35s\tremaining: 32m 28s\n",
      "228:\tlearn: 3.7062477\ttest: 3.7802972\tbest: 3.7802972 (228)\ttotal: 9m 37s\tremaining: 32m 25s\n",
      "229:\tlearn: 3.7058348\ttest: 3.7803328\tbest: 3.7802972 (228)\ttotal: 9m 40s\tremaining: 32m 23s\n",
      "230:\tlearn: 3.7054972\ttest: 3.7802906\tbest: 3.7802906 (230)\ttotal: 9m 42s\tremaining: 32m 20s\n",
      "231:\tlearn: 3.7052259\ttest: 3.7802650\tbest: 3.7802650 (231)\ttotal: 9m 45s\tremaining: 32m 17s\n",
      "232:\tlearn: 3.7049410\ttest: 3.7802547\tbest: 3.7802547 (232)\ttotal: 9m 47s\tremaining: 32m 14s\n",
      "233:\tlearn: 3.7041781\ttest: 3.7802437\tbest: 3.7802437 (233)\ttotal: 9m 50s\tremaining: 32m 11s\n",
      "234:\tlearn: 3.7039219\ttest: 3.7802286\tbest: 3.7802286 (234)\ttotal: 9m 52s\tremaining: 32m 8s\n",
      "235:\tlearn: 3.7035889\ttest: 3.7802147\tbest: 3.7802147 (235)\ttotal: 9m 54s\tremaining: 32m 6s\n",
      "236:\tlearn: 3.7032463\ttest: 3.7801891\tbest: 3.7801891 (236)\ttotal: 9m 57s\tremaining: 32m 3s\n",
      "237:\tlearn: 3.7030223\ttest: 3.7801674\tbest: 3.7801674 (237)\ttotal: 9m 59s\tremaining: 32m\n",
      "238:\tlearn: 3.7025062\ttest: 3.7801179\tbest: 3.7801179 (238)\ttotal: 10m 2s\tremaining: 31m 57s\n",
      "239:\tlearn: 3.7021964\ttest: 3.7801041\tbest: 3.7801041 (239)\ttotal: 10m 4s\tremaining: 31m 55s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240:\tlearn: 3.7016046\ttest: 3.7801041\tbest: 3.7801041 (239)\ttotal: 10m 7s\tremaining: 31m 52s\n",
      "241:\tlearn: 3.7010418\ttest: 3.7800637\tbest: 3.7800637 (241)\ttotal: 10m 9s\tremaining: 31m 49s\n",
      "242:\tlearn: 3.7005951\ttest: 3.7800327\tbest: 3.7800327 (242)\ttotal: 10m 12s\tremaining: 31m 47s\n",
      "243:\tlearn: 3.7001826\ttest: 3.7800311\tbest: 3.7800311 (243)\ttotal: 10m 14s\tremaining: 31m 44s\n",
      "244:\tlearn: 3.6995812\ttest: 3.7800172\tbest: 3.7800172 (244)\ttotal: 10m 17s\tremaining: 31m 43s\n",
      "245:\tlearn: 3.6990635\ttest: 3.7799992\tbest: 3.7799992 (245)\ttotal: 10m 20s\tremaining: 31m 41s\n",
      "246:\tlearn: 3.6985970\ttest: 3.7799938\tbest: 3.7799938 (246)\ttotal: 10m 22s\tremaining: 31m 39s\n",
      "247:\tlearn: 3.6982987\ttest: 3.7800247\tbest: 3.7799938 (246)\ttotal: 10m 25s\tremaining: 31m 36s\n",
      "248:\tlearn: 3.6978234\ttest: 3.7800497\tbest: 3.7799938 (246)\ttotal: 10m 27s\tremaining: 31m 33s\n",
      "249:\tlearn: 3.6975687\ttest: 3.7800735\tbest: 3.7799938 (246)\ttotal: 10m 30s\tremaining: 31m 30s\n",
      "250:\tlearn: 3.6970147\ttest: 3.7800316\tbest: 3.7799938 (246)\ttotal: 10m 32s\tremaining: 31m 28s\n",
      "251:\tlearn: 3.6964258\ttest: 3.7800102\tbest: 3.7799938 (246)\ttotal: 10m 35s\tremaining: 31m 25s\n",
      "252:\tlearn: 3.6960867\ttest: 3.7800036\tbest: 3.7799938 (246)\ttotal: 10m 37s\tremaining: 31m 22s\n",
      "253:\tlearn: 3.6957904\ttest: 3.7799770\tbest: 3.7799770 (253)\ttotal: 10m 40s\tremaining: 31m 19s\n",
      "254:\tlearn: 3.6956555\ttest: 3.7799435\tbest: 3.7799435 (254)\ttotal: 10m 42s\tremaining: 31m 17s\n",
      "255:\tlearn: 3.6953343\ttest: 3.7799155\tbest: 3.7799155 (255)\ttotal: 10m 44s\tremaining: 31m 14s\n",
      "256:\tlearn: 3.6948059\ttest: 3.7799551\tbest: 3.7799155 (255)\ttotal: 10m 47s\tremaining: 31m 11s\n",
      "257:\tlearn: 3.6945067\ttest: 3.7799237\tbest: 3.7799155 (255)\ttotal: 10m 49s\tremaining: 31m 8s\n",
      "258:\tlearn: 3.6942178\ttest: 3.7799330\tbest: 3.7799155 (255)\ttotal: 10m 52s\tremaining: 31m 5s\n",
      "259:\tlearn: 3.6938496\ttest: 3.7799438\tbest: 3.7799155 (255)\ttotal: 10m 54s\tremaining: 31m 3s\n",
      "260:\tlearn: 3.6933500\ttest: 3.7799152\tbest: 3.7799152 (260)\ttotal: 10m 57s\tremaining: 31m\n",
      "261:\tlearn: 3.6930761\ttest: 3.7798968\tbest: 3.7798968 (261)\ttotal: 10m 59s\tremaining: 30m 57s\n",
      "262:\tlearn: 3.6928405\ttest: 3.7798714\tbest: 3.7798714 (262)\ttotal: 11m 1s\tremaining: 30m 54s\n",
      "263:\tlearn: 3.6923191\ttest: 3.7799129\tbest: 3.7798714 (262)\ttotal: 11m 4s\tremaining: 30m 52s\n",
      "264:\tlearn: 3.6919499\ttest: 3.7799297\tbest: 3.7798714 (262)\ttotal: 11m 7s\tremaining: 30m 50s\n",
      "265:\tlearn: 3.6917148\ttest: 3.7799157\tbest: 3.7798714 (262)\ttotal: 11m 9s\tremaining: 30m 47s\n",
      "266:\tlearn: 3.6915485\ttest: 3.7798807\tbest: 3.7798714 (262)\ttotal: 11m 12s\tremaining: 30m 45s\n",
      "267:\tlearn: 3.6913371\ttest: 3.7798333\tbest: 3.7798333 (267)\ttotal: 11m 14s\tremaining: 30m 42s\n",
      "268:\tlearn: 3.6911288\ttest: 3.7798131\tbest: 3.7798131 (268)\ttotal: 11m 17s\tremaining: 30m 41s\n",
      "269:\tlearn: 3.6907427\ttest: 3.7797511\tbest: 3.7797511 (269)\ttotal: 11m 20s\tremaining: 30m 38s\n",
      "270:\tlearn: 3.6903041\ttest: 3.7797354\tbest: 3.7797354 (270)\ttotal: 11m 22s\tremaining: 30m 36s\n",
      "271:\tlearn: 3.6900463\ttest: 3.7797308\tbest: 3.7797308 (271)\ttotal: 11m 25s\tremaining: 30m 34s\n",
      "272:\tlearn: 3.6897540\ttest: 3.7797000\tbest: 3.7797000 (272)\ttotal: 11m 27s\tremaining: 30m 31s\n",
      "273:\tlearn: 3.6893042\ttest: 3.7797265\tbest: 3.7797000 (272)\ttotal: 11m 30s\tremaining: 30m 29s\n",
      "274:\tlearn: 3.6886962\ttest: 3.7797569\tbest: 3.7797000 (272)\ttotal: 11m 32s\tremaining: 30m 26s\n",
      "275:\tlearn: 3.6879571\ttest: 3.7797420\tbest: 3.7797000 (272)\ttotal: 11m 35s\tremaining: 30m 23s\n",
      "276:\tlearn: 3.6876817\ttest: 3.7797307\tbest: 3.7797000 (272)\ttotal: 11m 37s\tremaining: 30m 21s\n",
      "277:\tlearn: 3.6870367\ttest: 3.7797327\tbest: 3.7797000 (272)\ttotal: 11m 40s\tremaining: 30m 18s\n",
      "278:\tlearn: 3.6866914\ttest: 3.7797152\tbest: 3.7797000 (272)\ttotal: 11m 42s\tremaining: 30m 15s\n",
      "279:\tlearn: 3.6863063\ttest: 3.7796557\tbest: 3.7796557 (279)\ttotal: 11m 45s\tremaining: 30m 12s\n",
      "280:\tlearn: 3.6859940\ttest: 3.7796623\tbest: 3.7796557 (279)\ttotal: 11m 47s\tremaining: 30m 10s\n",
      "281:\tlearn: 3.6858238\ttest: 3.7796691\tbest: 3.7796557 (279)\ttotal: 11m 49s\tremaining: 30m 7s\n",
      "282:\tlearn: 3.6852971\ttest: 3.7796922\tbest: 3.7796557 (279)\ttotal: 11m 52s\tremaining: 30m 4s\n",
      "283:\tlearn: 3.6849925\ttest: 3.7796784\tbest: 3.7796557 (279)\ttotal: 11m 54s\tremaining: 30m 1s\n",
      "284:\tlearn: 3.6845711\ttest: 3.7796289\tbest: 3.7796289 (284)\ttotal: 11m 57s\tremaining: 29m 59s\n",
      "285:\tlearn: 3.6839723\ttest: 3.7796025\tbest: 3.7796025 (285)\ttotal: 11m 59s\tremaining: 29m 56s\n",
      "286:\tlearn: 3.6836437\ttest: 3.7796274\tbest: 3.7796025 (285)\ttotal: 12m 2s\tremaining: 29m 53s\n",
      "287:\tlearn: 3.6833963\ttest: 3.7796109\tbest: 3.7796025 (285)\ttotal: 12m 4s\tremaining: 29m 51s\n",
      "288:\tlearn: 3.6831851\ttest: 3.7795981\tbest: 3.7795981 (288)\ttotal: 12m 6s\tremaining: 29m 48s\n",
      "289:\tlearn: 3.6829220\ttest: 3.7795679\tbest: 3.7795679 (289)\ttotal: 12m 9s\tremaining: 29m 45s\n",
      "290:\tlearn: 3.6822694\ttest: 3.7795735\tbest: 3.7795679 (289)\ttotal: 12m 11s\tremaining: 29m 43s\n",
      "291:\tlearn: 3.6820082\ttest: 3.7795295\tbest: 3.7795295 (291)\ttotal: 12m 14s\tremaining: 29m 40s\n",
      "292:\tlearn: 3.6816489\ttest: 3.7795107\tbest: 3.7795107 (292)\ttotal: 12m 16s\tremaining: 29m 37s\n",
      "293:\tlearn: 3.6814179\ttest: 3.7794853\tbest: 3.7794853 (293)\ttotal: 12m 19s\tremaining: 29m 34s\n",
      "294:\tlearn: 3.6809050\ttest: 3.7795088\tbest: 3.7794853 (293)\ttotal: 12m 21s\tremaining: 29m 32s\n",
      "295:\tlearn: 3.6805534\ttest: 3.7795321\tbest: 3.7794853 (293)\ttotal: 12m 24s\tremaining: 29m 29s\n",
      "296:\tlearn: 3.6799205\ttest: 3.7795602\tbest: 3.7794853 (293)\ttotal: 12m 26s\tremaining: 29m 27s\n",
      "297:\tlearn: 3.6795123\ttest: 3.7795717\tbest: 3.7794853 (293)\ttotal: 12m 29s\tremaining: 29m 24s\n",
      "298:\tlearn: 3.6793786\ttest: 3.7795500\tbest: 3.7794853 (293)\ttotal: 12m 31s\tremaining: 29m 22s\n",
      "299:\tlearn: 3.6790366\ttest: 3.7795265\tbest: 3.7794853 (293)\ttotal: 12m 35s\tremaining: 29m 22s\n",
      "300:\tlearn: 3.6787118\ttest: 3.7794668\tbest: 3.7794668 (300)\ttotal: 12m 38s\tremaining: 29m 21s\n",
      "301:\tlearn: 3.6784755\ttest: 3.7794745\tbest: 3.7794668 (300)\ttotal: 12m 41s\tremaining: 29m 20s\n",
      "302:\tlearn: 3.6779510\ttest: 3.7794807\tbest: 3.7794668 (300)\ttotal: 12m 44s\tremaining: 29m 18s\n",
      "303:\tlearn: 3.6777268\ttest: 3.7794852\tbest: 3.7794668 (300)\ttotal: 12m 47s\tremaining: 29m 18s\n",
      "304:\tlearn: 3.6773288\ttest: 3.7794653\tbest: 3.7794653 (304)\ttotal: 12m 51s\tremaining: 29m 17s\n",
      "305:\tlearn: 3.6768971\ttest: 3.7794833\tbest: 3.7794653 (304)\ttotal: 12m 54s\tremaining: 29m 16s\n",
      "306:\tlearn: 3.6767085\ttest: 3.7794684\tbest: 3.7794653 (304)\ttotal: 12m 57s\tremaining: 29m 14s\n",
      "307:\tlearn: 3.6761578\ttest: 3.7794827\tbest: 3.7794653 (304)\ttotal: 13m\tremaining: 29m 12s\n",
      "308:\tlearn: 3.6757058\ttest: 3.7795076\tbest: 3.7794653 (304)\ttotal: 13m 3s\tremaining: 29m 11s\n",
      "309:\tlearn: 3.6752599\ttest: 3.7795120\tbest: 3.7794653 (304)\ttotal: 13m 5s\tremaining: 29m 8s\n",
      "310:\tlearn: 3.6748790\ttest: 3.7795291\tbest: 3.7794653 (304)\ttotal: 13m 7s\tremaining: 29m 5s\n",
      "311:\tlearn: 3.6745808\ttest: 3.7795208\tbest: 3.7794653 (304)\ttotal: 13m 10s\tremaining: 29m 2s\n",
      "312:\tlearn: 3.6744003\ttest: 3.7794964\tbest: 3.7794653 (304)\ttotal: 13m 12s\tremaining: 28m 59s\n",
      "313:\tlearn: 3.6738985\ttest: 3.7794962\tbest: 3.7794653 (304)\ttotal: 13m 15s\tremaining: 28m 57s\n",
      "314:\tlearn: 3.6735172\ttest: 3.7794565\tbest: 3.7794565 (314)\ttotal: 13m 17s\tremaining: 28m 54s\n",
      "315:\tlearn: 3.6733293\ttest: 3.7794581\tbest: 3.7794565 (314)\ttotal: 13m 19s\tremaining: 28m 51s\n",
      "316:\tlearn: 3.6730615\ttest: 3.7794522\tbest: 3.7794522 (316)\ttotal: 13m 22s\tremaining: 28m 48s\n",
      "317:\tlearn: 3.6727564\ttest: 3.7794621\tbest: 3.7794522 (316)\ttotal: 13m 24s\tremaining: 28m 45s\n",
      "318:\tlearn: 3.6723861\ttest: 3.7794789\tbest: 3.7794522 (316)\ttotal: 13m 27s\tremaining: 28m 43s\n",
      "319:\tlearn: 3.6721179\ttest: 3.7794620\tbest: 3.7794522 (316)\ttotal: 13m 29s\tremaining: 28m 40s\n",
      "320:\tlearn: 3.6718914\ttest: 3.7794425\tbest: 3.7794425 (320)\ttotal: 13m 31s\tremaining: 28m 37s\n",
      "321:\tlearn: 3.6716160\ttest: 3.7794523\tbest: 3.7794425 (320)\ttotal: 13m 34s\tremaining: 28m 34s\n",
      "322:\tlearn: 3.6712257\ttest: 3.7794312\tbest: 3.7794312 (322)\ttotal: 13m 36s\tremaining: 28m 32s\n",
      "323:\tlearn: 3.6709952\ttest: 3.7794547\tbest: 3.7794312 (322)\ttotal: 13m 39s\tremaining: 28m 29s\n",
      "324:\tlearn: 3.6706234\ttest: 3.7794237\tbest: 3.7794237 (324)\ttotal: 13m 41s\tremaining: 28m 26s\n",
      "325:\tlearn: 3.6702260\ttest: 3.7794662\tbest: 3.7794237 (324)\ttotal: 13m 44s\tremaining: 28m 23s\n",
      "326:\tlearn: 3.6698476\ttest: 3.7794752\tbest: 3.7794237 (324)\ttotal: 13m 46s\tremaining: 28m 20s\n",
      "327:\tlearn: 3.6693399\ttest: 3.7794863\tbest: 3.7794237 (324)\ttotal: 13m 48s\tremaining: 28m 18s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328:\tlearn: 3.6692362\ttest: 3.7794813\tbest: 3.7794237 (324)\ttotal: 13m 51s\tremaining: 28m 15s\n",
      "329:\tlearn: 3.6688892\ttest: 3.7794683\tbest: 3.7794237 (324)\ttotal: 13m 53s\tremaining: 28m 13s\n",
      "330:\tlearn: 3.6682680\ttest: 3.7794642\tbest: 3.7794237 (324)\ttotal: 13m 56s\tremaining: 28m 10s\n",
      "331:\tlearn: 3.6679718\ttest: 3.7794383\tbest: 3.7794237 (324)\ttotal: 13m 58s\tremaining: 28m 7s\n",
      "332:\tlearn: 3.6676865\ttest: 3.7794663\tbest: 3.7794237 (324)\ttotal: 14m 1s\tremaining: 28m 5s\n",
      "333:\tlearn: 3.6674390\ttest: 3.7794284\tbest: 3.7794237 (324)\ttotal: 14m 3s\tremaining: 28m 2s\n",
      "334:\tlearn: 3.6671051\ttest: 3.7794254\tbest: 3.7794237 (324)\ttotal: 14m 6s\tremaining: 27m 59s\n",
      "335:\tlearn: 3.6665957\ttest: 3.7794580\tbest: 3.7794237 (324)\ttotal: 14m 8s\tremaining: 27m 56s\n",
      "336:\tlearn: 3.6662554\ttest: 3.7795222\tbest: 3.7794237 (324)\ttotal: 14m 10s\tremaining: 27m 53s\n",
      "337:\tlearn: 3.6660799\ttest: 3.7795162\tbest: 3.7794237 (324)\ttotal: 14m 13s\tremaining: 27m 51s\n",
      "338:\tlearn: 3.6654954\ttest: 3.7795341\tbest: 3.7794237 (324)\ttotal: 14m 15s\tremaining: 27m 48s\n",
      "339:\tlearn: 3.6652221\ttest: 3.7795059\tbest: 3.7794237 (324)\ttotal: 14m 18s\tremaining: 27m 45s\n",
      "340:\tlearn: 3.6646900\ttest: 3.7794940\tbest: 3.7794237 (324)\ttotal: 14m 20s\tremaining: 27m 42s\n",
      "341:\tlearn: 3.6641097\ttest: 3.7795247\tbest: 3.7794237 (324)\ttotal: 14m 22s\tremaining: 27m 40s\n",
      "342:\tlearn: 3.6636324\ttest: 3.7795280\tbest: 3.7794237 (324)\ttotal: 14m 25s\tremaining: 27m 37s\n",
      "343:\tlearn: 3.6633237\ttest: 3.7795475\tbest: 3.7794237 (324)\ttotal: 14m 27s\tremaining: 27m 34s\n",
      "344:\tlearn: 3.6628917\ttest: 3.7795228\tbest: 3.7794237 (324)\ttotal: 14m 30s\tremaining: 27m 32s\n",
      "345:\tlearn: 3.6624085\ttest: 3.7795526\tbest: 3.7794237 (324)\ttotal: 14m 32s\tremaining: 27m 29s\n",
      "346:\tlearn: 3.6620650\ttest: 3.7795663\tbest: 3.7794237 (324)\ttotal: 14m 35s\tremaining: 27m 26s\n",
      "347:\tlearn: 3.6615347\ttest: 3.7796057\tbest: 3.7794237 (324)\ttotal: 14m 37s\tremaining: 27m 23s\n",
      "348:\tlearn: 3.6612142\ttest: 3.7796095\tbest: 3.7794237 (324)\ttotal: 14m 39s\tremaining: 27m 21s\n",
      "349:\tlearn: 3.6609415\ttest: 3.7796103\tbest: 3.7794237 (324)\ttotal: 14m 42s\tremaining: 27m 18s\n",
      "350:\tlearn: 3.6607006\ttest: 3.7796084\tbest: 3.7794237 (324)\ttotal: 14m 44s\tremaining: 27m 15s\n",
      "351:\tlearn: 3.6600827\ttest: 3.7796764\tbest: 3.7794237 (324)\ttotal: 14m 47s\tremaining: 27m 12s\n",
      "352:\tlearn: 3.6596958\ttest: 3.7796526\tbest: 3.7794237 (324)\ttotal: 14m 49s\tremaining: 27m 10s\n",
      "353:\tlearn: 3.6593317\ttest: 3.7796719\tbest: 3.7794237 (324)\ttotal: 14m 51s\tremaining: 27m 7s\n",
      "354:\tlearn: 3.6588975\ttest: 3.7796779\tbest: 3.7794237 (324)\ttotal: 14m 54s\tremaining: 27m 5s\n",
      "355:\tlearn: 3.6583627\ttest: 3.7797050\tbest: 3.7794237 (324)\ttotal: 14m 56s\tremaining: 27m 2s\n",
      "356:\tlearn: 3.6578149\ttest: 3.7797137\tbest: 3.7794237 (324)\ttotal: 14m 59s\tremaining: 26m 59s\n",
      "357:\tlearn: 3.6574686\ttest: 3.7797093\tbest: 3.7794237 (324)\ttotal: 15m 1s\tremaining: 26m 57s\n",
      "358:\tlearn: 3.6570780\ttest: 3.7797405\tbest: 3.7794237 (324)\ttotal: 15m 4s\tremaining: 26m 54s\n",
      "359:\tlearn: 3.6568097\ttest: 3.7797220\tbest: 3.7794237 (324)\ttotal: 15m 6s\tremaining: 26m 51s\n",
      "360:\tlearn: 3.6566823\ttest: 3.7797417\tbest: 3.7794237 (324)\ttotal: 15m 8s\tremaining: 26m 48s\n",
      "361:\tlearn: 3.6562525\ttest: 3.7797284\tbest: 3.7794237 (324)\ttotal: 15m 11s\tremaining: 26m 46s\n",
      "362:\tlearn: 3.6560572\ttest: 3.7797366\tbest: 3.7794237 (324)\ttotal: 15m 13s\tremaining: 26m 43s\n",
      "363:\tlearn: 3.6557626\ttest: 3.7797089\tbest: 3.7794237 (324)\ttotal: 15m 16s\tremaining: 26m 40s\n",
      "364:\tlearn: 3.6556507\ttest: 3.7797146\tbest: 3.7794237 (324)\ttotal: 15m 18s\tremaining: 26m 38s\n",
      "365:\tlearn: 3.6554586\ttest: 3.7797650\tbest: 3.7794237 (324)\ttotal: 15m 21s\tremaining: 26m 35s\n",
      "366:\tlearn: 3.6551234\ttest: 3.7797686\tbest: 3.7794237 (324)\ttotal: 15m 23s\tremaining: 26m 32s\n",
      "367:\tlearn: 3.6547563\ttest: 3.7797625\tbest: 3.7794237 (324)\ttotal: 15m 25s\tremaining: 26m 30s\n",
      "368:\tlearn: 3.6545781\ttest: 3.7797723\tbest: 3.7794237 (324)\ttotal: 15m 28s\tremaining: 26m 27s\n",
      "369:\tlearn: 3.6543853\ttest: 3.7797744\tbest: 3.7794237 (324)\ttotal: 15m 30s\tremaining: 26m 24s\n",
      "370:\tlearn: 3.6537307\ttest: 3.7797713\tbest: 3.7794237 (324)\ttotal: 15m 33s\tremaining: 26m 22s\n",
      "371:\tlearn: 3.6533820\ttest: 3.7797835\tbest: 3.7794237 (324)\ttotal: 15m 35s\tremaining: 26m 19s\n",
      "372:\tlearn: 3.6529080\ttest: 3.7797559\tbest: 3.7794237 (324)\ttotal: 15m 38s\tremaining: 26m 16s\n",
      "373:\tlearn: 3.6527738\ttest: 3.7797537\tbest: 3.7794237 (324)\ttotal: 15m 40s\tremaining: 26m 14s\n",
      "374:\tlearn: 3.6525861\ttest: 3.7797421\tbest: 3.7794237 (324)\ttotal: 15m 42s\tremaining: 26m 11s\n",
      "375:\tlearn: 3.6522024\ttest: 3.7797130\tbest: 3.7794237 (324)\ttotal: 15m 45s\tremaining: 26m 8s\n",
      "376:\tlearn: 3.6520387\ttest: 3.7797241\tbest: 3.7794237 (324)\ttotal: 15m 47s\tremaining: 26m 6s\n",
      "377:\tlearn: 3.6517065\ttest: 3.7797111\tbest: 3.7794237 (324)\ttotal: 15m 50s\tremaining: 26m 3s\n",
      "378:\tlearn: 3.6511405\ttest: 3.7797527\tbest: 3.7794237 (324)\ttotal: 15m 52s\tremaining: 26m\n",
      "379:\tlearn: 3.6506683\ttest: 3.7797505\tbest: 3.7794237 (324)\ttotal: 15m 55s\tremaining: 25m 58s\n",
      "380:\tlearn: 3.6503700\ttest: 3.7797743\tbest: 3.7794237 (324)\ttotal: 15m 57s\tremaining: 25m 55s\n",
      "381:\tlearn: 3.6498660\ttest: 3.7798163\tbest: 3.7794237 (324)\ttotal: 15m 59s\tremaining: 25m 53s\n",
      "382:\tlearn: 3.6495831\ttest: 3.7798154\tbest: 3.7794237 (324)\ttotal: 16m 2s\tremaining: 25m 50s\n",
      "383:\tlearn: 3.6493346\ttest: 3.7798185\tbest: 3.7794237 (324)\ttotal: 16m 4s\tremaining: 25m 47s\n",
      "384:\tlearn: 3.6490600\ttest: 3.7798533\tbest: 3.7794237 (324)\ttotal: 16m 7s\tremaining: 25m 45s\n",
      "385:\tlearn: 3.6488087\ttest: 3.7799329\tbest: 3.7794237 (324)\ttotal: 16m 9s\tremaining: 25m 42s\n",
      "386:\tlearn: 3.6486639\ttest: 3.7799271\tbest: 3.7794237 (324)\ttotal: 16m 12s\tremaining: 25m 39s\n",
      "387:\tlearn: 3.6485292\ttest: 3.7799342\tbest: 3.7794237 (324)\ttotal: 16m 14s\tremaining: 25m 37s\n",
      "388:\tlearn: 3.6482007\ttest: 3.7799638\tbest: 3.7794237 (324)\ttotal: 16m 16s\tremaining: 25m 34s\n",
      "389:\tlearn: 3.6479630\ttest: 3.7799503\tbest: 3.7794237 (324)\ttotal: 16m 19s\tremaining: 25m 31s\n",
      "390:\tlearn: 3.6476551\ttest: 3.7799491\tbest: 3.7794237 (324)\ttotal: 16m 21s\tremaining: 25m 29s\n",
      "391:\tlearn: 3.6474433\ttest: 3.7799719\tbest: 3.7794237 (324)\ttotal: 16m 24s\tremaining: 25m 26s\n",
      "392:\tlearn: 3.6471298\ttest: 3.7799503\tbest: 3.7794237 (324)\ttotal: 16m 26s\tremaining: 25m 24s\n",
      "393:\tlearn: 3.6468050\ttest: 3.7799280\tbest: 3.7794237 (324)\ttotal: 16m 29s\tremaining: 25m 21s\n",
      "394:\tlearn: 3.6463314\ttest: 3.7799129\tbest: 3.7794237 (324)\ttotal: 16m 31s\tremaining: 25m 18s\n",
      "395:\tlearn: 3.6461282\ttest: 3.7798854\tbest: 3.7794237 (324)\ttotal: 16m 34s\tremaining: 25m 16s\n",
      "396:\tlearn: 3.6458109\ttest: 3.7799226\tbest: 3.7794237 (324)\ttotal: 16m 36s\tremaining: 25m 13s\n",
      "397:\tlearn: 3.6454615\ttest: 3.7799385\tbest: 3.7794237 (324)\ttotal: 16m 38s\tremaining: 25m 10s\n",
      "398:\tlearn: 3.6452667\ttest: 3.7799454\tbest: 3.7794237 (324)\ttotal: 16m 41s\tremaining: 25m 8s\n",
      "399:\tlearn: 3.6450607\ttest: 3.7799518\tbest: 3.7794237 (324)\ttotal: 16m 44s\tremaining: 25m 6s\n",
      "400:\tlearn: 3.6446390\ttest: 3.7799974\tbest: 3.7794237 (324)\ttotal: 16m 46s\tremaining: 25m 3s\n",
      "401:\tlearn: 3.6443925\ttest: 3.7800172\tbest: 3.7794237 (324)\ttotal: 16m 48s\tremaining: 25m\n",
      "402:\tlearn: 3.6441241\ttest: 3.7800312\tbest: 3.7794237 (324)\ttotal: 16m 51s\tremaining: 24m 58s\n",
      "403:\tlearn: 3.6438492\ttest: 3.7800404\tbest: 3.7794237 (324)\ttotal: 16m 53s\tremaining: 24m 55s\n",
      "404:\tlearn: 3.6435421\ttest: 3.7799987\tbest: 3.7794237 (324)\ttotal: 16m 56s\tremaining: 24m 53s\n",
      "405:\tlearn: 3.6430904\ttest: 3.7799470\tbest: 3.7794237 (324)\ttotal: 16m 58s\tremaining: 24m 50s\n",
      "406:\tlearn: 3.6426290\ttest: 3.7799529\tbest: 3.7794237 (324)\ttotal: 17m 1s\tremaining: 24m 48s\n",
      "407:\tlearn: 3.6422940\ttest: 3.7799666\tbest: 3.7794237 (324)\ttotal: 17m 3s\tremaining: 24m 45s\n",
      "408:\tlearn: 3.6419034\ttest: 3.7799519\tbest: 3.7794237 (324)\ttotal: 17m 6s\tremaining: 24m 43s\n",
      "409:\tlearn: 3.6415974\ttest: 3.7799785\tbest: 3.7794237 (324)\ttotal: 17m 10s\tremaining: 24m 42s\n",
      "410:\tlearn: 3.6412420\ttest: 3.7800136\tbest: 3.7794237 (324)\ttotal: 17m 13s\tremaining: 24m 40s\n",
      "411:\tlearn: 3.6409492\ttest: 3.7800167\tbest: 3.7794237 (324)\ttotal: 17m 16s\tremaining: 24m 39s\n",
      "412:\tlearn: 3.6405797\ttest: 3.7800435\tbest: 3.7794237 (324)\ttotal: 17m 19s\tremaining: 24m 36s\n",
      "413:\tlearn: 3.6403015\ttest: 3.7800465\tbest: 3.7794237 (324)\ttotal: 17m 21s\tremaining: 24m 34s\n",
      "414:\tlearn: 3.6398129\ttest: 3.7800378\tbest: 3.7794237 (324)\ttotal: 17m 24s\tremaining: 24m 31s\n",
      "415:\tlearn: 3.6394763\ttest: 3.7800503\tbest: 3.7794237 (324)\ttotal: 17m 26s\tremaining: 24m 29s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416:\tlearn: 3.6392075\ttest: 3.7800721\tbest: 3.7794237 (324)\ttotal: 17m 28s\tremaining: 24m 26s\n",
      "417:\tlearn: 3.6387486\ttest: 3.7800291\tbest: 3.7794237 (324)\ttotal: 17m 31s\tremaining: 24m 23s\n",
      "418:\tlearn: 3.6384956\ttest: 3.7800552\tbest: 3.7794237 (324)\ttotal: 17m 33s\tremaining: 24m 21s\n",
      "419:\tlearn: 3.6382459\ttest: 3.7800920\tbest: 3.7794237 (324)\ttotal: 17m 36s\tremaining: 24m 18s\n",
      "420:\tlearn: 3.6379367\ttest: 3.7801297\tbest: 3.7794237 (324)\ttotal: 17m 38s\tremaining: 24m 16s\n",
      "421:\tlearn: 3.6373564\ttest: 3.7801155\tbest: 3.7794237 (324)\ttotal: 17m 41s\tremaining: 24m 13s\n",
      "422:\tlearn: 3.6370280\ttest: 3.7801023\tbest: 3.7794237 (324)\ttotal: 17m 43s\tremaining: 24m 10s\n",
      "423:\tlearn: 3.6365878\ttest: 3.7801363\tbest: 3.7794237 (324)\ttotal: 17m 46s\tremaining: 24m 8s\n",
      "424:\tlearn: 3.6362048\ttest: 3.7801813\tbest: 3.7794237 (324)\ttotal: 17m 48s\tremaining: 24m 5s\n",
      "425:\tlearn: 3.6357914\ttest: 3.7802268\tbest: 3.7794237 (324)\ttotal: 17m 50s\tremaining: 24m 3s\n",
      "426:\tlearn: 3.6354792\ttest: 3.7802305\tbest: 3.7794237 (324)\ttotal: 17m 53s\tremaining: 24m\n",
      "427:\tlearn: 3.6350199\ttest: 3.7802501\tbest: 3.7794237 (324)\ttotal: 17m 55s\tremaining: 23m 57s\n",
      "428:\tlearn: 3.6346501\ttest: 3.7802061\tbest: 3.7794237 (324)\ttotal: 17m 58s\tremaining: 23m 55s\n",
      "429:\tlearn: 3.6342879\ttest: 3.7802150\tbest: 3.7794237 (324)\ttotal: 18m\tremaining: 23m 52s\n",
      "430:\tlearn: 3.6340732\ttest: 3.7802429\tbest: 3.7794237 (324)\ttotal: 18m 3s\tremaining: 23m 49s\n",
      "431:\tlearn: 3.6338843\ttest: 3.7802464\tbest: 3.7794237 (324)\ttotal: 18m 5s\tremaining: 23m 47s\n",
      "432:\tlearn: 3.6331867\ttest: 3.7802470\tbest: 3.7794237 (324)\ttotal: 18m 8s\tremaining: 23m 44s\n",
      "433:\tlearn: 3.6327843\ttest: 3.7803132\tbest: 3.7794237 (324)\ttotal: 18m 10s\tremaining: 23m 42s\n",
      "434:\tlearn: 3.6324555\ttest: 3.7802903\tbest: 3.7794237 (324)\ttotal: 18m 12s\tremaining: 23m 39s\n",
      "435:\tlearn: 3.6318850\ttest: 3.7803592\tbest: 3.7794237 (324)\ttotal: 18m 15s\tremaining: 23m 36s\n",
      "436:\tlearn: 3.6315153\ttest: 3.7803633\tbest: 3.7794237 (324)\ttotal: 18m 17s\tremaining: 23m 34s\n",
      "437:\tlearn: 3.6311416\ttest: 3.7804201\tbest: 3.7794237 (324)\ttotal: 18m 20s\tremaining: 23m 31s\n",
      "438:\tlearn: 3.6307638\ttest: 3.7804248\tbest: 3.7794237 (324)\ttotal: 18m 22s\tremaining: 23m 29s\n",
      "439:\tlearn: 3.6305423\ttest: 3.7804242\tbest: 3.7794237 (324)\ttotal: 18m 25s\tremaining: 23m 26s\n",
      "440:\tlearn: 3.6302412\ttest: 3.7804021\tbest: 3.7794237 (324)\ttotal: 18m 27s\tremaining: 23m 23s\n",
      "441:\tlearn: 3.6298664\ttest: 3.7804345\tbest: 3.7794237 (324)\ttotal: 18m 30s\tremaining: 23m 21s\n",
      "442:\tlearn: 3.6295948\ttest: 3.7804636\tbest: 3.7794237 (324)\ttotal: 18m 32s\tremaining: 23m 18s\n",
      "443:\tlearn: 3.6290836\ttest: 3.7805001\tbest: 3.7794237 (324)\ttotal: 18m 34s\tremaining: 23m 16s\n",
      "444:\tlearn: 3.6287876\ttest: 3.7804388\tbest: 3.7794237 (324)\ttotal: 18m 37s\tremaining: 23m 13s\n",
      "445:\tlearn: 3.6284684\ttest: 3.7804401\tbest: 3.7794237 (324)\ttotal: 18m 39s\tremaining: 23m 11s\n",
      "446:\tlearn: 3.6283312\ttest: 3.7804478\tbest: 3.7794237 (324)\ttotal: 18m 42s\tremaining: 23m 8s\n",
      "447:\tlearn: 3.6280920\ttest: 3.7804979\tbest: 3.7794237 (324)\ttotal: 18m 44s\tremaining: 23m 5s\n",
      "448:\tlearn: 3.6277113\ttest: 3.7805052\tbest: 3.7794237 (324)\ttotal: 18m 47s\tremaining: 23m 3s\n",
      "449:\tlearn: 3.6273067\ttest: 3.7805357\tbest: 3.7794237 (324)\ttotal: 18m 49s\tremaining: 23m\n",
      "450:\tlearn: 3.6271113\ttest: 3.7805255\tbest: 3.7794237 (324)\ttotal: 18m 52s\tremaining: 22m 58s\n",
      "451:\tlearn: 3.6269294\ttest: 3.7804960\tbest: 3.7794237 (324)\ttotal: 18m 54s\tremaining: 22m 55s\n",
      "452:\tlearn: 3.6265731\ttest: 3.7805273\tbest: 3.7794237 (324)\ttotal: 18m 57s\tremaining: 22m 52s\n",
      "453:\tlearn: 3.6262090\ttest: 3.7805195\tbest: 3.7794237 (324)\ttotal: 18m 59s\tremaining: 22m 50s\n",
      "454:\tlearn: 3.6259267\ttest: 3.7805181\tbest: 3.7794237 (324)\ttotal: 19m 1s\tremaining: 22m 47s\n",
      "455:\tlearn: 3.6256844\ttest: 3.7805210\tbest: 3.7794237 (324)\ttotal: 19m 4s\tremaining: 22m 45s\n",
      "456:\tlearn: 3.6254402\ttest: 3.7805201\tbest: 3.7794237 (324)\ttotal: 19m 6s\tremaining: 22m 42s\n",
      "457:\tlearn: 3.6251713\ttest: 3.7805335\tbest: 3.7794237 (324)\ttotal: 19m 9s\tremaining: 22m 40s\n",
      "458:\tlearn: 3.6247908\ttest: 3.7805663\tbest: 3.7794237 (324)\ttotal: 19m 11s\tremaining: 22m 37s\n",
      "459:\tlearn: 3.6246087\ttest: 3.7805621\tbest: 3.7794237 (324)\ttotal: 19m 14s\tremaining: 22m 34s\n",
      "460:\tlearn: 3.6241248\ttest: 3.7805391\tbest: 3.7794237 (324)\ttotal: 19m 16s\tremaining: 22m 32s\n",
      "461:\tlearn: 3.6237141\ttest: 3.7805641\tbest: 3.7794237 (324)\ttotal: 19m 19s\tremaining: 22m 29s\n",
      "462:\tlearn: 3.6232556\ttest: 3.7806046\tbest: 3.7794237 (324)\ttotal: 19m 21s\tremaining: 22m 27s\n",
      "463:\tlearn: 3.6231079\ttest: 3.7806217\tbest: 3.7794237 (324)\ttotal: 19m 24s\tremaining: 22m 24s\n",
      "464:\tlearn: 3.6227747\ttest: 3.7805950\tbest: 3.7794237 (324)\ttotal: 19m 26s\tremaining: 22m 22s\n",
      "465:\tlearn: 3.6225160\ttest: 3.7806069\tbest: 3.7794237 (324)\ttotal: 19m 28s\tremaining: 22m 19s\n",
      "466:\tlearn: 3.6220150\ttest: 3.7806296\tbest: 3.7794237 (324)\ttotal: 19m 31s\tremaining: 22m 16s\n",
      "467:\tlearn: 3.6216444\ttest: 3.7806638\tbest: 3.7794237 (324)\ttotal: 19m 33s\tremaining: 22m 14s\n",
      "468:\tlearn: 3.6211405\ttest: 3.7806409\tbest: 3.7794237 (324)\ttotal: 19m 36s\tremaining: 22m 11s\n",
      "469:\tlearn: 3.6209657\ttest: 3.7806527\tbest: 3.7794237 (324)\ttotal: 19m 38s\tremaining: 22m 9s\n",
      "470:\tlearn: 3.6204204\ttest: 3.7807441\tbest: 3.7794237 (324)\ttotal: 19m 41s\tremaining: 22m 6s\n",
      "471:\tlearn: 3.6198955\ttest: 3.7807804\tbest: 3.7794237 (324)\ttotal: 19m 43s\tremaining: 22m 4s\n",
      "472:\tlearn: 3.6193783\ttest: 3.7808231\tbest: 3.7794237 (324)\ttotal: 19m 46s\tremaining: 22m 1s\n",
      "473:\tlearn: 3.6191820\ttest: 3.7808248\tbest: 3.7794237 (324)\ttotal: 19m 48s\tremaining: 21m 59s\n",
      "474:\tlearn: 3.6188740\ttest: 3.7808172\tbest: 3.7794237 (324)\ttotal: 19m 51s\tremaining: 21m 56s\n",
      "475:\tlearn: 3.6186135\ttest: 3.7808269\tbest: 3.7794237 (324)\ttotal: 19m 53s\tremaining: 21m 53s\n",
      "476:\tlearn: 3.6182620\ttest: 3.7808559\tbest: 3.7794237 (324)\ttotal: 19m 55s\tremaining: 21m 51s\n",
      "477:\tlearn: 3.6180684\ttest: 3.7808562\tbest: 3.7794237 (324)\ttotal: 19m 58s\tremaining: 21m 48s\n",
      "478:\tlearn: 3.6177766\ttest: 3.7808801\tbest: 3.7794237 (324)\ttotal: 20m 1s\tremaining: 21m 46s\n",
      "479:\tlearn: 3.6171411\ttest: 3.7809380\tbest: 3.7794237 (324)\ttotal: 20m 3s\tremaining: 21m 43s\n",
      "480:\tlearn: 3.6169315\ttest: 3.7809163\tbest: 3.7794237 (324)\ttotal: 20m 6s\tremaining: 21m 41s\n",
      "481:\tlearn: 3.6167292\ttest: 3.7808976\tbest: 3.7794237 (324)\ttotal: 20m 8s\tremaining: 21m 39s\n",
      "482:\tlearn: 3.6165652\ttest: 3.7809100\tbest: 3.7794237 (324)\ttotal: 20m 11s\tremaining: 21m 36s\n",
      "483:\tlearn: 3.6160414\ttest: 3.7808955\tbest: 3.7794237 (324)\ttotal: 20m 14s\tremaining: 21m 34s\n",
      "484:\tlearn: 3.6156578\ttest: 3.7808797\tbest: 3.7794237 (324)\ttotal: 20m 16s\tremaining: 21m 31s\n",
      "485:\tlearn: 3.6153596\ttest: 3.7809305\tbest: 3.7794237 (324)\ttotal: 20m 19s\tremaining: 21m 29s\n",
      "486:\tlearn: 3.6151271\ttest: 3.7809284\tbest: 3.7794237 (324)\ttotal: 20m 21s\tremaining: 21m 27s\n",
      "487:\tlearn: 3.6147335\ttest: 3.7809493\tbest: 3.7794237 (324)\ttotal: 20m 24s\tremaining: 21m 24s\n",
      "488:\tlearn: 3.6142969\ttest: 3.7809868\tbest: 3.7794237 (324)\ttotal: 20m 27s\tremaining: 21m 22s\n",
      "489:\tlearn: 3.6139604\ttest: 3.7809737\tbest: 3.7794237 (324)\ttotal: 20m 29s\tremaining: 21m 19s\n",
      "490:\tlearn: 3.6134930\ttest: 3.7809942\tbest: 3.7794237 (324)\ttotal: 20m 32s\tremaining: 21m 17s\n",
      "491:\tlearn: 3.6130485\ttest: 3.7810073\tbest: 3.7794237 (324)\ttotal: 20m 34s\tremaining: 21m 14s\n",
      "492:\tlearn: 3.6126002\ttest: 3.7810288\tbest: 3.7794237 (324)\ttotal: 20m 37s\tremaining: 21m 12s\n",
      "493:\tlearn: 3.6122133\ttest: 3.7810896\tbest: 3.7794237 (324)\ttotal: 20m 39s\tremaining: 21m 9s\n",
      "494:\tlearn: 3.6120967\ttest: 3.7810877\tbest: 3.7794237 (324)\ttotal: 20m 42s\tremaining: 21m 7s\n",
      "495:\tlearn: 3.6119212\ttest: 3.7810795\tbest: 3.7794237 (324)\ttotal: 20m 44s\tremaining: 21m 4s\n",
      "496:\tlearn: 3.6115953\ttest: 3.7810324\tbest: 3.7794237 (324)\ttotal: 20m 47s\tremaining: 21m 2s\n",
      "497:\tlearn: 3.6112500\ttest: 3.7810627\tbest: 3.7794237 (324)\ttotal: 20m 49s\tremaining: 20m 59s\n",
      "498:\tlearn: 3.6108856\ttest: 3.7810993\tbest: 3.7794237 (324)\ttotal: 20m 52s\tremaining: 20m 57s\n",
      "499:\tlearn: 3.6107163\ttest: 3.7810962\tbest: 3.7794237 (324)\ttotal: 20m 54s\tremaining: 20m 54s\n",
      "500:\tlearn: 3.6102654\ttest: 3.7811191\tbest: 3.7794237 (324)\ttotal: 20m 57s\tremaining: 20m 52s\n",
      "501:\tlearn: 3.6099624\ttest: 3.7811141\tbest: 3.7794237 (324)\ttotal: 21m\tremaining: 20m 50s\n",
      "502:\tlearn: 3.6096464\ttest: 3.7811267\tbest: 3.7794237 (324)\ttotal: 21m 3s\tremaining: 20m 48s\n",
      "503:\tlearn: 3.6094542\ttest: 3.7811481\tbest: 3.7794237 (324)\ttotal: 21m 5s\tremaining: 20m 45s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504:\tlearn: 3.6092126\ttest: 3.7811389\tbest: 3.7794237 (324)\ttotal: 21m 8s\tremaining: 20m 43s\n",
      "505:\tlearn: 3.6088237\ttest: 3.7811278\tbest: 3.7794237 (324)\ttotal: 21m 11s\tremaining: 20m 41s\n",
      "506:\tlearn: 3.6084186\ttest: 3.7811161\tbest: 3.7794237 (324)\ttotal: 21m 13s\tremaining: 20m 38s\n",
      "507:\tlearn: 3.6080090\ttest: 3.7811324\tbest: 3.7794237 (324)\ttotal: 21m 16s\tremaining: 20m 36s\n",
      "508:\tlearn: 3.6076505\ttest: 3.7812063\tbest: 3.7794237 (324)\ttotal: 21m 18s\tremaining: 20m 33s\n",
      "509:\tlearn: 3.6072054\ttest: 3.7812487\tbest: 3.7794237 (324)\ttotal: 21m 21s\tremaining: 20m 30s\n",
      "510:\tlearn: 3.6068310\ttest: 3.7812859\tbest: 3.7794237 (324)\ttotal: 21m 23s\tremaining: 20m 28s\n",
      "511:\tlearn: 3.6065106\ttest: 3.7812922\tbest: 3.7794237 (324)\ttotal: 21m 26s\tremaining: 20m 25s\n",
      "512:\tlearn: 3.6063069\ttest: 3.7812712\tbest: 3.7794237 (324)\ttotal: 21m 28s\tremaining: 20m 23s\n",
      "513:\tlearn: 3.6060938\ttest: 3.7812584\tbest: 3.7794237 (324)\ttotal: 21m 31s\tremaining: 20m 20s\n",
      "514:\tlearn: 3.6059152\ttest: 3.7812314\tbest: 3.7794237 (324)\ttotal: 21m 33s\tremaining: 20m 18s\n",
      "515:\tlearn: 3.6055791\ttest: 3.7812702\tbest: 3.7794237 (324)\ttotal: 21m 35s\tremaining: 20m 15s\n",
      "516:\tlearn: 3.6053472\ttest: 3.7812979\tbest: 3.7794237 (324)\ttotal: 21m 38s\tremaining: 20m 12s\n",
      "517:\tlearn: 3.6049606\ttest: 3.7813506\tbest: 3.7794237 (324)\ttotal: 21m 40s\tremaining: 20m 10s\n",
      "518:\tlearn: 3.6045261\ttest: 3.7813498\tbest: 3.7794237 (324)\ttotal: 21m 43s\tremaining: 20m 7s\n",
      "519:\tlearn: 3.6043364\ttest: 3.7813485\tbest: 3.7794237 (324)\ttotal: 21m 45s\tremaining: 20m 5s\n",
      "520:\tlearn: 3.6041382\ttest: 3.7813756\tbest: 3.7794237 (324)\ttotal: 21m 48s\tremaining: 20m 2s\n",
      "521:\tlearn: 3.6038533\ttest: 3.7813941\tbest: 3.7794237 (324)\ttotal: 21m 50s\tremaining: 20m\n",
      "522:\tlearn: 3.6033965\ttest: 3.7814212\tbest: 3.7794237 (324)\ttotal: 21m 53s\tremaining: 19m 57s\n",
      "523:\tlearn: 3.6030925\ttest: 3.7813858\tbest: 3.7794237 (324)\ttotal: 21m 55s\tremaining: 19m 55s\n",
      "524:\tlearn: 3.6028431\ttest: 3.7814017\tbest: 3.7794237 (324)\ttotal: 21m 57s\tremaining: 19m 52s\n",
      "525:\tlearn: 3.6024698\ttest: 3.7814291\tbest: 3.7794237 (324)\ttotal: 22m\tremaining: 19m 49s\n",
      "526:\tlearn: 3.6019773\ttest: 3.7814706\tbest: 3.7794237 (324)\ttotal: 22m 2s\tremaining: 19m 47s\n",
      "527:\tlearn: 3.6015452\ttest: 3.7815160\tbest: 3.7794237 (324)\ttotal: 22m 5s\tremaining: 19m 44s\n",
      "528:\tlearn: 3.6013984\ttest: 3.7815156\tbest: 3.7794237 (324)\ttotal: 22m 7s\tremaining: 19m 42s\n",
      "529:\tlearn: 3.6010843\ttest: 3.7815147\tbest: 3.7794237 (324)\ttotal: 22m 10s\tremaining: 19m 39s\n",
      "530:\tlearn: 3.6008881\ttest: 3.7815324\tbest: 3.7794237 (324)\ttotal: 22m 12s\tremaining: 19m 37s\n",
      "531:\tlearn: 3.6007177\ttest: 3.7815547\tbest: 3.7794237 (324)\ttotal: 22m 15s\tremaining: 19m 34s\n",
      "532:\tlearn: 3.6005225\ttest: 3.7815548\tbest: 3.7794237 (324)\ttotal: 22m 17s\tremaining: 19m 32s\n",
      "533:\tlearn: 3.5999690\ttest: 3.7816171\tbest: 3.7794237 (324)\ttotal: 22m 20s\tremaining: 19m 29s\n",
      "534:\tlearn: 3.5996223\ttest: 3.7816215\tbest: 3.7794237 (324)\ttotal: 22m 22s\tremaining: 19m 27s\n",
      "535:\tlearn: 3.5992233\ttest: 3.7816510\tbest: 3.7794237 (324)\ttotal: 22m 25s\tremaining: 19m 24s\n",
      "536:\tlearn: 3.5989410\ttest: 3.7816473\tbest: 3.7794237 (324)\ttotal: 22m 27s\tremaining: 19m 22s\n",
      "537:\tlearn: 3.5986333\ttest: 3.7816384\tbest: 3.7794237 (324)\ttotal: 22m 30s\tremaining: 19m 19s\n",
      "538:\tlearn: 3.5983612\ttest: 3.7816115\tbest: 3.7794237 (324)\ttotal: 22m 32s\tremaining: 19m 16s\n",
      "539:\tlearn: 3.5981239\ttest: 3.7816427\tbest: 3.7794237 (324)\ttotal: 22m 35s\tremaining: 19m 14s\n",
      "540:\tlearn: 3.5978246\ttest: 3.7816801\tbest: 3.7794237 (324)\ttotal: 22m 37s\tremaining: 19m 11s\n",
      "541:\tlearn: 3.5975812\ttest: 3.7816905\tbest: 3.7794237 (324)\ttotal: 22m 40s\tremaining: 19m 9s\n",
      "542:\tlearn: 3.5973667\ttest: 3.7817001\tbest: 3.7794237 (324)\ttotal: 22m 42s\tremaining: 19m 6s\n",
      "543:\tlearn: 3.5971659\ttest: 3.7817098\tbest: 3.7794237 (324)\ttotal: 22m 45s\tremaining: 19m 4s\n",
      "544:\tlearn: 3.5968135\ttest: 3.7817278\tbest: 3.7794237 (324)\ttotal: 22m 47s\tremaining: 19m 1s\n",
      "545:\tlearn: 3.5962803\ttest: 3.7816976\tbest: 3.7794237 (324)\ttotal: 22m 50s\tremaining: 18m 59s\n",
      "546:\tlearn: 3.5957776\ttest: 3.7817105\tbest: 3.7794237 (324)\ttotal: 22m 52s\tremaining: 18m 56s\n",
      "547:\tlearn: 3.5952708\ttest: 3.7817660\tbest: 3.7794237 (324)\ttotal: 22m 55s\tremaining: 18m 54s\n",
      "548:\tlearn: 3.5948953\ttest: 3.7818069\tbest: 3.7794237 (324)\ttotal: 22m 57s\tremaining: 18m 51s\n",
      "549:\tlearn: 3.5945164\ttest: 3.7818385\tbest: 3.7794237 (324)\ttotal: 23m\tremaining: 18m 49s\n",
      "550:\tlearn: 3.5943251\ttest: 3.7817955\tbest: 3.7794237 (324)\ttotal: 23m 2s\tremaining: 18m 46s\n",
      "551:\tlearn: 3.5940824\ttest: 3.7818045\tbest: 3.7794237 (324)\ttotal: 23m 5s\tremaining: 18m 44s\n",
      "552:\tlearn: 3.5937871\ttest: 3.7817817\tbest: 3.7794237 (324)\ttotal: 23m 7s\tremaining: 18m 41s\n",
      "553:\tlearn: 3.5934158\ttest: 3.7818306\tbest: 3.7794237 (324)\ttotal: 23m 10s\tremaining: 18m 39s\n",
      "554:\tlearn: 3.5928816\ttest: 3.7818278\tbest: 3.7794237 (324)\ttotal: 23m 13s\tremaining: 18m 36s\n",
      "555:\tlearn: 3.5924383\ttest: 3.7818528\tbest: 3.7794237 (324)\ttotal: 23m 15s\tremaining: 18m 34s\n",
      "556:\tlearn: 3.5919583\ttest: 3.7818843\tbest: 3.7794237 (324)\ttotal: 23m 18s\tremaining: 18m 32s\n",
      "557:\tlearn: 3.5915892\ttest: 3.7819060\tbest: 3.7794237 (324)\ttotal: 23m 20s\tremaining: 18m 29s\n",
      "558:\tlearn: 3.5912745\ttest: 3.7819199\tbest: 3.7794237 (324)\ttotal: 23m 23s\tremaining: 18m 27s\n",
      "559:\tlearn: 3.5909746\ttest: 3.7819255\tbest: 3.7794237 (324)\ttotal: 23m 25s\tremaining: 18m 24s\n",
      "560:\tlearn: 3.5906695\ttest: 3.7819395\tbest: 3.7794237 (324)\ttotal: 23m 28s\tremaining: 18m 22s\n",
      "561:\tlearn: 3.5903797\ttest: 3.7820137\tbest: 3.7794237 (324)\ttotal: 23m 31s\tremaining: 18m 19s\n",
      "562:\tlearn: 3.5900557\ttest: 3.7820608\tbest: 3.7794237 (324)\ttotal: 23m 33s\tremaining: 18m 17s\n",
      "563:\tlearn: 3.5896970\ttest: 3.7820976\tbest: 3.7794237 (324)\ttotal: 23m 36s\tremaining: 18m 14s\n",
      "564:\tlearn: 3.5892240\ttest: 3.7821157\tbest: 3.7794237 (324)\ttotal: 23m 38s\tremaining: 18m 12s\n",
      "565:\tlearn: 3.5888905\ttest: 3.7821393\tbest: 3.7794237 (324)\ttotal: 23m 41s\tremaining: 18m 9s\n",
      "566:\tlearn: 3.5886292\ttest: 3.7821596\tbest: 3.7794237 (324)\ttotal: 23m 44s\tremaining: 18m 7s\n",
      "567:\tlearn: 3.5883945\ttest: 3.7821661\tbest: 3.7794237 (324)\ttotal: 23m 46s\tremaining: 18m 5s\n",
      "568:\tlearn: 3.5878251\ttest: 3.7821200\tbest: 3.7794237 (324)\ttotal: 23m 49s\tremaining: 18m 2s\n",
      "569:\tlearn: 3.5875597\ttest: 3.7821361\tbest: 3.7794237 (324)\ttotal: 23m 52s\tremaining: 18m\n",
      "570:\tlearn: 3.5873909\ttest: 3.7821351\tbest: 3.7794237 (324)\ttotal: 23m 54s\tremaining: 17m 57s\n",
      "571:\tlearn: 3.5869641\ttest: 3.7821680\tbest: 3.7794237 (324)\ttotal: 23m 57s\tremaining: 17m 55s\n",
      "572:\tlearn: 3.5866288\ttest: 3.7821893\tbest: 3.7794237 (324)\ttotal: 23m 59s\tremaining: 17m 52s\n",
      "573:\tlearn: 3.5861764\ttest: 3.7822567\tbest: 3.7794237 (324)\ttotal: 24m 2s\tremaining: 17m 50s\n",
      "574:\tlearn: 3.5857941\ttest: 3.7822491\tbest: 3.7794237 (324)\ttotal: 24m 4s\tremaining: 17m 47s\n",
      "575:\tlearn: 3.5855720\ttest: 3.7822560\tbest: 3.7794237 (324)\ttotal: 24m 7s\tremaining: 17m 45s\n",
      "576:\tlearn: 3.5849937\ttest: 3.7822679\tbest: 3.7794237 (324)\ttotal: 24m 9s\tremaining: 17m 42s\n",
      "577:\tlearn: 3.5847307\ttest: 3.7822973\tbest: 3.7794237 (324)\ttotal: 24m 11s\tremaining: 17m 40s\n",
      "578:\tlearn: 3.5845055\ttest: 3.7823033\tbest: 3.7794237 (324)\ttotal: 24m 14s\tremaining: 17m 37s\n",
      "579:\tlearn: 3.5842703\ttest: 3.7823251\tbest: 3.7794237 (324)\ttotal: 24m 16s\tremaining: 17m 34s\n",
      "580:\tlearn: 3.5838151\ttest: 3.7823746\tbest: 3.7794237 (324)\ttotal: 24m 19s\tremaining: 17m 32s\n",
      "581:\tlearn: 3.5836379\ttest: 3.7823806\tbest: 3.7794237 (324)\ttotal: 24m 21s\tremaining: 17m 29s\n",
      "582:\tlearn: 3.5832722\ttest: 3.7823776\tbest: 3.7794237 (324)\ttotal: 24m 24s\tremaining: 17m 27s\n",
      "583:\tlearn: 3.5830044\ttest: 3.7823946\tbest: 3.7794237 (324)\ttotal: 24m 27s\tremaining: 17m 24s\n",
      "584:\tlearn: 3.5827035\ttest: 3.7824215\tbest: 3.7794237 (324)\ttotal: 24m 29s\tremaining: 17m 22s\n",
      "585:\tlearn: 3.5822708\ttest: 3.7824080\tbest: 3.7794237 (324)\ttotal: 24m 32s\tremaining: 17m 20s\n",
      "586:\tlearn: 3.5820332\ttest: 3.7823983\tbest: 3.7794237 (324)\ttotal: 24m 35s\tremaining: 17m 18s\n",
      "587:\tlearn: 3.5816676\ttest: 3.7824128\tbest: 3.7794237 (324)\ttotal: 24m 39s\tremaining: 17m 16s\n",
      "588:\tlearn: 3.5813989\ttest: 3.7824436\tbest: 3.7794237 (324)\ttotal: 24m 41s\tremaining: 17m 14s\n",
      "589:\tlearn: 3.5810413\ttest: 3.7825332\tbest: 3.7794237 (324)\ttotal: 24m 44s\tremaining: 17m 11s\n",
      "590:\tlearn: 3.5808200\ttest: 3.7825745\tbest: 3.7794237 (324)\ttotal: 24m 47s\tremaining: 17m 9s\n",
      "591:\tlearn: 3.5803837\ttest: 3.7825934\tbest: 3.7794237 (324)\ttotal: 24m 50s\tremaining: 17m 7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592:\tlearn: 3.5800141\ttest: 3.7826313\tbest: 3.7794237 (324)\ttotal: 24m 53s\tremaining: 17m 4s\n",
      "593:\tlearn: 3.5796501\ttest: 3.7826628\tbest: 3.7794237 (324)\ttotal: 24m 57s\tremaining: 17m 3s\n",
      "594:\tlearn: 3.5793311\ttest: 3.7826968\tbest: 3.7794237 (324)\ttotal: 24m 59s\tremaining: 17m\n",
      "595:\tlearn: 3.5790540\ttest: 3.7827169\tbest: 3.7794237 (324)\ttotal: 25m 2s\tremaining: 16m 58s\n",
      "596:\tlearn: 3.5787754\ttest: 3.7827175\tbest: 3.7794237 (324)\ttotal: 25m 6s\tremaining: 16m 56s\n",
      "597:\tlearn: 3.5783724\ttest: 3.7827155\tbest: 3.7794237 (324)\ttotal: 25m 9s\tremaining: 16m 54s\n",
      "598:\tlearn: 3.5780834\ttest: 3.7827522\tbest: 3.7794237 (324)\ttotal: 25m 12s\tremaining: 16m 52s\n",
      "599:\tlearn: 3.5777213\ttest: 3.7827738\tbest: 3.7794237 (324)\ttotal: 25m 14s\tremaining: 16m 49s\n",
      "600:\tlearn: 3.5775020\ttest: 3.7827609\tbest: 3.7794237 (324)\ttotal: 25m 17s\tremaining: 16m 47s\n",
      "601:\tlearn: 3.5772685\ttest: 3.7828029\tbest: 3.7794237 (324)\ttotal: 25m 19s\tremaining: 16m 44s\n",
      "602:\tlearn: 3.5769548\ttest: 3.7827970\tbest: 3.7794237 (324)\ttotal: 25m 22s\tremaining: 16m 42s\n",
      "603:\tlearn: 3.5767632\ttest: 3.7828157\tbest: 3.7794237 (324)\ttotal: 25m 25s\tremaining: 16m 40s\n",
      "604:\tlearn: 3.5764847\ttest: 3.7828196\tbest: 3.7794237 (324)\ttotal: 25m 28s\tremaining: 16m 38s\n",
      "605:\tlearn: 3.5763494\ttest: 3.7828148\tbest: 3.7794237 (324)\ttotal: 25m 31s\tremaining: 16m 35s\n",
      "606:\tlearn: 3.5760523\ttest: 3.7827961\tbest: 3.7794237 (324)\ttotal: 25m 33s\tremaining: 16m 32s\n",
      "607:\tlearn: 3.5757646\ttest: 3.7828262\tbest: 3.7794237 (324)\ttotal: 25m 36s\tremaining: 16m 30s\n",
      "608:\tlearn: 3.5754983\ttest: 3.7828563\tbest: 3.7794237 (324)\ttotal: 25m 38s\tremaining: 16m 27s\n",
      "609:\tlearn: 3.5751254\ttest: 3.7828687\tbest: 3.7794237 (324)\ttotal: 25m 41s\tremaining: 16m 25s\n",
      "610:\tlearn: 3.5748422\ttest: 3.7828893\tbest: 3.7794237 (324)\ttotal: 25m 44s\tremaining: 16m 23s\n",
      "611:\tlearn: 3.5744196\ttest: 3.7829061\tbest: 3.7794237 (324)\ttotal: 25m 47s\tremaining: 16m 20s\n",
      "612:\tlearn: 3.5741400\ttest: 3.7829237\tbest: 3.7794237 (324)\ttotal: 25m 49s\tremaining: 16m 18s\n",
      "613:\tlearn: 3.5739609\ttest: 3.7829276\tbest: 3.7794237 (324)\ttotal: 25m 52s\tremaining: 16m 15s\n",
      "614:\tlearn: 3.5736469\ttest: 3.7829422\tbest: 3.7794237 (324)\ttotal: 25m 54s\tremaining: 16m 13s\n",
      "615:\tlearn: 3.5734780\ttest: 3.7829233\tbest: 3.7794237 (324)\ttotal: 25m 57s\tremaining: 16m 10s\n",
      "616:\tlearn: 3.5732818\ttest: 3.7829031\tbest: 3.7794237 (324)\ttotal: 25m 59s\tremaining: 16m 8s\n",
      "617:\tlearn: 3.5729515\ttest: 3.7829380\tbest: 3.7794237 (324)\ttotal: 26m 2s\tremaining: 16m 5s\n",
      "618:\tlearn: 3.5727391\ttest: 3.7829699\tbest: 3.7794237 (324)\ttotal: 26m 4s\tremaining: 16m 3s\n",
      "619:\tlearn: 3.5724091\ttest: 3.7829502\tbest: 3.7794237 (324)\ttotal: 26m 7s\tremaining: 16m\n",
      "620:\tlearn: 3.5721342\ttest: 3.7829649\tbest: 3.7794237 (324)\ttotal: 26m 9s\tremaining: 15m 58s\n",
      "621:\tlearn: 3.5719462\ttest: 3.7829802\tbest: 3.7794237 (324)\ttotal: 26m 12s\tremaining: 15m 55s\n",
      "622:\tlearn: 3.5717063\ttest: 3.7829716\tbest: 3.7794237 (324)\ttotal: 26m 14s\tremaining: 15m 52s\n",
      "623:\tlearn: 3.5714081\ttest: 3.7829682\tbest: 3.7794237 (324)\ttotal: 26m 17s\tremaining: 15m 50s\n",
      "624:\tlearn: 3.5712043\ttest: 3.7829869\tbest: 3.7794237 (324)\ttotal: 26m 19s\tremaining: 15m 47s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 3.779423675\n",
      "bestIteration = 324\n",
      "\n",
      "Shrink model to first 325 iterations.\n",
      "0:\tlearn: 3.8668187\ttest: 3.8754143\tbest: 3.8754143 (0)\ttotal: 2.4s\tremaining: 39m 55s\n",
      "1:\tlearn: 3.8649099\ttest: 3.8738269\tbest: 3.8738269 (1)\ttotal: 4.87s\tremaining: 40m 27s\n",
      "2:\tlearn: 3.8628976\ttest: 3.8723012\tbest: 3.8723012 (2)\ttotal: 7.34s\tremaining: 40m 40s\n",
      "3:\tlearn: 3.8610665\ttest: 3.8707380\tbest: 3.8707380 (3)\ttotal: 9.76s\tremaining: 40m 29s\n",
      "4:\tlearn: 3.8592601\ttest: 3.8692018\tbest: 3.8692018 (4)\ttotal: 12.2s\tremaining: 40m 20s\n",
      "5:\tlearn: 3.8575557\ttest: 3.8676405\tbest: 3.8676405 (5)\ttotal: 14.6s\tremaining: 40m 15s\n",
      "6:\tlearn: 3.8557995\ttest: 3.8662337\tbest: 3.8662337 (6)\ttotal: 17s\tremaining: 40m 11s\n",
      "7:\tlearn: 3.8540647\ttest: 3.8648409\tbest: 3.8648409 (7)\ttotal: 19.4s\tremaining: 40m 5s\n",
      "8:\tlearn: 3.8524181\ttest: 3.8634504\tbest: 3.8634504 (8)\ttotal: 21.8s\tremaining: 40m\n",
      "9:\tlearn: 3.8507777\ttest: 3.8620921\tbest: 3.8620921 (9)\ttotal: 24.2s\tremaining: 39m 55s\n",
      "10:\tlearn: 3.8490270\ttest: 3.8607035\tbest: 3.8607035 (10)\ttotal: 26.6s\tremaining: 39m 55s\n",
      "11:\tlearn: 3.8473493\ttest: 3.8594140\tbest: 3.8594140 (11)\ttotal: 29.1s\tremaining: 39m 52s\n",
      "12:\tlearn: 3.8457842\ttest: 3.8581254\tbest: 3.8581254 (12)\ttotal: 31.5s\tremaining: 39m 50s\n",
      "13:\tlearn: 3.8441239\ttest: 3.8568197\tbest: 3.8568197 (13)\ttotal: 33.9s\tremaining: 39m 45s\n",
      "14:\tlearn: 3.8426307\ttest: 3.8555446\tbest: 3.8555446 (14)\ttotal: 36.3s\tremaining: 39m 43s\n",
      "15:\tlearn: 3.8411564\ttest: 3.8543272\tbest: 3.8543272 (15)\ttotal: 38.7s\tremaining: 39m 39s\n",
      "16:\tlearn: 3.8395931\ttest: 3.8530792\tbest: 3.8530792 (16)\ttotal: 41.1s\tremaining: 39m 36s\n",
      "17:\tlearn: 3.8382118\ttest: 3.8518258\tbest: 3.8518258 (17)\ttotal: 43.5s\tremaining: 39m 32s\n",
      "18:\tlearn: 3.8366881\ttest: 3.8506165\tbest: 3.8506165 (18)\ttotal: 45.9s\tremaining: 39m 29s\n",
      "19:\tlearn: 3.8349864\ttest: 3.8494850\tbest: 3.8494850 (19)\ttotal: 48.3s\tremaining: 39m 25s\n",
      "20:\tlearn: 3.8336698\ttest: 3.8483400\tbest: 3.8483400 (20)\ttotal: 50.7s\tremaining: 39m 21s\n",
      "21:\tlearn: 3.8323004\ttest: 3.8472403\tbest: 3.8472403 (21)\ttotal: 53.1s\tremaining: 39m 18s\n",
      "22:\tlearn: 3.8307860\ttest: 3.8462274\tbest: 3.8462274 (22)\ttotal: 55.5s\tremaining: 39m 16s\n",
      "23:\tlearn: 3.8294635\ttest: 3.8450985\tbest: 3.8450985 (23)\ttotal: 57.9s\tremaining: 39m 12s\n",
      "24:\tlearn: 3.8281833\ttest: 3.8440617\tbest: 3.8440617 (24)\ttotal: 1m\tremaining: 39m 9s\n",
      "25:\tlearn: 3.8267433\ttest: 3.8430210\tbest: 3.8430210 (25)\ttotal: 1m 2s\tremaining: 39m 7s\n",
      "26:\tlearn: 3.8254613\ttest: 3.8420135\tbest: 3.8420135 (26)\ttotal: 1m 5s\tremaining: 39m 5s\n",
      "27:\tlearn: 3.8240082\ttest: 3.8410483\tbest: 3.8410483 (27)\ttotal: 1m 7s\tremaining: 39m 2s\n",
      "28:\tlearn: 3.8226815\ttest: 3.8400805\tbest: 3.8400805 (28)\ttotal: 1m 9s\tremaining: 38m 59s\n",
      "29:\tlearn: 3.8215091\ttest: 3.8391042\tbest: 3.8391042 (29)\ttotal: 1m 12s\tremaining: 38m 55s\n",
      "30:\tlearn: 3.8202559\ttest: 3.8382028\tbest: 3.8382028 (30)\ttotal: 1m 14s\tremaining: 38m 52s\n",
      "31:\tlearn: 3.8190122\ttest: 3.8373005\tbest: 3.8373005 (31)\ttotal: 1m 17s\tremaining: 38m 50s\n",
      "32:\tlearn: 3.8177394\ttest: 3.8363999\tbest: 3.8363999 (32)\ttotal: 1m 19s\tremaining: 38m 47s\n",
      "33:\tlearn: 3.8166463\ttest: 3.8355185\tbest: 3.8355185 (33)\ttotal: 1m 21s\tremaining: 38m 44s\n",
      "34:\tlearn: 3.8155285\ttest: 3.8346672\tbest: 3.8346672 (34)\ttotal: 1m 24s\tremaining: 38m 41s\n",
      "35:\tlearn: 3.8143803\ttest: 3.8338037\tbest: 3.8338037 (35)\ttotal: 1m 26s\tremaining: 38m 39s\n",
      "36:\tlearn: 3.8132355\ttest: 3.8329558\tbest: 3.8329558 (36)\ttotal: 1m 28s\tremaining: 38m 36s\n",
      "37:\tlearn: 3.8119614\ttest: 3.8321297\tbest: 3.8321297 (37)\ttotal: 1m 31s\tremaining: 38m 34s\n",
      "38:\tlearn: 3.8108636\ttest: 3.8313355\tbest: 3.8313355 (38)\ttotal: 1m 33s\tremaining: 38m 32s\n",
      "39:\tlearn: 3.8096523\ttest: 3.8305032\tbest: 3.8305032 (39)\ttotal: 1m 36s\tremaining: 38m 30s\n",
      "40:\tlearn: 3.8085523\ttest: 3.8296881\tbest: 3.8296881 (40)\ttotal: 1m 38s\tremaining: 38m 27s\n",
      "41:\tlearn: 3.8074507\ttest: 3.8289465\tbest: 3.8289465 (41)\ttotal: 1m 41s\tremaining: 38m 25s\n",
      "42:\tlearn: 3.8062987\ttest: 3.8281870\tbest: 3.8281870 (42)\ttotal: 1m 43s\tremaining: 38m 22s\n",
      "43:\tlearn: 3.8052823\ttest: 3.8274682\tbest: 3.8274682 (43)\ttotal: 1m 45s\tremaining: 38m 19s\n",
      "44:\tlearn: 3.8042376\ttest: 3.8267084\tbest: 3.8267084 (44)\ttotal: 1m 48s\tremaining: 38m 17s\n",
      "45:\tlearn: 3.8031527\ttest: 3.8260468\tbest: 3.8260468 (45)\ttotal: 1m 50s\tremaining: 38m 14s\n",
      "46:\tlearn: 3.8022283\ttest: 3.8253196\tbest: 3.8253196 (46)\ttotal: 1m 53s\tremaining: 38m 14s\n",
      "47:\tlearn: 3.8012892\ttest: 3.8246085\tbest: 3.8246085 (47)\ttotal: 1m 55s\tremaining: 38m 13s\n",
      "48:\tlearn: 3.8001540\ttest: 3.8240097\tbest: 3.8240097 (48)\ttotal: 1m 58s\tremaining: 38m 12s\n",
      "49:\tlearn: 3.7990040\ttest: 3.8233821\tbest: 3.8233821 (49)\ttotal: 2m\tremaining: 38m 9s\n",
      "50:\tlearn: 3.7982418\ttest: 3.8227310\tbest: 3.8227310 (50)\ttotal: 2m 3s\tremaining: 38m 9s\n",
      "51:\tlearn: 3.7973413\ttest: 3.8221450\tbest: 3.8221450 (51)\ttotal: 2m 5s\tremaining: 38m 6s\n",
      "52:\tlearn: 3.7965601\ttest: 3.8214715\tbest: 3.8214715 (52)\ttotal: 2m 7s\tremaining: 38m 3s\n",
      "53:\tlearn: 3.7956821\ttest: 3.8208554\tbest: 3.8208554 (53)\ttotal: 2m 10s\tremaining: 38m\n",
      "54:\tlearn: 3.7949080\ttest: 3.8202463\tbest: 3.8202463 (54)\ttotal: 2m 12s\tremaining: 37m 58s\n",
      "55:\tlearn: 3.7940611\ttest: 3.8196765\tbest: 3.8196765 (55)\ttotal: 2m 14s\tremaining: 37m 55s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56:\tlearn: 3.7932501\ttest: 3.8190341\tbest: 3.8190341 (56)\ttotal: 2m 17s\tremaining: 37m 53s\n",
      "57:\tlearn: 3.7922266\ttest: 3.8184363\tbest: 3.8184363 (57)\ttotal: 2m 19s\tremaining: 37m 50s\n",
      "58:\tlearn: 3.7914919\ttest: 3.8178063\tbest: 3.8178063 (58)\ttotal: 2m 22s\tremaining: 37m 47s\n",
      "59:\tlearn: 3.7906619\ttest: 3.8172803\tbest: 3.8172803 (59)\ttotal: 2m 24s\tremaining: 37m 45s\n",
      "60:\tlearn: 3.7899449\ttest: 3.8167451\tbest: 3.8167451 (60)\ttotal: 2m 27s\tremaining: 37m 43s\n",
      "61:\tlearn: 3.7891523\ttest: 3.8162972\tbest: 3.8162972 (61)\ttotal: 2m 29s\tremaining: 37m 40s\n",
      "62:\tlearn: 3.7881757\ttest: 3.8157701\tbest: 3.8157701 (62)\ttotal: 2m 31s\tremaining: 37m 38s\n",
      "63:\tlearn: 3.7872558\ttest: 3.8152210\tbest: 3.8152210 (63)\ttotal: 2m 34s\tremaining: 37m 35s\n",
      "64:\tlearn: 3.7862676\ttest: 3.8146552\tbest: 3.8146552 (64)\ttotal: 2m 36s\tremaining: 37m 33s\n",
      "65:\tlearn: 3.7853633\ttest: 3.8141955\tbest: 3.8141955 (65)\ttotal: 2m 39s\tremaining: 37m 30s\n",
      "66:\tlearn: 3.7845803\ttest: 3.8136727\tbest: 3.8136727 (66)\ttotal: 2m 41s\tremaining: 37m 28s\n",
      "67:\tlearn: 3.7839016\ttest: 3.8131865\tbest: 3.8131865 (67)\ttotal: 2m 43s\tremaining: 37m 25s\n",
      "68:\tlearn: 3.7831715\ttest: 3.8127229\tbest: 3.8127229 (68)\ttotal: 2m 46s\tremaining: 37m 23s\n",
      "69:\tlearn: 3.7822694\ttest: 3.8123104\tbest: 3.8123104 (69)\ttotal: 2m 48s\tremaining: 37m 20s\n",
      "70:\tlearn: 3.7814070\ttest: 3.8118647\tbest: 3.8118647 (70)\ttotal: 2m 51s\tremaining: 37m 18s\n",
      "71:\tlearn: 3.7805062\ttest: 3.8114245\tbest: 3.8114245 (71)\ttotal: 2m 53s\tremaining: 37m 15s\n",
      "72:\tlearn: 3.7796041\ttest: 3.8110163\tbest: 3.8110163 (72)\ttotal: 2m 55s\tremaining: 37m 13s\n",
      "73:\tlearn: 3.7789203\ttest: 3.8105695\tbest: 3.8105695 (73)\ttotal: 2m 58s\tremaining: 37m 10s\n",
      "74:\tlearn: 3.7782043\ttest: 3.8101413\tbest: 3.8101413 (74)\ttotal: 3m\tremaining: 37m 8s\n",
      "75:\tlearn: 3.7775613\ttest: 3.8097197\tbest: 3.8097197 (75)\ttotal: 3m 3s\tremaining: 37m 6s\n",
      "76:\tlearn: 3.7767861\ttest: 3.8092740\tbest: 3.8092740 (76)\ttotal: 3m 5s\tremaining: 37m 3s\n",
      "77:\tlearn: 3.7761379\ttest: 3.8089223\tbest: 3.8089223 (77)\ttotal: 3m 7s\tremaining: 37m 2s\n",
      "78:\tlearn: 3.7755471\ttest: 3.8085145\tbest: 3.8085145 (78)\ttotal: 3m 10s\tremaining: 37m 2s\n",
      "79:\tlearn: 3.7746157\ttest: 3.8082281\tbest: 3.8082281 (79)\ttotal: 3m 13s\tremaining: 37m 1s\n",
      "80:\tlearn: 3.7738072\ttest: 3.8078426\tbest: 3.8078426 (80)\ttotal: 3m 15s\tremaining: 36m 59s\n",
      "81:\tlearn: 3.7731244\ttest: 3.8075015\tbest: 3.8075015 (81)\ttotal: 3m 17s\tremaining: 36m 56s\n",
      "82:\tlearn: 3.7724218\ttest: 3.8071721\tbest: 3.8071721 (82)\ttotal: 3m 20s\tremaining: 36m 54s\n",
      "83:\tlearn: 3.7715495\ttest: 3.8067952\tbest: 3.8067952 (83)\ttotal: 3m 22s\tremaining: 36m 51s\n",
      "84:\tlearn: 3.7709711\ttest: 3.8064438\tbest: 3.8064438 (84)\ttotal: 3m 25s\tremaining: 36m 49s\n",
      "85:\tlearn: 3.7703289\ttest: 3.8060566\tbest: 3.8060566 (85)\ttotal: 3m 27s\tremaining: 36m 46s\n",
      "86:\tlearn: 3.7697688\ttest: 3.8057488\tbest: 3.8057488 (86)\ttotal: 3m 29s\tremaining: 36m 43s\n",
      "87:\tlearn: 3.7689697\ttest: 3.8053656\tbest: 3.8053656 (87)\ttotal: 3m 32s\tremaining: 36m 41s\n",
      "88:\tlearn: 3.7683719\ttest: 3.8049876\tbest: 3.8049876 (88)\ttotal: 3m 34s\tremaining: 36m 39s\n",
      "89:\tlearn: 3.7676007\ttest: 3.8046731\tbest: 3.8046731 (89)\ttotal: 3m 37s\tremaining: 36m 36s\n",
      "90:\tlearn: 3.7669645\ttest: 3.8043252\tbest: 3.8043252 (90)\ttotal: 3m 39s\tremaining: 36m 34s\n",
      "91:\tlearn: 3.7663852\ttest: 3.8040055\tbest: 3.8040055 (91)\ttotal: 3m 42s\tremaining: 36m 31s\n",
      "92:\tlearn: 3.7657890\ttest: 3.8036919\tbest: 3.8036919 (92)\ttotal: 3m 44s\tremaining: 36m 29s\n",
      "93:\tlearn: 3.7651263\ttest: 3.8034033\tbest: 3.8034033 (93)\ttotal: 3m 46s\tremaining: 36m 26s\n",
      "94:\tlearn: 3.7645631\ttest: 3.8030868\tbest: 3.8030868 (94)\ttotal: 3m 49s\tremaining: 36m 24s\n",
      "95:\tlearn: 3.7638462\ttest: 3.8027841\tbest: 3.8027841 (95)\ttotal: 3m 51s\tremaining: 36m 21s\n",
      "96:\tlearn: 3.7631333\ttest: 3.8025940\tbest: 3.8025940 (96)\ttotal: 3m 54s\tremaining: 36m 18s\n",
      "97:\tlearn: 3.7624105\ttest: 3.8023504\tbest: 3.8023504 (97)\ttotal: 3m 56s\tremaining: 36m 16s\n",
      "98:\tlearn: 3.7619474\ttest: 3.8020881\tbest: 3.8020881 (98)\ttotal: 3m 58s\tremaining: 36m 13s\n",
      "99:\tlearn: 3.7612220\ttest: 3.8018159\tbest: 3.8018159 (99)\ttotal: 4m 1s\tremaining: 36m 11s\n",
      "100:\tlearn: 3.7607205\ttest: 3.8015395\tbest: 3.8015395 (100)\ttotal: 4m 3s\tremaining: 36m 9s\n",
      "101:\tlearn: 3.7603166\ttest: 3.8012606\tbest: 3.8012606 (101)\ttotal: 4m 6s\tremaining: 36m 7s\n",
      "102:\tlearn: 3.7598924\ttest: 3.8010499\tbest: 3.8010499 (102)\ttotal: 4m 8s\tremaining: 36m 4s\n",
      "103:\tlearn: 3.7593687\ttest: 3.8007941\tbest: 3.8007941 (103)\ttotal: 4m 10s\tremaining: 36m 1s\n",
      "104:\tlearn: 3.7588959\ttest: 3.8005755\tbest: 3.8005755 (104)\ttotal: 4m 13s\tremaining: 35m 58s\n",
      "105:\tlearn: 3.7582723\ttest: 3.8002972\tbest: 3.8002972 (105)\ttotal: 4m 15s\tremaining: 35m 55s\n",
      "106:\tlearn: 3.7577144\ttest: 3.8000732\tbest: 3.8000732 (106)\ttotal: 4m 18s\tremaining: 35m 53s\n",
      "107:\tlearn: 3.7570756\ttest: 3.7997994\tbest: 3.7997994 (107)\ttotal: 4m 20s\tremaining: 35m 50s\n",
      "108:\tlearn: 3.7565901\ttest: 3.7995879\tbest: 3.7995879 (108)\ttotal: 4m 23s\tremaining: 35m 50s\n",
      "109:\tlearn: 3.7562150\ttest: 3.7994033\tbest: 3.7994033 (109)\ttotal: 4m 25s\tremaining: 35m 48s\n",
      "110:\tlearn: 3.7554946\ttest: 3.7991832\tbest: 3.7991832 (110)\ttotal: 4m 27s\tremaining: 35m 46s\n",
      "111:\tlearn: 3.7548542\ttest: 3.7989875\tbest: 3.7989875 (111)\ttotal: 4m 30s\tremaining: 35m 45s\n",
      "112:\tlearn: 3.7542552\ttest: 3.7988170\tbest: 3.7988170 (112)\ttotal: 4m 33s\tremaining: 35m 44s\n",
      "113:\tlearn: 3.7534808\ttest: 3.7985948\tbest: 3.7985948 (113)\ttotal: 4m 35s\tremaining: 35m 44s\n",
      "114:\tlearn: 3.7527646\ttest: 3.7983577\tbest: 3.7983577 (114)\ttotal: 4m 38s\tremaining: 35m 43s\n",
      "115:\tlearn: 3.7522298\ttest: 3.7981244\tbest: 3.7981244 (115)\ttotal: 4m 41s\tremaining: 35m 42s\n",
      "116:\tlearn: 3.7517469\ttest: 3.7978893\tbest: 3.7978893 (116)\ttotal: 4m 43s\tremaining: 35m 40s\n",
      "117:\tlearn: 3.7511353\ttest: 3.7976853\tbest: 3.7976853 (117)\ttotal: 4m 45s\tremaining: 35m 37s\n",
      "118:\tlearn: 3.7505445\ttest: 3.7974458\tbest: 3.7974458 (118)\ttotal: 4m 48s\tremaining: 35m 34s\n",
      "119:\tlearn: 3.7501255\ttest: 3.7972021\tbest: 3.7972021 (119)\ttotal: 4m 50s\tremaining: 35m 31s\n",
      "120:\tlearn: 3.7495900\ttest: 3.7970335\tbest: 3.7970335 (120)\ttotal: 4m 53s\tremaining: 35m 28s\n",
      "121:\tlearn: 3.7489428\ttest: 3.7968475\tbest: 3.7968475 (121)\ttotal: 4m 55s\tremaining: 35m 26s\n",
      "122:\tlearn: 3.7484114\ttest: 3.7966579\tbest: 3.7966579 (122)\ttotal: 4m 57s\tremaining: 35m 23s\n",
      "123:\tlearn: 3.7479277\ttest: 3.7964850\tbest: 3.7964850 (123)\ttotal: 5m\tremaining: 35m 20s\n",
      "124:\tlearn: 3.7474435\ttest: 3.7962951\tbest: 3.7962951 (124)\ttotal: 5m 2s\tremaining: 35m 18s\n",
      "125:\tlearn: 3.7470437\ttest: 3.7961555\tbest: 3.7961555 (125)\ttotal: 5m 4s\tremaining: 35m 15s\n",
      "126:\tlearn: 3.7465286\ttest: 3.7960171\tbest: 3.7960171 (126)\ttotal: 5m 7s\tremaining: 35m 12s\n",
      "127:\tlearn: 3.7461804\ttest: 3.7958197\tbest: 3.7958197 (127)\ttotal: 5m 9s\tremaining: 35m 10s\n",
      "128:\tlearn: 3.7456249\ttest: 3.7956536\tbest: 3.7956536 (128)\ttotal: 5m 12s\tremaining: 35m 7s\n",
      "129:\tlearn: 3.7453134\ttest: 3.7955151\tbest: 3.7955151 (129)\ttotal: 5m 14s\tremaining: 35m 4s\n",
      "130:\tlearn: 3.7448273\ttest: 3.7953019\tbest: 3.7953019 (130)\ttotal: 5m 16s\tremaining: 35m 2s\n",
      "131:\tlearn: 3.7445542\ttest: 3.7951814\tbest: 3.7951814 (131)\ttotal: 5m 19s\tremaining: 34m 59s\n",
      "132:\tlearn: 3.7440862\ttest: 3.7950401\tbest: 3.7950401 (132)\ttotal: 5m 21s\tremaining: 34m 56s\n",
      "133:\tlearn: 3.7434929\ttest: 3.7949088\tbest: 3.7949088 (133)\ttotal: 5m 24s\tremaining: 34m 54s\n",
      "134:\tlearn: 3.7429895\ttest: 3.7947739\tbest: 3.7947739 (134)\ttotal: 5m 26s\tremaining: 34m 51s\n",
      "135:\tlearn: 3.7425607\ttest: 3.7946399\tbest: 3.7946399 (135)\ttotal: 5m 28s\tremaining: 34m 49s\n",
      "136:\tlearn: 3.7422025\ttest: 3.7944886\tbest: 3.7944886 (136)\ttotal: 5m 31s\tremaining: 34m 47s\n",
      "137:\tlearn: 3.7416630\ttest: 3.7943855\tbest: 3.7943855 (137)\ttotal: 5m 33s\tremaining: 34m 44s\n",
      "138:\tlearn: 3.7411476\ttest: 3.7942626\tbest: 3.7942626 (138)\ttotal: 5m 36s\tremaining: 34m 41s\n",
      "139:\tlearn: 3.7406162\ttest: 3.7941063\tbest: 3.7941063 (139)\ttotal: 5m 38s\tremaining: 34m 38s\n",
      "140:\tlearn: 3.7400421\ttest: 3.7940050\tbest: 3.7940050 (140)\ttotal: 5m 40s\tremaining: 34m 36s\n",
      "141:\tlearn: 3.7396345\ttest: 3.7939084\tbest: 3.7939084 (141)\ttotal: 5m 43s\tremaining: 34m 33s\n",
      "142:\tlearn: 3.7390596\ttest: 3.7938689\tbest: 3.7938689 (142)\ttotal: 5m 45s\tremaining: 34m 30s\n",
      "143:\tlearn: 3.7384489\ttest: 3.7937605\tbest: 3.7937605 (143)\ttotal: 5m 47s\tremaining: 34m 28s\n",
      "144:\tlearn: 3.7377780\ttest: 3.7936740\tbest: 3.7936740 (144)\ttotal: 5m 50s\tremaining: 34m 25s\n",
      "145:\tlearn: 3.7372659\ttest: 3.7935851\tbest: 3.7935851 (145)\ttotal: 5m 52s\tremaining: 34m 22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146:\tlearn: 3.7366548\ttest: 3.7934635\tbest: 3.7934635 (146)\ttotal: 5m 55s\tremaining: 34m 20s\n",
      "147:\tlearn: 3.7361585\ttest: 3.7933753\tbest: 3.7933753 (147)\ttotal: 5m 57s\tremaining: 34m 20s\n",
      "148:\tlearn: 3.7358359\ttest: 3.7932525\tbest: 3.7932525 (148)\ttotal: 6m\tremaining: 34m 21s\n",
      "149:\tlearn: 3.7354596\ttest: 3.7931261\tbest: 3.7931261 (149)\ttotal: 6m 3s\tremaining: 34m 20s\n",
      "150:\tlearn: 3.7350813\ttest: 3.7930050\tbest: 3.7930050 (150)\ttotal: 6m 6s\tremaining: 34m 20s\n",
      "151:\tlearn: 3.7347365\ttest: 3.7928834\tbest: 3.7928834 (151)\ttotal: 6m 9s\tremaining: 34m 21s\n",
      "152:\tlearn: 3.7344254\ttest: 3.7927795\tbest: 3.7927795 (152)\ttotal: 6m 12s\tremaining: 34m 20s\n",
      "153:\tlearn: 3.7338839\ttest: 3.7926961\tbest: 3.7926961 (153)\ttotal: 6m 14s\tremaining: 34m 19s\n",
      "154:\tlearn: 3.7332971\ttest: 3.7925849\tbest: 3.7925849 (154)\ttotal: 6m 17s\tremaining: 34m 19s\n",
      "155:\tlearn: 3.7327876\ttest: 3.7924635\tbest: 3.7924635 (155)\ttotal: 6m 20s\tremaining: 34m 18s\n",
      "156:\tlearn: 3.7321151\ttest: 3.7923810\tbest: 3.7923810 (156)\ttotal: 6m 23s\tremaining: 34m 17s\n",
      "157:\tlearn: 3.7317709\ttest: 3.7922681\tbest: 3.7922681 (157)\ttotal: 6m 25s\tremaining: 34m 14s\n",
      "158:\tlearn: 3.7311765\ttest: 3.7921250\tbest: 3.7921250 (158)\ttotal: 6m 28s\tremaining: 34m 12s\n",
      "159:\tlearn: 3.7307646\ttest: 3.7920207\tbest: 3.7920207 (159)\ttotal: 6m 30s\tremaining: 34m 10s\n",
      "160:\tlearn: 3.7303107\ttest: 3.7919179\tbest: 3.7919179 (160)\ttotal: 6m 33s\tremaining: 34m 8s\n",
      "161:\tlearn: 3.7299999\ttest: 3.7918289\tbest: 3.7918289 (161)\ttotal: 6m 35s\tremaining: 34m 6s\n",
      "162:\tlearn: 3.7296472\ttest: 3.7917513\tbest: 3.7917513 (162)\ttotal: 6m 38s\tremaining: 34m 7s\n",
      "163:\tlearn: 3.7292729\ttest: 3.7916736\tbest: 3.7916736 (163)\ttotal: 6m 41s\tremaining: 34m 7s\n",
      "164:\tlearn: 3.7289731\ttest: 3.7915703\tbest: 3.7915703 (164)\ttotal: 6m 44s\tremaining: 34m 6s\n",
      "165:\tlearn: 3.7285855\ttest: 3.7915217\tbest: 3.7915217 (165)\ttotal: 6m 46s\tremaining: 34m 3s\n",
      "166:\tlearn: 3.7279336\ttest: 3.7914603\tbest: 3.7914603 (166)\ttotal: 6m 49s\tremaining: 34m 3s\n",
      "167:\tlearn: 3.7275380\ttest: 3.7914034\tbest: 3.7914034 (167)\ttotal: 6m 52s\tremaining: 34m 3s\n",
      "168:\tlearn: 3.7270597\ttest: 3.7912864\tbest: 3.7912864 (168)\ttotal: 6m 56s\tremaining: 34m 6s\n",
      "169:\tlearn: 3.7266825\ttest: 3.7911650\tbest: 3.7911650 (169)\ttotal: 6m 58s\tremaining: 34m 4s\n",
      "170:\tlearn: 3.7263056\ttest: 3.7911125\tbest: 3.7911125 (170)\ttotal: 7m 1s\tremaining: 34m 2s\n",
      "171:\tlearn: 3.7258971\ttest: 3.7910189\tbest: 3.7910189 (171)\ttotal: 7m 4s\tremaining: 34m 2s\n",
      "172:\tlearn: 3.7255922\ttest: 3.7909141\tbest: 3.7909141 (172)\ttotal: 7m 6s\tremaining: 33m 59s\n",
      "173:\tlearn: 3.7250457\ttest: 3.7908445\tbest: 3.7908445 (173)\ttotal: 7m 9s\tremaining: 34m 1s\n",
      "174:\tlearn: 3.7247260\ttest: 3.7907851\tbest: 3.7907851 (174)\ttotal: 7m 12s\tremaining: 33m 59s\n",
      "175:\tlearn: 3.7242263\ttest: 3.7907431\tbest: 3.7907431 (175)\ttotal: 7m 16s\tremaining: 34m 3s\n",
      "176:\tlearn: 3.7238171\ttest: 3.7907231\tbest: 3.7907231 (176)\ttotal: 7m 19s\tremaining: 34m 4s\n",
      "177:\tlearn: 3.7234462\ttest: 3.7906713\tbest: 3.7906713 (177)\ttotal: 7m 22s\tremaining: 34m 5s\n",
      "178:\tlearn: 3.7229085\ttest: 3.7905975\tbest: 3.7905975 (178)\ttotal: 7m 26s\tremaining: 34m 6s\n",
      "179:\tlearn: 3.7225867\ttest: 3.7905545\tbest: 3.7905545 (179)\ttotal: 7m 29s\tremaining: 34m 7s\n",
      "180:\tlearn: 3.7222356\ttest: 3.7904871\tbest: 3.7904871 (180)\ttotal: 7m 32s\tremaining: 34m 9s\n",
      "181:\tlearn: 3.7218907\ttest: 3.7904542\tbest: 3.7904542 (181)\ttotal: 7m 36s\tremaining: 34m 9s\n",
      "182:\tlearn: 3.7213085\ttest: 3.7903817\tbest: 3.7903817 (182)\ttotal: 7m 39s\tremaining: 34m 9s\n",
      "183:\tlearn: 3.7208720\ttest: 3.7903443\tbest: 3.7903443 (183)\ttotal: 7m 41s\tremaining: 34m 8s\n",
      "184:\tlearn: 3.7202073\ttest: 3.7902650\tbest: 3.7902650 (184)\ttotal: 7m 44s\tremaining: 34m 6s\n",
      "185:\tlearn: 3.7197358\ttest: 3.7901669\tbest: 3.7901669 (185)\ttotal: 7m 47s\tremaining: 34m 4s\n",
      "186:\tlearn: 3.7193387\ttest: 3.7901044\tbest: 3.7901044 (186)\ttotal: 7m 49s\tremaining: 34m 2s\n",
      "187:\tlearn: 3.7189901\ttest: 3.7900467\tbest: 3.7900467 (187)\ttotal: 7m 52s\tremaining: 34m 1s\n",
      "188:\tlearn: 3.7184746\ttest: 3.7900279\tbest: 3.7900279 (188)\ttotal: 7m 56s\tremaining: 34m 2s\n",
      "189:\tlearn: 3.7180962\ttest: 3.7899607\tbest: 3.7899607 (189)\ttotal: 7m 59s\tremaining: 34m 4s\n",
      "190:\tlearn: 3.7178001\ttest: 3.7899247\tbest: 3.7899247 (190)\ttotal: 8m 2s\tremaining: 34m 3s\n",
      "191:\tlearn: 3.7175587\ttest: 3.7898495\tbest: 3.7898495 (191)\ttotal: 8m 5s\tremaining: 34m 3s\n",
      "192:\tlearn: 3.7171057\ttest: 3.7897703\tbest: 3.7897703 (192)\ttotal: 8m 8s\tremaining: 34m 2s\n",
      "193:\tlearn: 3.7165551\ttest: 3.7897297\tbest: 3.7897297 (193)\ttotal: 8m 11s\tremaining: 34m 2s\n",
      "194:\tlearn: 3.7159206\ttest: 3.7896496\tbest: 3.7896496 (194)\ttotal: 8m 15s\tremaining: 34m 4s\n",
      "195:\tlearn: 3.7154815\ttest: 3.7895349\tbest: 3.7895349 (195)\ttotal: 8m 18s\tremaining: 34m 3s\n",
      "196:\tlearn: 3.7152167\ttest: 3.7894503\tbest: 3.7894503 (196)\ttotal: 8m 20s\tremaining: 34m 1s\n",
      "197:\tlearn: 3.7147936\ttest: 3.7893768\tbest: 3.7893768 (197)\ttotal: 8m 23s\tremaining: 33m 59s\n",
      "198:\tlearn: 3.7143070\ttest: 3.7893691\tbest: 3.7893691 (198)\ttotal: 8m 26s\tremaining: 33m 58s\n",
      "199:\tlearn: 3.7139095\ttest: 3.7893180\tbest: 3.7893180 (199)\ttotal: 8m 29s\tremaining: 33m 58s\n",
      "200:\tlearn: 3.7134698\ttest: 3.7892985\tbest: 3.7892985 (200)\ttotal: 8m 32s\tremaining: 33m 57s\n",
      "201:\tlearn: 3.7130294\ttest: 3.7892536\tbest: 3.7892536 (201)\ttotal: 8m 35s\tremaining: 33m 55s\n",
      "202:\tlearn: 3.7126391\ttest: 3.7892281\tbest: 3.7892281 (202)\ttotal: 8m 37s\tremaining: 33m 53s\n",
      "203:\tlearn: 3.7122014\ttest: 3.7892034\tbest: 3.7892034 (203)\ttotal: 8m 40s\tremaining: 33m 51s\n",
      "204:\tlearn: 3.7119562\ttest: 3.7891498\tbest: 3.7891498 (204)\ttotal: 8m 43s\tremaining: 33m 49s\n",
      "205:\tlearn: 3.7112275\ttest: 3.7891585\tbest: 3.7891498 (204)\ttotal: 8m 46s\tremaining: 33m 49s\n",
      "206:\tlearn: 3.7109076\ttest: 3.7891248\tbest: 3.7891248 (206)\ttotal: 8m 49s\tremaining: 33m 46s\n",
      "207:\tlearn: 3.7105850\ttest: 3.7890975\tbest: 3.7890975 (207)\ttotal: 8m 51s\tremaining: 33m 44s\n",
      "208:\tlearn: 3.7101109\ttest: 3.7891012\tbest: 3.7890975 (207)\ttotal: 8m 54s\tremaining: 33m 42s\n",
      "209:\tlearn: 3.7097671\ttest: 3.7890237\tbest: 3.7890237 (209)\ttotal: 8m 57s\tremaining: 33m 42s\n",
      "210:\tlearn: 3.7095805\ttest: 3.7889717\tbest: 3.7889717 (210)\ttotal: 9m\tremaining: 33m 41s\n",
      "211:\tlearn: 3.7093144\ttest: 3.7888939\tbest: 3.7888939 (211)\ttotal: 9m 3s\tremaining: 33m 41s\n",
      "212:\tlearn: 3.7089820\ttest: 3.7888680\tbest: 3.7888680 (212)\ttotal: 9m 6s\tremaining: 33m 39s\n",
      "213:\tlearn: 3.7086908\ttest: 3.7888449\tbest: 3.7888449 (213)\ttotal: 9m 9s\tremaining: 33m 38s\n",
      "214:\tlearn: 3.7081341\ttest: 3.7888128\tbest: 3.7888128 (214)\ttotal: 9m 12s\tremaining: 33m 38s\n",
      "215:\tlearn: 3.7078290\ttest: 3.7887741\tbest: 3.7887741 (215)\ttotal: 9m 15s\tremaining: 33m 37s\n",
      "216:\tlearn: 3.7075919\ttest: 3.7887530\tbest: 3.7887530 (216)\ttotal: 9m 18s\tremaining: 33m 36s\n",
      "217:\tlearn: 3.7071575\ttest: 3.7887456\tbest: 3.7887456 (217)\ttotal: 9m 21s\tremaining: 33m 35s\n",
      "218:\tlearn: 3.7068752\ttest: 3.7886949\tbest: 3.7886949 (218)\ttotal: 9m 24s\tremaining: 33m 32s\n",
      "219:\tlearn: 3.7065668\ttest: 3.7886363\tbest: 3.7886363 (219)\ttotal: 9m 26s\tremaining: 33m 29s\n",
      "220:\tlearn: 3.7061428\ttest: 3.7886186\tbest: 3.7886186 (220)\ttotal: 9m 29s\tremaining: 33m 26s\n",
      "221:\tlearn: 3.7056741\ttest: 3.7886013\tbest: 3.7886013 (221)\ttotal: 9m 31s\tremaining: 33m 23s\n",
      "222:\tlearn: 3.7054221\ttest: 3.7886157\tbest: 3.7886013 (221)\ttotal: 9m 34s\tremaining: 33m 20s\n",
      "223:\tlearn: 3.7050252\ttest: 3.7885520\tbest: 3.7885520 (223)\ttotal: 9m 36s\tremaining: 33m 17s\n",
      "224:\tlearn: 3.7047623\ttest: 3.7885311\tbest: 3.7885311 (224)\ttotal: 9m 39s\tremaining: 33m 14s\n",
      "225:\tlearn: 3.7043615\ttest: 3.7885437\tbest: 3.7885311 (224)\ttotal: 9m 41s\tremaining: 33m 11s\n",
      "226:\tlearn: 3.7038859\ttest: 3.7885313\tbest: 3.7885311 (224)\ttotal: 9m 44s\tremaining: 33m 8s\n",
      "227:\tlearn: 3.7036294\ttest: 3.7884898\tbest: 3.7884898 (227)\ttotal: 9m 46s\tremaining: 33m 5s\n",
      "228:\tlearn: 3.7031935\ttest: 3.7885131\tbest: 3.7884898 (227)\ttotal: 9m 49s\tremaining: 33m 4s\n",
      "229:\tlearn: 3.7027646\ttest: 3.7884294\tbest: 3.7884294 (229)\ttotal: 9m 52s\tremaining: 33m 2s\n",
      "230:\tlearn: 3.7025520\ttest: 3.7883790\tbest: 3.7883790 (230)\ttotal: 9m 54s\tremaining: 32m 59s\n",
      "231:\tlearn: 3.7022255\ttest: 3.7883596\tbest: 3.7883596 (231)\ttotal: 9m 57s\tremaining: 32m 58s\n",
      "232:\tlearn: 3.7017909\ttest: 3.7883477\tbest: 3.7883477 (232)\ttotal: 10m 1s\tremaining: 32m 59s\n",
      "233:\tlearn: 3.7013455\ttest: 3.7883275\tbest: 3.7883275 (233)\ttotal: 10m 4s\tremaining: 32m 57s\n",
      "234:\tlearn: 3.7011229\ttest: 3.7883119\tbest: 3.7883119 (234)\ttotal: 10m 7s\tremaining: 32m 56s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235:\tlearn: 3.7009148\ttest: 3.7882715\tbest: 3.7882715 (235)\ttotal: 10m 10s\tremaining: 32m 55s\n",
      "236:\tlearn: 3.7007323\ttest: 3.7882210\tbest: 3.7882210 (236)\ttotal: 10m 12s\tremaining: 32m 53s\n",
      "237:\tlearn: 3.7003909\ttest: 3.7882215\tbest: 3.7882210 (236)\ttotal: 10m 15s\tremaining: 32m 51s\n",
      "238:\tlearn: 3.7000827\ttest: 3.7881571\tbest: 3.7881571 (238)\ttotal: 10m 18s\tremaining: 32m 49s\n",
      "239:\tlearn: 3.6997045\ttest: 3.7881228\tbest: 3.7881228 (239)\ttotal: 10m 21s\tremaining: 32m 47s\n",
      "240:\tlearn: 3.6992858\ttest: 3.7880705\tbest: 3.7880705 (240)\ttotal: 10m 23s\tremaining: 32m 45s\n",
      "241:\tlearn: 3.6987620\ttest: 3.7880409\tbest: 3.7880409 (241)\ttotal: 10m 26s\tremaining: 32m 43s\n",
      "242:\tlearn: 3.6980217\ttest: 3.7881240\tbest: 3.7880409 (241)\ttotal: 10m 29s\tremaining: 32m 41s\n",
      "243:\tlearn: 3.6975061\ttest: 3.7881228\tbest: 3.7880409 (241)\ttotal: 10m 32s\tremaining: 32m 38s\n",
      "244:\tlearn: 3.6971670\ttest: 3.7881202\tbest: 3.7880409 (241)\ttotal: 10m 34s\tremaining: 32m 36s\n",
      "245:\tlearn: 3.6966804\ttest: 3.7881580\tbest: 3.7880409 (241)\ttotal: 10m 37s\tremaining: 32m 34s\n",
      "246:\tlearn: 3.6964819\ttest: 3.7881345\tbest: 3.7880409 (241)\ttotal: 10m 40s\tremaining: 32m 32s\n",
      "247:\tlearn: 3.6959711\ttest: 3.7881322\tbest: 3.7880409 (241)\ttotal: 10m 43s\tremaining: 32m 30s\n",
      "248:\tlearn: 3.6956266\ttest: 3.7880973\tbest: 3.7880409 (241)\ttotal: 10m 45s\tremaining: 32m 28s\n",
      "249:\tlearn: 3.6952122\ttest: 3.7880811\tbest: 3.7880409 (241)\ttotal: 10m 48s\tremaining: 32m 26s\n",
      "250:\tlearn: 3.6950354\ttest: 3.7880454\tbest: 3.7880409 (241)\ttotal: 10m 51s\tremaining: 32m 24s\n",
      "251:\tlearn: 3.6946776\ttest: 3.7880313\tbest: 3.7880313 (251)\ttotal: 10m 54s\tremaining: 32m 22s\n",
      "252:\tlearn: 3.6943779\ttest: 3.7880456\tbest: 3.7880313 (251)\ttotal: 10m 57s\tremaining: 32m 21s\n",
      "253:\tlearn: 3.6938175\ttest: 3.7880093\tbest: 3.7880093 (253)\ttotal: 11m 1s\tremaining: 32m 21s\n",
      "254:\tlearn: 3.6936143\ttest: 3.7879872\tbest: 3.7879872 (254)\ttotal: 11m 4s\tremaining: 32m 21s\n",
      "255:\tlearn: 3.6932612\ttest: 3.7879612\tbest: 3.7879612 (255)\ttotal: 11m 7s\tremaining: 32m 19s\n",
      "256:\tlearn: 3.6930349\ttest: 3.7879313\tbest: 3.7879313 (256)\ttotal: 11m 10s\tremaining: 32m 17s\n",
      "257:\tlearn: 3.6926478\ttest: 3.7879365\tbest: 3.7879313 (256)\ttotal: 11m 12s\tremaining: 32m 14s\n",
      "258:\tlearn: 3.6920281\ttest: 3.7879643\tbest: 3.7879313 (256)\ttotal: 11m 15s\tremaining: 32m 12s\n",
      "259:\tlearn: 3.6916018\ttest: 3.7879727\tbest: 3.7879313 (256)\ttotal: 11m 18s\tremaining: 32m 10s\n",
      "260:\tlearn: 3.6914329\ttest: 3.7879901\tbest: 3.7879313 (256)\ttotal: 11m 21s\tremaining: 32m 8s\n",
      "261:\tlearn: 3.6909663\ttest: 3.7879566\tbest: 3.7879313 (256)\ttotal: 11m 23s\tremaining: 32m 6s\n",
      "262:\tlearn: 3.6907553\ttest: 3.7879685\tbest: 3.7879313 (256)\ttotal: 11m 26s\tremaining: 32m 3s\n",
      "263:\tlearn: 3.6905769\ttest: 3.7879577\tbest: 3.7879313 (256)\ttotal: 11m 29s\tremaining: 32m 1s\n",
      "264:\tlearn: 3.6904036\ttest: 3.7879225\tbest: 3.7879225 (264)\ttotal: 11m 31s\tremaining: 31m 59s\n",
      "265:\tlearn: 3.6897889\ttest: 3.7879246\tbest: 3.7879225 (264)\ttotal: 11m 34s\tremaining: 31m 57s\n",
      "266:\tlearn: 3.6891384\ttest: 3.7879082\tbest: 3.7879082 (266)\ttotal: 11m 38s\tremaining: 31m 56s\n",
      "267:\tlearn: 3.6890197\ttest: 3.7878898\tbest: 3.7878898 (267)\ttotal: 11m 41s\tremaining: 31m 56s\n",
      "268:\tlearn: 3.6887944\ttest: 3.7878616\tbest: 3.7878616 (268)\ttotal: 11m 44s\tremaining: 31m 55s\n",
      "269:\tlearn: 3.6883662\ttest: 3.7878233\tbest: 3.7878233 (269)\ttotal: 11m 47s\tremaining: 31m 53s\n",
      "270:\tlearn: 3.6879068\ttest: 3.7877664\tbest: 3.7877664 (270)\ttotal: 11m 50s\tremaining: 31m 52s\n",
      "271:\tlearn: 3.6874475\ttest: 3.7877331\tbest: 3.7877331 (271)\ttotal: 11m 53s\tremaining: 31m 50s\n",
      "272:\tlearn: 3.6871673\ttest: 3.7877260\tbest: 3.7877260 (272)\ttotal: 11m 56s\tremaining: 31m 47s\n",
      "273:\tlearn: 3.6867491\ttest: 3.7877625\tbest: 3.7877260 (272)\ttotal: 11m 59s\tremaining: 31m 46s\n",
      "274:\tlearn: 3.6863880\ttest: 3.7877190\tbest: 3.7877190 (274)\ttotal: 12m 2s\tremaining: 31m 45s\n",
      "275:\tlearn: 3.6861799\ttest: 3.7876958\tbest: 3.7876958 (275)\ttotal: 12m 6s\tremaining: 31m 45s\n",
      "276:\tlearn: 3.6856671\ttest: 3.7876305\tbest: 3.7876305 (276)\ttotal: 12m 9s\tremaining: 31m 43s\n",
      "277:\tlearn: 3.6854158\ttest: 3.7876403\tbest: 3.7876305 (276)\ttotal: 12m 12s\tremaining: 31m 41s\n",
      "278:\tlearn: 3.6850413\ttest: 3.7876473\tbest: 3.7876305 (276)\ttotal: 12m 14s\tremaining: 31m 38s\n",
      "279:\tlearn: 3.6846707\ttest: 3.7876622\tbest: 3.7876305 (276)\ttotal: 12m 17s\tremaining: 31m 35s\n",
      "280:\tlearn: 3.6843908\ttest: 3.7876696\tbest: 3.7876305 (276)\ttotal: 12m 19s\tremaining: 31m 32s\n",
      "281:\tlearn: 3.6841368\ttest: 3.7876594\tbest: 3.7876305 (276)\ttotal: 12m 22s\tremaining: 31m 30s\n",
      "282:\tlearn: 3.6839784\ttest: 3.7876223\tbest: 3.7876223 (282)\ttotal: 12m 25s\tremaining: 31m 28s\n",
      "283:\tlearn: 3.6834748\ttest: 3.7875796\tbest: 3.7875796 (283)\ttotal: 12m 29s\tremaining: 31m 29s\n",
      "284:\tlearn: 3.6830422\ttest: 3.7875842\tbest: 3.7875796 (283)\ttotal: 12m 32s\tremaining: 31m 27s\n",
      "285:\tlearn: 3.6828238\ttest: 3.7875812\tbest: 3.7875796 (283)\ttotal: 12m 34s\tremaining: 31m 24s\n",
      "286:\tlearn: 3.6825185\ttest: 3.7875955\tbest: 3.7875796 (283)\ttotal: 12m 37s\tremaining: 31m 22s\n",
      "287:\tlearn: 3.6821149\ttest: 3.7876513\tbest: 3.7875796 (283)\ttotal: 12m 40s\tremaining: 31m 19s\n",
      "288:\tlearn: 3.6818534\ttest: 3.7876124\tbest: 3.7875796 (283)\ttotal: 12m 42s\tremaining: 31m 16s\n",
      "289:\tlearn: 3.6815692\ttest: 3.7875935\tbest: 3.7875796 (283)\ttotal: 12m 45s\tremaining: 31m 13s\n",
      "290:\tlearn: 3.6813067\ttest: 3.7875844\tbest: 3.7875796 (283)\ttotal: 12m 47s\tremaining: 31m 10s\n",
      "291:\tlearn: 3.6809039\ttest: 3.7876021\tbest: 3.7875796 (283)\ttotal: 12m 50s\tremaining: 31m 7s\n",
      "292:\tlearn: 3.6805655\ttest: 3.7876019\tbest: 3.7875796 (283)\ttotal: 12m 53s\tremaining: 31m 5s\n",
      "293:\tlearn: 3.6799383\ttest: 3.7876501\tbest: 3.7875796 (283)\ttotal: 12m 56s\tremaining: 31m 4s\n",
      "294:\tlearn: 3.6796184\ttest: 3.7876702\tbest: 3.7875796 (283)\ttotal: 12m 59s\tremaining: 31m 2s\n",
      "295:\tlearn: 3.6792196\ttest: 3.7877247\tbest: 3.7875796 (283)\ttotal: 13m 2s\tremaining: 31m\n",
      "296:\tlearn: 3.6788317\ttest: 3.7877354\tbest: 3.7875796 (283)\ttotal: 13m 5s\tremaining: 30m 58s\n",
      "297:\tlearn: 3.6783161\ttest: 3.7877386\tbest: 3.7875796 (283)\ttotal: 13m 8s\tremaining: 30m 57s\n",
      "298:\tlearn: 3.6780515\ttest: 3.7877608\tbest: 3.7875796 (283)\ttotal: 13m 11s\tremaining: 30m 56s\n",
      "299:\tlearn: 3.6775103\ttest: 3.7877398\tbest: 3.7875796 (283)\ttotal: 13m 15s\tremaining: 30m 55s\n",
      "300:\tlearn: 3.6771442\ttest: 3.7877457\tbest: 3.7875796 (283)\ttotal: 13m 17s\tremaining: 30m 52s\n",
      "301:\tlearn: 3.6769164\ttest: 3.7877414\tbest: 3.7875796 (283)\ttotal: 13m 20s\tremaining: 30m 50s\n",
      "302:\tlearn: 3.6763243\ttest: 3.7877837\tbest: 3.7875796 (283)\ttotal: 13m 23s\tremaining: 30m 47s\n",
      "303:\tlearn: 3.6759077\ttest: 3.7877846\tbest: 3.7875796 (283)\ttotal: 13m 25s\tremaining: 30m 44s\n",
      "304:\tlearn: 3.6757054\ttest: 3.7877989\tbest: 3.7875796 (283)\ttotal: 13m 28s\tremaining: 30m 41s\n",
      "305:\tlearn: 3.6751185\ttest: 3.7878348\tbest: 3.7875796 (283)\ttotal: 13m 30s\tremaining: 30m 39s\n",
      "306:\tlearn: 3.6749218\ttest: 3.7878233\tbest: 3.7875796 (283)\ttotal: 13m 33s\tremaining: 30m 36s\n",
      "307:\tlearn: 3.6744071\ttest: 3.7878105\tbest: 3.7875796 (283)\ttotal: 13m 35s\tremaining: 30m 33s\n",
      "308:\tlearn: 3.6738533\ttest: 3.7877762\tbest: 3.7875796 (283)\ttotal: 13m 38s\tremaining: 30m 30s\n",
      "309:\tlearn: 3.6733680\ttest: 3.7877795\tbest: 3.7875796 (283)\ttotal: 13m 41s\tremaining: 30m 28s\n",
      "310:\tlearn: 3.6731212\ttest: 3.7877757\tbest: 3.7875796 (283)\ttotal: 13m 44s\tremaining: 30m 25s\n",
      "311:\tlearn: 3.6728705\ttest: 3.7878138\tbest: 3.7875796 (283)\ttotal: 13m 46s\tremaining: 30m 22s\n",
      "312:\tlearn: 3.6725584\ttest: 3.7878139\tbest: 3.7875796 (283)\ttotal: 13m 48s\tremaining: 30m 19s\n",
      "313:\tlearn: 3.6721125\ttest: 3.7877789\tbest: 3.7875796 (283)\ttotal: 13m 51s\tremaining: 30m 16s\n",
      "314:\tlearn: 3.6716625\ttest: 3.7878017\tbest: 3.7875796 (283)\ttotal: 13m 53s\tremaining: 30m 13s\n",
      "315:\tlearn: 3.6713426\ttest: 3.7878125\tbest: 3.7875796 (283)\ttotal: 13m 56s\tremaining: 30m 10s\n",
      "316:\tlearn: 3.6711928\ttest: 3.7877674\tbest: 3.7875796 (283)\ttotal: 13m 59s\tremaining: 30m 8s\n",
      "317:\tlearn: 3.6708869\ttest: 3.7877713\tbest: 3.7875796 (283)\ttotal: 14m 1s\tremaining: 30m 5s\n",
      "318:\tlearn: 3.6705465\ttest: 3.7877389\tbest: 3.7875796 (283)\ttotal: 14m 4s\tremaining: 30m 3s\n",
      "319:\tlearn: 3.6701709\ttest: 3.7876942\tbest: 3.7875796 (283)\ttotal: 14m 7s\tremaining: 30m\n",
      "320:\tlearn: 3.6699607\ttest: 3.7876809\tbest: 3.7875796 (283)\ttotal: 14m 9s\tremaining: 29m 57s\n",
      "321:\tlearn: 3.6697715\ttest: 3.7876747\tbest: 3.7875796 (283)\ttotal: 14m 12s\tremaining: 29m 54s\n",
      "322:\tlearn: 3.6696194\ttest: 3.7876473\tbest: 3.7875796 (283)\ttotal: 14m 14s\tremaining: 29m 51s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323:\tlearn: 3.6692299\ttest: 3.7876383\tbest: 3.7875796 (283)\ttotal: 14m 17s\tremaining: 29m 48s\n",
      "324:\tlearn: 3.6688349\ttest: 3.7876262\tbest: 3.7875796 (283)\ttotal: 14m 19s\tremaining: 29m 45s\n",
      "325:\tlearn: 3.6684621\ttest: 3.7876208\tbest: 3.7875796 (283)\ttotal: 14m 22s\tremaining: 29m 42s\n",
      "326:\tlearn: 3.6682244\ttest: 3.7876383\tbest: 3.7875796 (283)\ttotal: 14m 25s\tremaining: 29m 41s\n",
      "327:\tlearn: 3.6679869\ttest: 3.7876062\tbest: 3.7875796 (283)\ttotal: 14m 28s\tremaining: 29m 39s\n",
      "328:\tlearn: 3.6677087\ttest: 3.7875917\tbest: 3.7875796 (283)\ttotal: 14m 31s\tremaining: 29m 36s\n",
      "329:\tlearn: 3.6671371\ttest: 3.7876191\tbest: 3.7875796 (283)\ttotal: 14m 33s\tremaining: 29m 33s\n",
      "330:\tlearn: 3.6669072\ttest: 3.7876137\tbest: 3.7875796 (283)\ttotal: 14m 36s\tremaining: 29m 31s\n",
      "331:\tlearn: 3.6663938\ttest: 3.7876163\tbest: 3.7875796 (283)\ttotal: 14m 39s\tremaining: 29m 28s\n",
      "332:\tlearn: 3.6659093\ttest: 3.7875883\tbest: 3.7875796 (283)\ttotal: 14m 41s\tremaining: 29m 26s\n",
      "333:\tlearn: 3.6655174\ttest: 3.7876383\tbest: 3.7875796 (283)\ttotal: 14m 44s\tremaining: 29m 23s\n",
      "334:\tlearn: 3.6652640\ttest: 3.7876253\tbest: 3.7875796 (283)\ttotal: 14m 46s\tremaining: 29m 20s\n",
      "335:\tlearn: 3.6650378\ttest: 3.7876020\tbest: 3.7875796 (283)\ttotal: 14m 49s\tremaining: 29m 17s\n",
      "336:\tlearn: 3.6647000\ttest: 3.7876032\tbest: 3.7875796 (283)\ttotal: 14m 51s\tremaining: 29m 13s\n",
      "337:\tlearn: 3.6642591\ttest: 3.7876310\tbest: 3.7875796 (283)\ttotal: 14m 53s\tremaining: 29m 10s\n",
      "338:\tlearn: 3.6639409\ttest: 3.7876543\tbest: 3.7875796 (283)\ttotal: 14m 56s\tremaining: 29m 7s\n",
      "339:\tlearn: 3.6636668\ttest: 3.7877055\tbest: 3.7875796 (283)\ttotal: 14m 58s\tremaining: 29m 4s\n",
      "340:\tlearn: 3.6634640\ttest: 3.7877220\tbest: 3.7875796 (283)\ttotal: 15m 1s\tremaining: 29m 1s\n",
      "341:\tlearn: 3.6630078\ttest: 3.7877429\tbest: 3.7875796 (283)\ttotal: 15m 3s\tremaining: 28m 58s\n",
      "342:\tlearn: 3.6626584\ttest: 3.7877601\tbest: 3.7875796 (283)\ttotal: 15m 6s\tremaining: 28m 55s\n",
      "343:\tlearn: 3.6622349\ttest: 3.7877780\tbest: 3.7875796 (283)\ttotal: 15m 8s\tremaining: 28m 52s\n",
      "344:\tlearn: 3.6617858\ttest: 3.7877711\tbest: 3.7875796 (283)\ttotal: 15m 10s\tremaining: 28m 49s\n",
      "345:\tlearn: 3.6614406\ttest: 3.7877947\tbest: 3.7875796 (283)\ttotal: 15m 13s\tremaining: 28m 46s\n",
      "346:\tlearn: 3.6611588\ttest: 3.7878324\tbest: 3.7875796 (283)\ttotal: 15m 15s\tremaining: 28m 43s\n",
      "347:\tlearn: 3.6607707\ttest: 3.7878875\tbest: 3.7875796 (283)\ttotal: 15m 18s\tremaining: 28m 40s\n",
      "348:\tlearn: 3.6604898\ttest: 3.7878906\tbest: 3.7875796 (283)\ttotal: 15m 20s\tremaining: 28m 37s\n",
      "349:\tlearn: 3.6602272\ttest: 3.7878387\tbest: 3.7875796 (283)\ttotal: 15m 22s\tremaining: 28m 34s\n",
      "350:\tlearn: 3.6597986\ttest: 3.7878443\tbest: 3.7875796 (283)\ttotal: 15m 25s\tremaining: 28m 31s\n",
      "351:\tlearn: 3.6593614\ttest: 3.7878495\tbest: 3.7875796 (283)\ttotal: 15m 27s\tremaining: 28m 28s\n",
      "352:\tlearn: 3.6589252\ttest: 3.7878415\tbest: 3.7875796 (283)\ttotal: 15m 30s\tremaining: 28m 25s\n",
      "353:\tlearn: 3.6587029\ttest: 3.7878423\tbest: 3.7875796 (283)\ttotal: 15m 33s\tremaining: 28m 22s\n",
      "354:\tlearn: 3.6583760\ttest: 3.7878638\tbest: 3.7875796 (283)\ttotal: 15m 36s\tremaining: 28m 22s\n",
      "355:\tlearn: 3.6580744\ttest: 3.7878910\tbest: 3.7875796 (283)\ttotal: 15m 40s\tremaining: 28m 20s\n",
      "356:\tlearn: 3.6578348\ttest: 3.7878959\tbest: 3.7875796 (283)\ttotal: 15m 43s\tremaining: 28m 18s\n",
      "357:\tlearn: 3.6576106\ttest: 3.7878777\tbest: 3.7875796 (283)\ttotal: 15m 46s\tremaining: 28m 17s\n",
      "358:\tlearn: 3.6572080\ttest: 3.7878663\tbest: 3.7875796 (283)\ttotal: 15m 49s\tremaining: 28m 15s\n",
      "359:\tlearn: 3.6569998\ttest: 3.7878679\tbest: 3.7875796 (283)\ttotal: 15m 52s\tremaining: 28m 14s\n",
      "360:\tlearn: 3.6564838\ttest: 3.7878762\tbest: 3.7875796 (283)\ttotal: 15m 55s\tremaining: 28m 11s\n",
      "361:\tlearn: 3.6561280\ttest: 3.7878797\tbest: 3.7875796 (283)\ttotal: 15m 58s\tremaining: 28m 9s\n",
      "362:\tlearn: 3.6555755\ttest: 3.7878175\tbest: 3.7875796 (283)\ttotal: 16m 1s\tremaining: 28m 6s\n",
      "363:\tlearn: 3.6551797\ttest: 3.7878711\tbest: 3.7875796 (283)\ttotal: 16m 3s\tremaining: 28m 3s\n",
      "364:\tlearn: 3.6547889\ttest: 3.7878847\tbest: 3.7875796 (283)\ttotal: 16m 6s\tremaining: 28m 1s\n",
      "365:\tlearn: 3.6544843\ttest: 3.7878551\tbest: 3.7875796 (283)\ttotal: 16m 9s\tremaining: 27m 59s\n",
      "366:\tlearn: 3.6542411\ttest: 3.7878389\tbest: 3.7875796 (283)\ttotal: 16m 12s\tremaining: 27m 57s\n",
      "367:\tlearn: 3.6540537\ttest: 3.7878247\tbest: 3.7875796 (283)\ttotal: 16m 15s\tremaining: 27m 55s\n",
      "368:\tlearn: 3.6534443\ttest: 3.7878013\tbest: 3.7875796 (283)\ttotal: 16m 18s\tremaining: 27m 52s\n",
      "369:\tlearn: 3.6528786\ttest: 3.7878066\tbest: 3.7875796 (283)\ttotal: 16m 20s\tremaining: 27m 50s\n",
      "370:\tlearn: 3.6526015\ttest: 3.7878100\tbest: 3.7875796 (283)\ttotal: 16m 23s\tremaining: 27m 47s\n",
      "371:\tlearn: 3.6522677\ttest: 3.7878055\tbest: 3.7875796 (283)\ttotal: 16m 26s\tremaining: 27m 44s\n",
      "372:\tlearn: 3.6518522\ttest: 3.7877769\tbest: 3.7875796 (283)\ttotal: 16m 28s\tremaining: 27m 41s\n",
      "373:\tlearn: 3.6514349\ttest: 3.7878291\tbest: 3.7875796 (283)\ttotal: 16m 31s\tremaining: 27m 39s\n",
      "374:\tlearn: 3.6510321\ttest: 3.7878492\tbest: 3.7875796 (283)\ttotal: 16m 33s\tremaining: 27m 36s\n",
      "375:\tlearn: 3.6508026\ttest: 3.7878439\tbest: 3.7875796 (283)\ttotal: 16m 36s\tremaining: 27m 33s\n",
      "376:\tlearn: 3.6502948\ttest: 3.7878690\tbest: 3.7875796 (283)\ttotal: 16m 39s\tremaining: 27m 31s\n",
      "377:\tlearn: 3.6500546\ttest: 3.7878555\tbest: 3.7875796 (283)\ttotal: 16m 41s\tremaining: 27m 28s\n",
      "378:\tlearn: 3.6496271\ttest: 3.7878629\tbest: 3.7875796 (283)\ttotal: 16m 44s\tremaining: 27m 26s\n",
      "379:\tlearn: 3.6493115\ttest: 3.7878404\tbest: 3.7875796 (283)\ttotal: 16m 47s\tremaining: 27m 23s\n",
      "380:\tlearn: 3.6489146\ttest: 3.7878418\tbest: 3.7875796 (283)\ttotal: 16m 49s\tremaining: 27m 20s\n",
      "381:\tlearn: 3.6486778\ttest: 3.7878204\tbest: 3.7875796 (283)\ttotal: 16m 52s\tremaining: 27m 17s\n",
      "382:\tlearn: 3.6482286\ttest: 3.7877974\tbest: 3.7875796 (283)\ttotal: 16m 55s\tremaining: 27m 15s\n",
      "383:\tlearn: 3.6479260\ttest: 3.7877937\tbest: 3.7875796 (283)\ttotal: 16m 57s\tremaining: 27m 12s\n",
      "384:\tlearn: 3.6476909\ttest: 3.7877883\tbest: 3.7875796 (283)\ttotal: 17m\tremaining: 27m 9s\n",
      "385:\tlearn: 3.6474117\ttest: 3.7878034\tbest: 3.7875796 (283)\ttotal: 17m 2s\tremaining: 27m 7s\n",
      "386:\tlearn: 3.6471761\ttest: 3.7878074\tbest: 3.7875796 (283)\ttotal: 17m 5s\tremaining: 27m 4s\n",
      "387:\tlearn: 3.6469402\ttest: 3.7878119\tbest: 3.7875796 (283)\ttotal: 17m 8s\tremaining: 27m 1s\n",
      "388:\tlearn: 3.6466518\ttest: 3.7877883\tbest: 3.7875796 (283)\ttotal: 17m 10s\tremaining: 26m 59s\n",
      "389:\tlearn: 3.6464277\ttest: 3.7878084\tbest: 3.7875796 (283)\ttotal: 17m 13s\tremaining: 26m 56s\n",
      "390:\tlearn: 3.6459915\ttest: 3.7878009\tbest: 3.7875796 (283)\ttotal: 17m 15s\tremaining: 26m 53s\n",
      "391:\tlearn: 3.6455936\ttest: 3.7878315\tbest: 3.7875796 (283)\ttotal: 17m 18s\tremaining: 26m 50s\n",
      "392:\tlearn: 3.6454008\ttest: 3.7878396\tbest: 3.7875796 (283)\ttotal: 17m 21s\tremaining: 26m 48s\n",
      "393:\tlearn: 3.6450974\ttest: 3.7878681\tbest: 3.7875796 (283)\ttotal: 17m 24s\tremaining: 26m 45s\n",
      "394:\tlearn: 3.6446497\ttest: 3.7879288\tbest: 3.7875796 (283)\ttotal: 17m 26s\tremaining: 26m 43s\n",
      "395:\tlearn: 3.6444457\ttest: 3.7878879\tbest: 3.7875796 (283)\ttotal: 17m 29s\tremaining: 26m 40s\n",
      "396:\tlearn: 3.6441528\ttest: 3.7878945\tbest: 3.7875796 (283)\ttotal: 17m 32s\tremaining: 26m 38s\n",
      "397:\tlearn: 3.6439345\ttest: 3.7879002\tbest: 3.7875796 (283)\ttotal: 17m 34s\tremaining: 26m 35s\n",
      "398:\tlearn: 3.6437513\ttest: 3.7879154\tbest: 3.7875796 (283)\ttotal: 17m 37s\tremaining: 26m 32s\n",
      "399:\tlearn: 3.6434480\ttest: 3.7879608\tbest: 3.7875796 (283)\ttotal: 17m 39s\tremaining: 26m 29s\n",
      "400:\tlearn: 3.6432450\ttest: 3.7879727\tbest: 3.7875796 (283)\ttotal: 17m 42s\tremaining: 26m 27s\n",
      "401:\tlearn: 3.6429337\ttest: 3.7879393\tbest: 3.7875796 (283)\ttotal: 17m 45s\tremaining: 26m 25s\n",
      "402:\tlearn: 3.6426000\ttest: 3.7879681\tbest: 3.7875796 (283)\ttotal: 17m 48s\tremaining: 26m 22s\n",
      "403:\tlearn: 3.6424099\ttest: 3.7879766\tbest: 3.7875796 (283)\ttotal: 17m 51s\tremaining: 26m 20s\n",
      "404:\tlearn: 3.6421925\ttest: 3.7879856\tbest: 3.7875796 (283)\ttotal: 17m 53s\tremaining: 26m 17s\n",
      "405:\tlearn: 3.6421078\ttest: 3.7879809\tbest: 3.7875796 (283)\ttotal: 17m 56s\tremaining: 26m 14s\n",
      "406:\tlearn: 3.6415364\ttest: 3.7879621\tbest: 3.7875796 (283)\ttotal: 17m 58s\tremaining: 26m 11s\n",
      "407:\tlearn: 3.6411695\ttest: 3.7879564\tbest: 3.7875796 (283)\ttotal: 18m 1s\tremaining: 26m 9s\n",
      "408:\tlearn: 3.6410045\ttest: 3.7879562\tbest: 3.7875796 (283)\ttotal: 18m 4s\tremaining: 26m 7s\n",
      "409:\tlearn: 3.6406097\ttest: 3.7879837\tbest: 3.7875796 (283)\ttotal: 18m 7s\tremaining: 26m 5s\n",
      "410:\tlearn: 3.6402302\ttest: 3.7880134\tbest: 3.7875796 (283)\ttotal: 18m 10s\tremaining: 26m 2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411:\tlearn: 3.6396925\ttest: 3.7880796\tbest: 3.7875796 (283)\ttotal: 18m 12s\tremaining: 25m 59s\n",
      "412:\tlearn: 3.6395537\ttest: 3.7880815\tbest: 3.7875796 (283)\ttotal: 18m 15s\tremaining: 25m 56s\n",
      "413:\tlearn: 3.6393057\ttest: 3.7881226\tbest: 3.7875796 (283)\ttotal: 18m 18s\tremaining: 25m 54s\n",
      "414:\tlearn: 3.6389647\ttest: 3.7880822\tbest: 3.7875796 (283)\ttotal: 18m 20s\tremaining: 25m 51s\n",
      "415:\tlearn: 3.6384853\ttest: 3.7881300\tbest: 3.7875796 (283)\ttotal: 18m 23s\tremaining: 25m 48s\n",
      "416:\tlearn: 3.6380284\ttest: 3.7881282\tbest: 3.7875796 (283)\ttotal: 18m 26s\tremaining: 25m 46s\n",
      "417:\tlearn: 3.6376933\ttest: 3.7881606\tbest: 3.7875796 (283)\ttotal: 18m 28s\tremaining: 25m 44s\n",
      "418:\tlearn: 3.6373971\ttest: 3.7881692\tbest: 3.7875796 (283)\ttotal: 18m 31s\tremaining: 25m 41s\n",
      "419:\tlearn: 3.6372046\ttest: 3.7881842\tbest: 3.7875796 (283)\ttotal: 18m 34s\tremaining: 25m 38s\n",
      "420:\tlearn: 3.6370522\ttest: 3.7881811\tbest: 3.7875796 (283)\ttotal: 18m 37s\tremaining: 25m 36s\n",
      "421:\tlearn: 3.6368521\ttest: 3.7881948\tbest: 3.7875796 (283)\ttotal: 18m 40s\tremaining: 25m 34s\n",
      "422:\tlearn: 3.6363467\ttest: 3.7882216\tbest: 3.7875796 (283)\ttotal: 18m 43s\tremaining: 25m 32s\n",
      "423:\tlearn: 3.6359961\ttest: 3.7882180\tbest: 3.7875796 (283)\ttotal: 18m 46s\tremaining: 25m 30s\n",
      "424:\tlearn: 3.6354904\ttest: 3.7882098\tbest: 3.7875796 (283)\ttotal: 18m 49s\tremaining: 25m 28s\n",
      "425:\tlearn: 3.6351393\ttest: 3.7882054\tbest: 3.7875796 (283)\ttotal: 18m 52s\tremaining: 25m 25s\n",
      "426:\tlearn: 3.6347405\ttest: 3.7882337\tbest: 3.7875796 (283)\ttotal: 18m 55s\tremaining: 25m 23s\n",
      "427:\tlearn: 3.6344110\ttest: 3.7882231\tbest: 3.7875796 (283)\ttotal: 18m 57s\tremaining: 25m 20s\n",
      "428:\tlearn: 3.6340333\ttest: 3.7882007\tbest: 3.7875796 (283)\ttotal: 19m\tremaining: 25m 18s\n",
      "429:\tlearn: 3.6335566\ttest: 3.7881826\tbest: 3.7875796 (283)\ttotal: 19m 3s\tremaining: 25m 15s\n",
      "430:\tlearn: 3.6332823\ttest: 3.7882167\tbest: 3.7875796 (283)\ttotal: 19m 5s\tremaining: 25m 12s\n",
      "431:\tlearn: 3.6328201\ttest: 3.7882515\tbest: 3.7875796 (283)\ttotal: 19m 8s\tremaining: 25m 9s\n",
      "432:\tlearn: 3.6323485\ttest: 3.7882610\tbest: 3.7875796 (283)\ttotal: 19m 10s\tremaining: 25m 7s\n",
      "433:\tlearn: 3.6318903\ttest: 3.7882814\tbest: 3.7875796 (283)\ttotal: 19m 13s\tremaining: 25m 4s\n",
      "434:\tlearn: 3.6315983\ttest: 3.7883077\tbest: 3.7875796 (283)\ttotal: 19m 16s\tremaining: 25m 1s\n",
      "435:\tlearn: 3.6312009\ttest: 3.7882910\tbest: 3.7875796 (283)\ttotal: 19m 18s\tremaining: 24m 59s\n",
      "436:\tlearn: 3.6310898\ttest: 3.7882943\tbest: 3.7875796 (283)\ttotal: 19m 21s\tremaining: 24m 56s\n",
      "437:\tlearn: 3.6303916\ttest: 3.7883529\tbest: 3.7875796 (283)\ttotal: 19m 24s\tremaining: 24m 53s\n",
      "438:\tlearn: 3.6300992\ttest: 3.7883516\tbest: 3.7875796 (283)\ttotal: 19m 26s\tremaining: 24m 51s\n",
      "439:\tlearn: 3.6298015\ttest: 3.7883450\tbest: 3.7875796 (283)\ttotal: 19m 29s\tremaining: 24m 48s\n",
      "440:\tlearn: 3.6293752\ttest: 3.7883793\tbest: 3.7875796 (283)\ttotal: 19m 32s\tremaining: 24m 45s\n",
      "441:\tlearn: 3.6290701\ttest: 3.7884189\tbest: 3.7875796 (283)\ttotal: 19m 34s\tremaining: 24m 43s\n",
      "442:\tlearn: 3.6286972\ttest: 3.7884521\tbest: 3.7875796 (283)\ttotal: 19m 37s\tremaining: 24m 40s\n",
      "443:\tlearn: 3.6284193\ttest: 3.7884678\tbest: 3.7875796 (283)\ttotal: 19m 40s\tremaining: 24m 38s\n",
      "444:\tlearn: 3.6280557\ttest: 3.7885130\tbest: 3.7875796 (283)\ttotal: 19m 43s\tremaining: 24m 35s\n",
      "445:\tlearn: 3.6278035\ttest: 3.7885344\tbest: 3.7875796 (283)\ttotal: 19m 45s\tremaining: 24m 33s\n",
      "446:\tlearn: 3.6275566\ttest: 3.7885460\tbest: 3.7875796 (283)\ttotal: 19m 48s\tremaining: 24m 30s\n",
      "447:\tlearn: 3.6272791\ttest: 3.7885695\tbest: 3.7875796 (283)\ttotal: 19m 51s\tremaining: 24m 28s\n",
      "448:\tlearn: 3.6269230\ttest: 3.7885924\tbest: 3.7875796 (283)\ttotal: 19m 54s\tremaining: 24m 25s\n",
      "449:\tlearn: 3.6263923\ttest: 3.7886173\tbest: 3.7875796 (283)\ttotal: 19m 56s\tremaining: 24m 22s\n",
      "450:\tlearn: 3.6259750\ttest: 3.7885837\tbest: 3.7875796 (283)\ttotal: 19m 59s\tremaining: 24m 20s\n",
      "451:\tlearn: 3.6258103\ttest: 3.7885896\tbest: 3.7875796 (283)\ttotal: 20m 2s\tremaining: 24m 17s\n",
      "452:\tlearn: 3.6253796\ttest: 3.7886487\tbest: 3.7875796 (283)\ttotal: 20m 5s\tremaining: 24m 15s\n",
      "453:\tlearn: 3.6252208\ttest: 3.7886620\tbest: 3.7875796 (283)\ttotal: 20m 7s\tremaining: 24m 12s\n",
      "454:\tlearn: 3.6249530\ttest: 3.7886450\tbest: 3.7875796 (283)\ttotal: 20m 10s\tremaining: 24m 10s\n",
      "455:\tlearn: 3.6243472\ttest: 3.7886589\tbest: 3.7875796 (283)\ttotal: 20m 13s\tremaining: 24m 7s\n",
      "456:\tlearn: 3.6241872\ttest: 3.7886717\tbest: 3.7875796 (283)\ttotal: 20m 15s\tremaining: 24m 4s\n",
      "457:\tlearn: 3.6238409\ttest: 3.7887171\tbest: 3.7875796 (283)\ttotal: 20m 18s\tremaining: 24m 2s\n",
      "458:\tlearn: 3.6236291\ttest: 3.7887198\tbest: 3.7875796 (283)\ttotal: 20m 21s\tremaining: 23m 59s\n",
      "459:\tlearn: 3.6232773\ttest: 3.7887370\tbest: 3.7875796 (283)\ttotal: 20m 24s\tremaining: 23m 57s\n",
      "460:\tlearn: 3.6230469\ttest: 3.7888041\tbest: 3.7875796 (283)\ttotal: 20m 27s\tremaining: 23m 55s\n",
      "461:\tlearn: 3.6228139\ttest: 3.7888223\tbest: 3.7875796 (283)\ttotal: 20m 30s\tremaining: 23m 52s\n",
      "462:\tlearn: 3.6223731\ttest: 3.7887926\tbest: 3.7875796 (283)\ttotal: 20m 33s\tremaining: 23m 50s\n",
      "463:\tlearn: 3.6221109\ttest: 3.7887999\tbest: 3.7875796 (283)\ttotal: 20m 35s\tremaining: 23m 47s\n",
      "464:\tlearn: 3.6219270\ttest: 3.7888023\tbest: 3.7875796 (283)\ttotal: 20m 38s\tremaining: 23m 45s\n",
      "465:\tlearn: 3.6217934\ttest: 3.7887904\tbest: 3.7875796 (283)\ttotal: 20m 41s\tremaining: 23m 42s\n",
      "466:\tlearn: 3.6214844\ttest: 3.7888099\tbest: 3.7875796 (283)\ttotal: 20m 44s\tremaining: 23m 40s\n",
      "467:\tlearn: 3.6209504\ttest: 3.7888695\tbest: 3.7875796 (283)\ttotal: 20m 47s\tremaining: 23m 37s\n",
      "468:\tlearn: 3.6205979\ttest: 3.7888618\tbest: 3.7875796 (283)\ttotal: 20m 50s\tremaining: 23m 36s\n",
      "469:\tlearn: 3.6202320\ttest: 3.7888563\tbest: 3.7875796 (283)\ttotal: 20m 53s\tremaining: 23m 33s\n",
      "470:\tlearn: 3.6198543\ttest: 3.7888995\tbest: 3.7875796 (283)\ttotal: 20m 56s\tremaining: 23m 31s\n",
      "471:\tlearn: 3.6195013\ttest: 3.7889108\tbest: 3.7875796 (283)\ttotal: 20m 59s\tremaining: 23m 28s\n",
      "472:\tlearn: 3.6190987\ttest: 3.7888828\tbest: 3.7875796 (283)\ttotal: 21m 2s\tremaining: 23m 26s\n",
      "473:\tlearn: 3.6187046\ttest: 3.7888846\tbest: 3.7875796 (283)\ttotal: 21m 6s\tremaining: 23m 25s\n",
      "474:\tlearn: 3.6183651\ttest: 3.7888777\tbest: 3.7875796 (283)\ttotal: 21m 9s\tremaining: 23m 22s\n",
      "475:\tlearn: 3.6179425\ttest: 3.7888911\tbest: 3.7875796 (283)\ttotal: 21m 12s\tremaining: 23m 20s\n",
      "476:\tlearn: 3.6177239\ttest: 3.7889301\tbest: 3.7875796 (283)\ttotal: 21m 15s\tremaining: 23m 18s\n",
      "477:\tlearn: 3.6173337\ttest: 3.7888866\tbest: 3.7875796 (283)\ttotal: 21m 18s\tremaining: 23m 15s\n",
      "478:\tlearn: 3.6170245\ttest: 3.7888657\tbest: 3.7875796 (283)\ttotal: 21m 21s\tremaining: 23m 13s\n",
      "479:\tlearn: 3.6166073\ttest: 3.7889033\tbest: 3.7875796 (283)\ttotal: 21m 24s\tremaining: 23m 11s\n",
      "480:\tlearn: 3.6163564\ttest: 3.7889327\tbest: 3.7875796 (283)\ttotal: 21m 27s\tremaining: 23m 8s\n",
      "481:\tlearn: 3.6161533\ttest: 3.7889530\tbest: 3.7875796 (283)\ttotal: 21m 30s\tremaining: 23m 7s\n",
      "482:\tlearn: 3.6158921\ttest: 3.7889928\tbest: 3.7875796 (283)\ttotal: 21m 33s\tremaining: 23m 4s\n",
      "483:\tlearn: 3.6155843\ttest: 3.7889672\tbest: 3.7875796 (283)\ttotal: 21m 36s\tremaining: 23m 2s\n",
      "484:\tlearn: 3.6153603\ttest: 3.7889548\tbest: 3.7875796 (283)\ttotal: 21m 40s\tremaining: 23m\n",
      "485:\tlearn: 3.6152091\ttest: 3.7889616\tbest: 3.7875796 (283)\ttotal: 21m 43s\tremaining: 22m 59s\n",
      "486:\tlearn: 3.6149966\ttest: 3.7889955\tbest: 3.7875796 (283)\ttotal: 21m 47s\tremaining: 22m 57s\n",
      "487:\tlearn: 3.6147579\ttest: 3.7890168\tbest: 3.7875796 (283)\ttotal: 21m 50s\tremaining: 22m 55s\n",
      "488:\tlearn: 3.6146488\ttest: 3.7890050\tbest: 3.7875796 (283)\ttotal: 21m 53s\tremaining: 22m 53s\n",
      "489:\tlearn: 3.6140753\ttest: 3.7890346\tbest: 3.7875796 (283)\ttotal: 21m 57s\tremaining: 22m 50s\n",
      "490:\tlearn: 3.6133243\ttest: 3.7890356\tbest: 3.7875796 (283)\ttotal: 21m 59s\tremaining: 22m 48s\n",
      "491:\tlearn: 3.6125899\ttest: 3.7890860\tbest: 3.7875796 (283)\ttotal: 22m 2s\tremaining: 22m 45s\n",
      "492:\tlearn: 3.6120079\ttest: 3.7890796\tbest: 3.7875796 (283)\ttotal: 22m 5s\tremaining: 22m 42s\n",
      "493:\tlearn: 3.6116567\ttest: 3.7891319\tbest: 3.7875796 (283)\ttotal: 22m 7s\tremaining: 22m 40s\n",
      "494:\tlearn: 3.6114182\ttest: 3.7891430\tbest: 3.7875796 (283)\ttotal: 22m 10s\tremaining: 22m 37s\n",
      "495:\tlearn: 3.6111017\ttest: 3.7891699\tbest: 3.7875796 (283)\ttotal: 22m 13s\tremaining: 22m 34s\n",
      "496:\tlearn: 3.6107976\ttest: 3.7891449\tbest: 3.7875796 (283)\ttotal: 22m 16s\tremaining: 22m 32s\n",
      "497:\tlearn: 3.6103478\ttest: 3.7891080\tbest: 3.7875796 (283)\ttotal: 22m 18s\tremaining: 22m 29s\n",
      "498:\tlearn: 3.6100723\ttest: 3.7890916\tbest: 3.7875796 (283)\ttotal: 22m 21s\tremaining: 22m 26s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499:\tlearn: 3.6097844\ttest: 3.7890997\tbest: 3.7875796 (283)\ttotal: 22m 24s\tremaining: 22m 24s\n",
      "500:\tlearn: 3.6095720\ttest: 3.7890857\tbest: 3.7875796 (283)\ttotal: 22m 27s\tremaining: 22m 22s\n",
      "501:\tlearn: 3.6091542\ttest: 3.7890505\tbest: 3.7875796 (283)\ttotal: 22m 30s\tremaining: 22m 19s\n",
      "502:\tlearn: 3.6088789\ttest: 3.7891043\tbest: 3.7875796 (283)\ttotal: 22m 33s\tremaining: 22m 17s\n",
      "503:\tlearn: 3.6086468\ttest: 3.7891024\tbest: 3.7875796 (283)\ttotal: 22m 36s\tremaining: 22m 15s\n",
      "504:\tlearn: 3.6084637\ttest: 3.7890717\tbest: 3.7875796 (283)\ttotal: 22m 40s\tremaining: 22m 13s\n",
      "505:\tlearn: 3.6082667\ttest: 3.7890661\tbest: 3.7875796 (283)\ttotal: 22m 43s\tremaining: 22m 11s\n",
      "506:\tlearn: 3.6080953\ttest: 3.7890581\tbest: 3.7875796 (283)\ttotal: 22m 46s\tremaining: 22m 8s\n",
      "507:\tlearn: 3.6077423\ttest: 3.7890751\tbest: 3.7875796 (283)\ttotal: 22m 48s\tremaining: 22m 5s\n",
      "508:\tlearn: 3.6071395\ttest: 3.7890663\tbest: 3.7875796 (283)\ttotal: 22m 51s\tremaining: 22m 2s\n",
      "509:\tlearn: 3.6067336\ttest: 3.7890671\tbest: 3.7875796 (283)\ttotal: 22m 53s\tremaining: 21m 59s\n",
      "510:\tlearn: 3.6064943\ttest: 3.7890623\tbest: 3.7875796 (283)\ttotal: 22m 56s\tremaining: 21m 57s\n",
      "511:\tlearn: 3.6061782\ttest: 3.7891085\tbest: 3.7875796 (283)\ttotal: 22m 58s\tremaining: 21m 54s\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor, Pool\n",
    "rskf=StratifiedKFold(5,shuffle=True,random_state=4590)\n",
    "val_pr=np.zeros(len(df_train))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "#test_pr=np.zeros(len(df_test))\n",
    "for train_index,val_index in rskf.split(df_train,df_train['outliers'].values):\n",
    "    pool=Pool(df_train[df_train_columns].loc[train_index],df_train['target'].loc[train_index])\n",
    "    val_pool=Pool(df_train[df_train_columns].loc[val_index],df_train['target'].loc[val_index])\n",
    "    num_round = 10000\n",
    "    model = CatBoostRegressor(iterations=1000,learning_rate=0.01,max_depth=14, loss_function='RMSE',early_stopping_rounds=300)\n",
    "    model.fit(pool,eval_set=val_pool,use_best_model=True,verbose_eval=True)\n",
    "    \n",
    "    val_pr[val_index]=model.predict(df_train[df_train_columns].loc[val_index])\n",
    "np.sqrt(mean_squared_error(val_pr,df_train['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1749.11 Mb (43.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "path=\"D:\\Python\\Elo\"\n",
    "df_hist = pd.read_csv(os.path.join(path,'historical_transactions.csv'))\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "df_train=reduce_mem_usage(df_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-e20cf2fc4ac6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgroupby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'card_id'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'category_3'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "df=df_train\n",
    "groupby='card_id'\n",
    "target='category_3'\n",
    "df_bag = pd.DataFrame(df[[groupby, target]])\n",
    "df_bag[target] = df_bag[target].astype(str)\n",
    "df_bag[target].fillna('NAN', inplace=True)\n",
    "df_bag = df_bag.groupby(groupby, as_index=False)[target].agg({'list':(lambda x: list(x))}).reset_index()\n",
    "doc_list = list(df_bag['list'].values)\n",
    "w2v = Word2Vec(doc_list, size=30, window=3, min_count=1, workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>card_id</th>\n",
       "      <th>list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>C_ID_00007093c1</td>\n",
       "      <td>[B, B, B, C, B, B, B, B, B, C, B, B, B, B, B, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>C_ID_0001238066</td>\n",
       "      <td>[nan, C, B, C, B, C, B, C, B, B, B, B, B, B, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>C_ID_0001506ef0</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>C_ID_0001793786</td>\n",
       "      <td>[A, A, A, A, A, A, A, B, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>C_ID_000183fdda</td>\n",
       "      <td>[B, B, C, C, B, C, B, B, B, B, B, C, C, C, B, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>C_ID_00024e244b</td>\n",
       "      <td>[A, B, A, A, A, A, A, B, A, A, B, B, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>C_ID_0002709b5a</td>\n",
       "      <td>[B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>C_ID_00027503e2</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>C_ID_000298032a</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>C_ID_0002ba3c2e</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>C_ID_0002c7c2c1</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>C_ID_00032df08f</td>\n",
       "      <td>[B, B, C, B, B, B, B, B, B, B, B, B, B, B, B, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>C_ID_0003754056</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>C_ID_000377f6a0</td>\n",
       "      <td>[A, A, A, A, A, A, C, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>C_ID_0003be3c83</td>\n",
       "      <td>[B, C, B, B, B, C, B, B, B, B, B, B, B, B, C, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>C_ID_0003f41435</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>C_ID_00042d509c</td>\n",
       "      <td>[B, C, B, B, B, B, B, B, B, B, B, B, B, B, C, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>C_ID_0004587331</td>\n",
       "      <td>[A, B, A, A, A, B, A, A, A, A, A, A, A, A]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>C_ID_0004725b87</td>\n",
       "      <td>[B, B, B, B, B, B, B, C, B, B, B, C, A, B, C, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>C_ID_0004888ddd</td>\n",
       "      <td>[A, A, A, A, A, A, B, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>C_ID_0004b68c49</td>\n",
       "      <td>[B, B, B, C, B]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>C_ID_0004c2a5ab</td>\n",
       "      <td>[B, B, B, B, B, C, B, C, C, C, B, B, C, B, B, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>C_ID_00057b99fe</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>C_ID_000599daf9</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>C_ID_0005b2f279</td>\n",
       "      <td>[B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>C_ID_0005b5804f</td>\n",
       "      <td>[A, A, A, A, A, A, A, A]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>C_ID_0005f16cc8</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, B, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>C_ID_0006152db8</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, B, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>C_ID_000616f4a8</td>\n",
       "      <td>[B, B, B, B, B, B, B, C, B, B, B, C, C, B, B, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>C_ID_000664aa02</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325510</th>\n",
       "      <td>325510</td>\n",
       "      <td>C_ID_fffb79fb56</td>\n",
       "      <td>[B, B, C, B, B, B, B, B, B, B, B, B, B, B, B, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325511</th>\n",
       "      <td>325511</td>\n",
       "      <td>C_ID_fffb9ea3f6</td>\n",
       "      <td>[A, B, A, A, A, A, A, A, A, A, A, B, A, B, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325512</th>\n",
       "      <td>325512</td>\n",
       "      <td>C_ID_fffba72dc5</td>\n",
       "      <td>[A, A, B, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325513</th>\n",
       "      <td>325513</td>\n",
       "      <td>C_ID_fffbdf036b</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325514</th>\n",
       "      <td>325514</td>\n",
       "      <td>C_ID_fffbee5c24</td>\n",
       "      <td>[C, B, C, C, C, B, C, C, B, C, C, C, B, B, B, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325515</th>\n",
       "      <td>325515</td>\n",
       "      <td>C_ID_fffc96bf24</td>\n",
       "      <td>[B, B, B, B, B, B, B, C, B, B, B, B, B, B, C, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325516</th>\n",
       "      <td>325516</td>\n",
       "      <td>C_ID_fffcb74f49</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325517</th>\n",
       "      <td>325517</td>\n",
       "      <td>C_ID_fffcf66e12</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325518</th>\n",
       "      <td>325518</td>\n",
       "      <td>C_ID_fffd070e0e</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325519</th>\n",
       "      <td>325519</td>\n",
       "      <td>C_ID_fffd1207a8</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325520</th>\n",
       "      <td>325520</td>\n",
       "      <td>C_ID_fffd24dcf3</td>\n",
       "      <td>[C, B, C, C, C, C, C, B, C, C, B, C, C, C]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325521</th>\n",
       "      <td>325521</td>\n",
       "      <td>C_ID_fffd337484</td>\n",
       "      <td>[C, B, B, B, B, C, C, C, C, C, C, B, B, C, C, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325522</th>\n",
       "      <td>325522</td>\n",
       "      <td>C_ID_fffd345048</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325523</th>\n",
       "      <td>325523</td>\n",
       "      <td>C_ID_fffd351e7f</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325524</th>\n",
       "      <td>325524</td>\n",
       "      <td>C_ID_fffd93c8bf</td>\n",
       "      <td>[A, A, B, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325525</th>\n",
       "      <td>325525</td>\n",
       "      <td>C_ID_fffd943b91</td>\n",
       "      <td>[B, B, B, B, B, B, B, B, B, B, B, C, B, C, B, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325526</th>\n",
       "      <td>325526</td>\n",
       "      <td>C_ID_fffdad8aa0</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325527</th>\n",
       "      <td>325527</td>\n",
       "      <td>C_ID_fffde15ab6</td>\n",
       "      <td>[B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325528</th>\n",
       "      <td>325528</td>\n",
       "      <td>C_ID_fffdf34798</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325529</th>\n",
       "      <td>325529</td>\n",
       "      <td>C_ID_fffe29a04a</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325530</th>\n",
       "      <td>325530</td>\n",
       "      <td>C_ID_fffe78b232</td>\n",
       "      <td>[B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325531</th>\n",
       "      <td>325531</td>\n",
       "      <td>C_ID_fffe866457</td>\n",
       "      <td>[B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325532</th>\n",
       "      <td>325532</td>\n",
       "      <td>C_ID_fffea6de74</td>\n",
       "      <td>[B, B, B, B, B, B, B, B, B, B, B, B, B, B, C, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325533</th>\n",
       "      <td>325533</td>\n",
       "      <td>C_ID_fffeced303</td>\n",
       "      <td>[B, B, B, B, B, B, B, C, B, B, B, B, B, B, B, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325534</th>\n",
       "      <td>325534</td>\n",
       "      <td>C_ID_fffeed3a89</td>\n",
       "      <td>[B, B, B, B, C, B, B, B, B, B, C, nan, B, B, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325535</th>\n",
       "      <td>325535</td>\n",
       "      <td>C_ID_ffff1d9928</td>\n",
       "      <td>[B, B, B, C, nan, B, B, B, B, B, B, B]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325536</th>\n",
       "      <td>325536</td>\n",
       "      <td>C_ID_ffff579d3a</td>\n",
       "      <td>[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325537</th>\n",
       "      <td>325537</td>\n",
       "      <td>C_ID_ffff756266</td>\n",
       "      <td>[C, B, B, C, B, C, C, B, C, B, C, C, C, B, C, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325538</th>\n",
       "      <td>325538</td>\n",
       "      <td>C_ID_ffff828181</td>\n",
       "      <td>[B, B, B, B, C, B, B, B, B, B, B, B, B, B, B, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325539</th>\n",
       "      <td>325539</td>\n",
       "      <td>C_ID_fffffd5772</td>\n",
       "      <td>[B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>325540 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index          card_id  \\\n",
       "0            0  C_ID_00007093c1   \n",
       "1            1  C_ID_0001238066   \n",
       "2            2  C_ID_0001506ef0   \n",
       "3            3  C_ID_0001793786   \n",
       "4            4  C_ID_000183fdda   \n",
       "5            5  C_ID_00024e244b   \n",
       "6            6  C_ID_0002709b5a   \n",
       "7            7  C_ID_00027503e2   \n",
       "8            8  C_ID_000298032a   \n",
       "9            9  C_ID_0002ba3c2e   \n",
       "10          10  C_ID_0002c7c2c1   \n",
       "11          11  C_ID_00032df08f   \n",
       "12          12  C_ID_0003754056   \n",
       "13          13  C_ID_000377f6a0   \n",
       "14          14  C_ID_0003be3c83   \n",
       "15          15  C_ID_0003f41435   \n",
       "16          16  C_ID_00042d509c   \n",
       "17          17  C_ID_0004587331   \n",
       "18          18  C_ID_0004725b87   \n",
       "19          19  C_ID_0004888ddd   \n",
       "20          20  C_ID_0004b68c49   \n",
       "21          21  C_ID_0004c2a5ab   \n",
       "22          22  C_ID_00057b99fe   \n",
       "23          23  C_ID_000599daf9   \n",
       "24          24  C_ID_0005b2f279   \n",
       "25          25  C_ID_0005b5804f   \n",
       "26          26  C_ID_0005f16cc8   \n",
       "27          27  C_ID_0006152db8   \n",
       "28          28  C_ID_000616f4a8   \n",
       "29          29  C_ID_000664aa02   \n",
       "...        ...              ...   \n",
       "325510  325510  C_ID_fffb79fb56   \n",
       "325511  325511  C_ID_fffb9ea3f6   \n",
       "325512  325512  C_ID_fffba72dc5   \n",
       "325513  325513  C_ID_fffbdf036b   \n",
       "325514  325514  C_ID_fffbee5c24   \n",
       "325515  325515  C_ID_fffc96bf24   \n",
       "325516  325516  C_ID_fffcb74f49   \n",
       "325517  325517  C_ID_fffcf66e12   \n",
       "325518  325518  C_ID_fffd070e0e   \n",
       "325519  325519  C_ID_fffd1207a8   \n",
       "325520  325520  C_ID_fffd24dcf3   \n",
       "325521  325521  C_ID_fffd337484   \n",
       "325522  325522  C_ID_fffd345048   \n",
       "325523  325523  C_ID_fffd351e7f   \n",
       "325524  325524  C_ID_fffd93c8bf   \n",
       "325525  325525  C_ID_fffd943b91   \n",
       "325526  325526  C_ID_fffdad8aa0   \n",
       "325527  325527  C_ID_fffde15ab6   \n",
       "325528  325528  C_ID_fffdf34798   \n",
       "325529  325529  C_ID_fffe29a04a   \n",
       "325530  325530  C_ID_fffe78b232   \n",
       "325531  325531  C_ID_fffe866457   \n",
       "325532  325532  C_ID_fffea6de74   \n",
       "325533  325533  C_ID_fffeced303   \n",
       "325534  325534  C_ID_fffeed3a89   \n",
       "325535  325535  C_ID_ffff1d9928   \n",
       "325536  325536  C_ID_ffff579d3a   \n",
       "325537  325537  C_ID_ffff756266   \n",
       "325538  325538  C_ID_ffff828181   \n",
       "325539  325539  C_ID_fffffd5772   \n",
       "\n",
       "                                                     list  \n",
       "0       [B, B, B, C, B, B, B, B, B, C, B, B, B, B, B, ...  \n",
       "1       [nan, C, B, C, B, C, B, C, B, B, B, B, B, B, B...  \n",
       "2       [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "3       [A, A, A, A, A, A, A, B, A, A, A, A, A, A, A, ...  \n",
       "4       [B, B, C, C, B, C, B, B, B, B, B, C, C, C, B, ...  \n",
       "5       [A, B, A, A, A, A, A, B, A, A, B, B, A, A, A, ...  \n",
       "6       [B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, ...  \n",
       "7       [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "8       [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "9       [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "10      [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "11      [B, B, C, B, B, B, B, B, B, B, B, B, B, B, B, ...  \n",
       "12      [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "13      [A, A, A, A, A, A, C, A, A, A, A, A, A, A, A, ...  \n",
       "14      [B, C, B, B, B, C, B, B, B, B, B, B, B, B, C, ...  \n",
       "15      [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "16      [B, C, B, B, B, B, B, B, B, B, B, B, B, B, C, ...  \n",
       "17             [A, B, A, A, A, B, A, A, A, A, A, A, A, A]  \n",
       "18      [B, B, B, B, B, B, B, C, B, B, B, C, A, B, C, ...  \n",
       "19      [A, A, A, A, A, A, B, A, A, A, A, A, A, A, A, ...  \n",
       "20                                        [B, B, B, C, B]  \n",
       "21      [B, B, B, B, B, C, B, C, C, C, B, B, C, B, B, ...  \n",
       "22      [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "23      [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "24       [B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B]  \n",
       "25                               [A, A, A, A, A, A, A, A]  \n",
       "26      [A, A, A, A, A, A, A, A, A, A, A, A, A, A, B, ...  \n",
       "27      [A, A, A, A, A, A, A, A, A, A, A, A, B, A, A, ...  \n",
       "28      [B, B, B, B, B, B, B, C, B, B, B, C, C, B, B, ...  \n",
       "29      [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "...                                                   ...  \n",
       "325510  [B, B, C, B, B, B, B, B, B, B, B, B, B, B, B, ...  \n",
       "325511  [A, B, A, A, A, A, A, A, A, A, A, B, A, B, A, ...  \n",
       "325512  [A, A, B, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "325513  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "325514  [C, B, C, C, C, B, C, C, B, C, C, C, B, B, B, ...  \n",
       "325515  [B, B, B, B, B, B, B, C, B, B, B, B, B, B, C, ...  \n",
       "325516  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "325517  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "325518  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "325519  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "325520         [C, B, C, C, C, C, C, B, C, C, B, C, C, C]  \n",
       "325521  [C, B, B, B, B, C, C, C, C, C, C, B, B, C, C, ...  \n",
       "325522  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "325523  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "325524  [A, A, B, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "325525  [B, B, B, B, B, B, B, B, B, B, B, C, B, C, B, ...  \n",
       "325526  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "325527  [B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, ...  \n",
       "325528  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "325529  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "325530  [B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, ...  \n",
       "325531  [B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, ...  \n",
       "325532  [B, B, B, B, B, B, B, B, B, B, B, B, B, B, C, ...  \n",
       "325533  [B, B, B, B, B, B, B, C, B, B, B, B, B, B, B, ...  \n",
       "325534  [B, B, B, B, C, B, B, B, B, B, C, nan, B, B, C...  \n",
       "325535             [B, B, B, C, nan, B, B, B, B, B, B, B]  \n",
       "325536  [A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, ...  \n",
       "325537  [C, B, B, C, B, C, C, B, C, B, C, C, C, B, C, ...  \n",
       "325538  [B, B, B, B, C, B, B, B, B, B, B, B, B, B, B, ...  \n",
       "325539  [B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, ...  \n",
       "\n",
       "[325540 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
